{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import and read csv files containing inputdata and labels\n",
    "\n",
    "traindata=pd.read_csv('traindata_gray.csv')\n",
    "traindata=traindata.drop(['Unnamed: 0'],axis=1)\n",
    "labeldata=pd.read_csv('labeldata_gray.csv')\n",
    "labeldata=labeldata.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 9216)\n",
      "(8000, 2)\n",
      "(2000, 9216)\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "#split data into training and testing data with 8000 images in traindata and 2000 images in test data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(traindata,labeldata, test_size=0.20, random_state=200)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert pandas dataframe to numpy matrix\n",
    "\n",
    "X_train=X_train.as_matrix()\n",
    "Y_train=Y_train.as_matrix()\n",
    "\n",
    "X_test=X_test.as_matrix()\n",
    "Y_test=Y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batches formed for input to cnn\n",
    "X_train_batch=np.vsplit(X_train,64)\n",
    "Y_train_batch=np.vsplit(Y_train,64)\n",
    "X_test_batch=np.vsplit(X_test,20)\n",
    "Y_test_batch=np.vsplit(Y_test,20)\n",
    "\n",
    "#reshaping input data for each batch to feed to CNN\n",
    "for i in range(0,64):\n",
    "    X_train_batch[i]=X_train_batch[i].reshape(125,128,72,1)\n",
    "\n",
    "for i in range(0,20):\n",
    "    X_test_batch[i]=X_test_batch[i].reshape(100,128,72,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 128, 72, 1)\n",
      "(125, 2)\n",
      "(100, 128, 72, 1)\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_batch[3].shape)\n",
    "print(Y_train_batch[3].shape)\n",
    "print(X_test_batch[3].shape)\n",
    "print(Y_test_batch[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 --batch : 1 --- Loss : 1841876.1250\n",
      "Epoch :  1 --batch : 2 --- Loss : 455891.7500\n",
      "Epoch :  1 --batch : 3 --- Loss : 24151.8594\n",
      "Epoch :  1 --batch : 4 --- Loss : 322538.0625\n",
      "Epoch :  1 --batch : 5 --- Loss : 11699.8838\n",
      "Epoch :  1 --batch : 6 --- Loss : 100318.6641\n",
      "Epoch :  1 --batch : 7 --- Loss : 43562.6797\n",
      "Epoch :  1 --batch : 8 --- Loss : 18860.0449\n",
      "Epoch :  1 --batch : 9 --- Loss : 12488.7471\n",
      "Epoch :  1 --batch : 10 --- Loss : 11406.8965\n",
      "Epoch :  1 --batch : 11 --- Loss :  8074.1431\n",
      "Epoch :  1 --batch : 12 --- Loss :  9934.1777\n",
      "Epoch :  1 --batch : 13 --- Loss :  6777.1875\n",
      "Epoch :  1 --batch : 14 --- Loss :  5679.1177\n",
      "Epoch :  1 --batch : 15 --- Loss :  5740.6191\n",
      "Epoch :  1 --batch : 16 --- Loss :  5886.0952\n",
      "Epoch :  1 --batch : 17 --- Loss :  5637.0659\n",
      "Epoch :  1 --batch : 18 --- Loss :  4811.0757\n",
      "Epoch :  1 --batch : 19 --- Loss :  5609.7310\n",
      "Epoch :  1 --batch : 20 --- Loss :  3362.3115\n",
      "Epoch :  1 --batch : 21 --- Loss :  3961.1973\n",
      "Epoch :  1 --batch : 22 --- Loss :  2819.6162\n",
      "Epoch :  1 --batch : 23 --- Loss :  2148.0605\n",
      "Epoch :  1 --batch : 24 --- Loss :  2709.7742\n",
      "Epoch :  1 --batch : 25 --- Loss :  2003.0947\n",
      "Epoch :  1 --batch : 26 --- Loss :  2172.2354\n",
      "Epoch :  1 --batch : 27 --- Loss :  1217.1466\n",
      "Epoch :  1 --batch : 28 --- Loss :  1075.6735\n",
      "Epoch :  1 --batch : 29 --- Loss :   716.7375\n",
      "Epoch :  1 --batch : 30 --- Loss :  1097.2122\n",
      "Epoch :  1 --batch : 31 --- Loss :   592.4482\n",
      "Epoch :  1 --batch : 32 --- Loss :   463.4571\n",
      "Epoch :  1 --batch : 33 --- Loss :   499.4646\n",
      "Epoch :  1 --batch : 34 --- Loss :   389.2201\n",
      "Epoch :  1 --batch : 35 --- Loss :   695.6660\n",
      "Epoch :  1 --batch : 36 --- Loss :   312.1450\n",
      "Epoch :  1 --batch : 37 --- Loss :   424.4272\n",
      "Epoch :  1 --batch : 38 --- Loss :   387.7315\n",
      "Epoch :  1 --batch : 39 --- Loss :   538.6694\n",
      "Epoch :  1 --batch : 40 --- Loss :   381.5502\n",
      "Epoch :  1 --batch : 41 --- Loss :   502.5649\n",
      "Epoch :  1 --batch : 42 --- Loss :   375.9550\n",
      "Epoch :  1 --batch : 43 --- Loss :   259.8174\n",
      "Epoch :  1 --batch : 44 --- Loss :   497.3469\n",
      "Epoch :  1 --batch : 45 --- Loss :   478.7024\n",
      "Epoch :  1 --batch : 46 --- Loss :   288.8086\n",
      "Epoch :  1 --batch : 47 --- Loss :   390.4564\n",
      "Epoch :  1 --batch : 48 --- Loss :   420.5094\n",
      "Epoch :  1 --batch : 49 --- Loss :   383.5028\n",
      "Epoch :  1 --batch : 50 --- Loss :   372.4457\n",
      "Epoch :  1 --batch : 51 --- Loss :   294.9508\n",
      "Epoch :  1 --batch : 52 --- Loss :   431.6805\n",
      "Epoch :  1 --batch : 53 --- Loss :   335.9149\n",
      "Epoch :  1 --batch : 54 --- Loss :   343.2892\n",
      "Epoch :  1 --batch : 55 --- Loss :   415.1822\n",
      "Epoch :  1 --batch : 56 --- Loss :   356.9112\n",
      "Epoch :  1 --batch : 57 --- Loss :   360.6646\n",
      "Epoch :  1 --batch : 58 --- Loss :   326.2644\n",
      "Epoch :  1 --batch : 59 --- Loss :   188.0091\n",
      "Epoch :  1 --batch : 60 --- Loss :   372.0275\n",
      "Epoch :  1 --batch : 61 --- Loss :   348.9940\n",
      "Epoch :  1 --batch : 62 --- Loss :   233.8284\n",
      "Epoch :  1 --batch : 63 --- Loss :   373.3793\n",
      "Epoch :  1 --batch : 64 --- Loss :   397.2302\n",
      "Testing Accuracy after epoch 1 : 0.825\n",
      "Epoch :  2 --batch : 1 --- Loss :   435.8918\n",
      "Epoch :  2 --batch : 2 --- Loss :   483.0149\n",
      "Epoch :  2 --batch : 3 --- Loss :   195.3739\n",
      "Epoch :  2 --batch : 4 --- Loss :   336.9023\n",
      "Epoch :  2 --batch : 5 --- Loss :   271.0080\n",
      "Epoch :  2 --batch : 6 --- Loss :   341.3924\n",
      "Epoch :  2 --batch : 7 --- Loss :   269.3624\n",
      "Epoch :  2 --batch : 8 --- Loss :   272.0421\n",
      "Epoch :  2 --batch : 9 --- Loss :   305.8570\n",
      "Epoch :  2 --batch : 10 --- Loss :   380.0541\n",
      "Epoch :  2 --batch : 11 --- Loss :   214.6480\n",
      "Epoch :  2 --batch : 12 --- Loss :   300.2720\n",
      "Epoch :  2 --batch : 13 --- Loss :   223.7541\n",
      "Epoch :  2 --batch : 14 --- Loss :   328.8922\n",
      "Epoch :  2 --batch : 15 --- Loss :   181.9106\n",
      "Epoch :  2 --batch : 16 --- Loss :   327.1240\n",
      "Epoch :  2 --batch : 17 --- Loss :   284.6501\n",
      "Epoch :  2 --batch : 18 --- Loss :   331.6266\n",
      "Epoch :  2 --batch : 19 --- Loss :   403.6836\n",
      "Epoch :  2 --batch : 20 --- Loss :   325.1081\n",
      "Epoch :  2 --batch : 21 --- Loss :   280.5187\n",
      "Epoch :  2 --batch : 22 --- Loss :   246.2074\n",
      "Epoch :  2 --batch : 23 --- Loss :   158.3379\n",
      "Epoch :  2 --batch : 24 --- Loss :   275.6251\n",
      "Epoch :  2 --batch : 25 --- Loss :   225.5086\n",
      "Epoch :  2 --batch : 26 --- Loss :   292.0820\n",
      "Epoch :  2 --batch : 27 --- Loss :   251.8783\n",
      "Epoch :  2 --batch : 28 --- Loss :   249.2655\n",
      "Epoch :  2 --batch : 29 --- Loss :   240.9910\n",
      "Epoch :  2 --batch : 30 --- Loss :   330.7791\n",
      "Epoch :  2 --batch : 31 --- Loss :   275.4991\n",
      "Epoch :  2 --batch : 32 --- Loss :   167.0072\n",
      "Epoch :  2 --batch : 33 --- Loss :   224.6593\n",
      "Epoch :  2 --batch : 34 --- Loss :   192.3508\n",
      "Epoch :  2 --batch : 35 --- Loss :   310.3482\n",
      "Epoch :  2 --batch : 36 --- Loss :   258.8319\n",
      "Epoch :  2 --batch : 37 --- Loss :   284.4857\n",
      "Epoch :  2 --batch : 38 --- Loss :   220.5140\n",
      "Epoch :  2 --batch : 39 --- Loss :   208.7041\n",
      "Epoch :  2 --batch : 40 --- Loss :   243.9074\n",
      "Epoch :  2 --batch : 41 --- Loss :   283.7085\n",
      "Epoch :  2 --batch : 42 --- Loss :   241.3740\n",
      "Epoch :  2 --batch : 43 --- Loss :   155.0729\n",
      "Epoch :  2 --batch : 44 --- Loss :   238.2932\n",
      "Epoch :  2 --batch : 45 --- Loss :   235.6067\n",
      "Epoch :  2 --batch : 46 --- Loss :   184.2190\n",
      "Epoch :  2 --batch : 47 --- Loss :   257.3511\n",
      "Epoch :  2 --batch : 48 --- Loss :   290.2058\n",
      "Epoch :  2 --batch : 49 --- Loss :   208.2792\n",
      "Epoch :  2 --batch : 50 --- Loss :   248.1895\n",
      "Epoch :  2 --batch : 51 --- Loss :   220.7349\n",
      "Epoch :  2 --batch : 52 --- Loss :   238.3005\n",
      "Epoch :  2 --batch : 53 --- Loss :   237.7842\n",
      "Epoch :  2 --batch : 54 --- Loss :   200.3536\n",
      "Epoch :  2 --batch : 55 --- Loss :   224.8757\n",
      "Epoch :  2 --batch : 56 --- Loss :   246.8492\n",
      "Epoch :  2 --batch : 57 --- Loss :   220.0494\n",
      "Epoch :  2 --batch : 58 --- Loss :   211.2650\n",
      "Epoch :  2 --batch : 59 --- Loss :   123.5724\n",
      "Epoch :  2 --batch : 60 --- Loss :   163.9455\n",
      "Epoch :  2 --batch : 61 --- Loss :   179.9164\n",
      "Epoch :  2 --batch : 62 --- Loss :   154.1918\n",
      "Epoch :  2 --batch : 63 --- Loss :   253.6578\n",
      "Epoch :  2 --batch : 64 --- Loss :   187.0844\n",
      "Testing Accuracy after epoch 2 : 0.8745000004768372\n",
      "Epoch :  3 --batch : 1 --- Loss :   306.2673\n",
      "Epoch :  3 --batch : 2 --- Loss :   236.1108\n",
      "Epoch :  3 --batch : 3 --- Loss :   184.5747\n",
      "Epoch :  3 --batch : 4 --- Loss :   281.5313\n",
      "Epoch :  3 --batch : 5 --- Loss :   179.6179\n",
      "Epoch :  3 --batch : 6 --- Loss :   182.3380\n",
      "Epoch :  3 --batch : 7 --- Loss :   189.2489\n",
      "Epoch :  3 --batch : 8 --- Loss :   132.6450\n",
      "Epoch :  3 --batch : 9 --- Loss :   204.3966\n",
      "Epoch :  3 --batch : 10 --- Loss :   241.9549\n",
      "Epoch :  3 --batch : 11 --- Loss :   143.3743\n",
      "Epoch :  3 --batch : 12 --- Loss :   209.1706\n",
      "Epoch :  3 --batch : 13 --- Loss :   118.6804\n",
      "Epoch :  3 --batch : 14 --- Loss :   201.9146\n",
      "Epoch :  3 --batch : 15 --- Loss :   147.7788\n",
      "Epoch :  3 --batch : 16 --- Loss :   128.1473\n",
      "Epoch :  3 --batch : 17 --- Loss :   180.1725\n",
      "Epoch :  3 --batch : 18 --- Loss :   195.8525\n",
      "Epoch :  3 --batch : 19 --- Loss :   142.2554\n",
      "Epoch :  3 --batch : 20 --- Loss :   140.5937\n",
      "Epoch :  3 --batch : 21 --- Loss :   215.7466\n",
      "Epoch :  3 --batch : 22 --- Loss :   168.6205\n",
      "Epoch :  3 --batch : 23 --- Loss :    90.8913\n",
      "Epoch :  3 --batch : 24 --- Loss :   176.3826\n",
      "Epoch :  3 --batch : 25 --- Loss :   217.7659\n",
      "Epoch :  3 --batch : 26 --- Loss :   181.7454\n",
      "Epoch :  3 --batch : 27 --- Loss :   209.9673\n",
      "Epoch :  3 --batch : 28 --- Loss :   217.7381\n",
      "Epoch :  3 --batch : 29 --- Loss :   139.8261\n",
      "Epoch :  3 --batch : 30 --- Loss :   225.5975\n",
      "Epoch :  3 --batch : 31 --- Loss :   170.2002\n",
      "Epoch :  3 --batch : 32 --- Loss :   126.1425\n",
      "Epoch :  3 --batch : 33 --- Loss :   144.5510\n",
      "Epoch :  3 --batch : 34 --- Loss :   132.3280\n",
      "Epoch :  3 --batch : 35 --- Loss :   236.9917\n",
      "Epoch :  3 --batch : 36 --- Loss :   227.5848\n",
      "Epoch :  3 --batch : 37 --- Loss :   239.2505\n",
      "Epoch :  3 --batch : 38 --- Loss :   195.1292\n",
      "Epoch :  3 --batch : 39 --- Loss :   130.0305\n",
      "Epoch :  3 --batch : 40 --- Loss :   167.1923\n",
      "Epoch :  3 --batch : 41 --- Loss :   233.7117\n",
      "Epoch :  3 --batch : 42 --- Loss :   223.7211\n",
      "Epoch :  3 --batch : 43 --- Loss :   151.2762\n",
      "Epoch :  3 --batch : 44 --- Loss :   201.5564\n",
      "Epoch :  3 --batch : 45 --- Loss :   182.1756\n",
      "Epoch :  3 --batch : 46 --- Loss :   129.4820\n",
      "Epoch :  3 --batch : 47 --- Loss :   196.5556\n",
      "Epoch :  3 --batch : 48 --- Loss :   258.9591\n",
      "Epoch :  3 --batch : 49 --- Loss :   151.4945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  3 --batch : 50 --- Loss :   103.9193\n",
      "Epoch :  3 --batch : 51 --- Loss :   148.8828\n",
      "Epoch :  3 --batch : 52 --- Loss :   140.9299\n",
      "Epoch :  3 --batch : 53 --- Loss :   173.1594\n",
      "Epoch :  3 --batch : 54 --- Loss :   122.6744\n",
      "Epoch :  3 --batch : 55 --- Loss :   121.9677\n",
      "Epoch :  3 --batch : 56 --- Loss :   203.8123\n",
      "Epoch :  3 --batch : 57 --- Loss :   157.9661\n",
      "Epoch :  3 --batch : 58 --- Loss :   150.9348\n",
      "Epoch :  3 --batch : 59 --- Loss :    89.4984\n",
      "Epoch :  3 --batch : 60 --- Loss :   101.0655\n",
      "Epoch :  3 --batch : 61 --- Loss :   154.2438\n",
      "Epoch :  3 --batch : 62 --- Loss :   123.2178\n",
      "Epoch :  3 --batch : 63 --- Loss :   229.8693\n",
      "Epoch :  3 --batch : 64 --- Loss :   130.5238\n",
      "Testing Accuracy after epoch 3 : 0.891499999165535\n",
      "Epoch :  4 --batch : 1 --- Loss :   244.0604\n",
      "Epoch :  4 --batch : 2 --- Loss :   229.5084\n",
      "Epoch :  4 --batch : 3 --- Loss :   170.8950\n",
      "Epoch :  4 --batch : 4 --- Loss :   230.4446\n",
      "Epoch :  4 --batch : 5 --- Loss :   124.1886\n",
      "Epoch :  4 --batch : 6 --- Loss :   138.4099\n",
      "Epoch :  4 --batch : 7 --- Loss :   151.5465\n",
      "Epoch :  4 --batch : 8 --- Loss :   103.2508\n",
      "Epoch :  4 --batch : 9 --- Loss :   173.8358\n",
      "Epoch :  4 --batch : 10 --- Loss :   183.1448\n",
      "Epoch :  4 --batch : 11 --- Loss :   110.0220\n",
      "Epoch :  4 --batch : 12 --- Loss :   192.3617\n",
      "Epoch :  4 --batch : 13 --- Loss :   106.5463\n",
      "Epoch :  4 --batch : 14 --- Loss :   157.2517\n",
      "Epoch :  4 --batch : 15 --- Loss :    97.5942\n",
      "Epoch :  4 --batch : 16 --- Loss :    76.2551\n",
      "Epoch :  4 --batch : 17 --- Loss :   122.3574\n",
      "Epoch :  4 --batch : 18 --- Loss :   169.2291\n",
      "Epoch :  4 --batch : 19 --- Loss :   149.8578\n",
      "Epoch :  4 --batch : 20 --- Loss :   134.8328\n",
      "Epoch :  4 --batch : 21 --- Loss :   165.2705\n",
      "Epoch :  4 --batch : 22 --- Loss :   110.2323\n",
      "Epoch :  4 --batch : 23 --- Loss :   110.7770\n",
      "Epoch :  4 --batch : 24 --- Loss :   180.9815\n",
      "Epoch :  4 --batch : 25 --- Loss :   214.3312\n",
      "Epoch :  4 --batch : 26 --- Loss :   160.7612\n",
      "Epoch :  4 --batch : 27 --- Loss :   181.9438\n",
      "Epoch :  4 --batch : 28 --- Loss :   144.6867\n",
      "Epoch :  4 --batch : 29 --- Loss :    98.1166\n",
      "Epoch :  4 --batch : 30 --- Loss :   176.4585\n",
      "Epoch :  4 --batch : 31 --- Loss :   120.6435\n",
      "Epoch :  4 --batch : 32 --- Loss :    87.6033\n",
      "Epoch :  4 --batch : 33 --- Loss :    90.2064\n",
      "Epoch :  4 --batch : 34 --- Loss :    90.0286\n",
      "Epoch :  4 --batch : 35 --- Loss :   126.4156\n",
      "Epoch :  4 --batch : 36 --- Loss :   148.7696\n",
      "Epoch :  4 --batch : 37 --- Loss :   146.2698\n",
      "Epoch :  4 --batch : 38 --- Loss :   148.2581\n",
      "Epoch :  4 --batch : 39 --- Loss :   120.6593\n",
      "Epoch :  4 --batch : 40 --- Loss :    94.2136\n",
      "Epoch :  4 --batch : 41 --- Loss :   193.0798\n",
      "Epoch :  4 --batch : 42 --- Loss :   164.9594\n",
      "Epoch :  4 --batch : 43 --- Loss :    64.1535\n",
      "Epoch :  4 --batch : 44 --- Loss :   152.2940\n",
      "Epoch :  4 --batch : 45 --- Loss :   130.0154\n",
      "Epoch :  4 --batch : 46 --- Loss :    91.5210\n",
      "Epoch :  4 --batch : 47 --- Loss :   153.2105\n",
      "Epoch :  4 --batch : 48 --- Loss :   157.8566\n",
      "Epoch :  4 --batch : 49 --- Loss :    99.0801\n",
      "Epoch :  4 --batch : 50 --- Loss :    94.6270\n",
      "Epoch :  4 --batch : 51 --- Loss :   104.6846\n",
      "Epoch :  4 --batch : 52 --- Loss :   122.8086\n",
      "Epoch :  4 --batch : 53 --- Loss :   138.9788\n",
      "Epoch :  4 --batch : 54 --- Loss :   117.5238\n",
      "Epoch :  4 --batch : 55 --- Loss :    91.8986\n",
      "Epoch :  4 --batch : 56 --- Loss :    93.1378\n",
      "Epoch :  4 --batch : 57 --- Loss :   101.4658\n",
      "Epoch :  4 --batch : 58 --- Loss :   111.6460\n",
      "Epoch :  4 --batch : 59 --- Loss :    61.4078\n",
      "Epoch :  4 --batch : 60 --- Loss :    79.0346\n",
      "Epoch :  4 --batch : 61 --- Loss :   125.0907\n",
      "Epoch :  4 --batch : 62 --- Loss :    92.2589\n",
      "Epoch :  4 --batch : 63 --- Loss :   169.7861\n",
      "Epoch :  4 --batch : 64 --- Loss :   120.7686\n",
      "Testing Accuracy after epoch 4 : 0.8929999977350235\n",
      "Epoch :  5 --batch : 1 --- Loss :   193.3878\n",
      "Epoch :  5 --batch : 2 --- Loss :   140.6471\n",
      "Epoch :  5 --batch : 3 --- Loss :   115.6686\n",
      "Epoch :  5 --batch : 4 --- Loss :   177.5569\n",
      "Epoch :  5 --batch : 5 --- Loss :   104.1486\n",
      "Epoch :  5 --batch : 6 --- Loss :   150.9966\n",
      "Epoch :  5 --batch : 7 --- Loss :   126.6222\n",
      "Epoch :  5 --batch : 8 --- Loss :   110.0023\n",
      "Epoch :  5 --batch : 9 --- Loss :   113.0661\n",
      "Epoch :  5 --batch : 10 --- Loss :   152.2904\n",
      "Epoch :  5 --batch : 11 --- Loss :    70.0328\n",
      "Epoch :  5 --batch : 12 --- Loss :   127.7196\n",
      "Epoch :  5 --batch : 13 --- Loss :    67.4333\n",
      "Epoch :  5 --batch : 14 --- Loss :   112.9847\n",
      "Epoch :  5 --batch : 15 --- Loss :   100.3828\n",
      "Epoch :  5 --batch : 16 --- Loss :    68.5814\n",
      "Epoch :  5 --batch : 17 --- Loss :   111.1211\n",
      "Epoch :  5 --batch : 18 --- Loss :   104.9049\n",
      "Epoch :  5 --batch : 19 --- Loss :   127.1106\n",
      "Epoch :  5 --batch : 20 --- Loss :    94.4733\n",
      "Epoch :  5 --batch : 21 --- Loss :    98.5077\n",
      "Epoch :  5 --batch : 22 --- Loss :    63.7682\n",
      "Epoch :  5 --batch : 23 --- Loss :    49.6454\n",
      "Epoch :  5 --batch : 24 --- Loss :   103.0371\n",
      "Epoch :  5 --batch : 25 --- Loss :   110.0888\n",
      "Epoch :  5 --batch : 26 --- Loss :   104.9153\n",
      "Epoch :  5 --batch : 27 --- Loss :   115.5627\n",
      "Epoch :  5 --batch : 28 --- Loss :    74.3988\n",
      "Epoch :  5 --batch : 29 --- Loss :    88.7657\n",
      "Epoch :  5 --batch : 30 --- Loss :   126.2138\n",
      "Epoch :  5 --batch : 31 --- Loss :   113.9078\n",
      "Epoch :  5 --batch : 32 --- Loss :    66.8744\n",
      "Epoch :  5 --batch : 33 --- Loss :    71.2722\n",
      "Epoch :  5 --batch : 34 --- Loss :    65.2220\n",
      "Epoch :  5 --batch : 35 --- Loss :    85.9450\n",
      "Epoch :  5 --batch : 36 --- Loss :   112.5112\n",
      "Epoch :  5 --batch : 37 --- Loss :   103.5835\n",
      "Epoch :  5 --batch : 38 --- Loss :   103.6851\n",
      "Epoch :  5 --batch : 39 --- Loss :    79.6472\n",
      "Epoch :  5 --batch : 40 --- Loss :    70.5821\n",
      "Epoch :  5 --batch : 41 --- Loss :   152.6691\n",
      "Epoch :  5 --batch : 42 --- Loss :   125.3112\n",
      "Epoch :  5 --batch : 43 --- Loss :    67.2475\n",
      "Epoch :  5 --batch : 44 --- Loss :   108.8379\n",
      "Epoch :  5 --batch : 45 --- Loss :    69.2901\n",
      "Epoch :  5 --batch : 46 --- Loss :    81.5374\n",
      "Epoch :  5 --batch : 47 --- Loss :    92.9297\n",
      "Epoch :  5 --batch : 48 --- Loss :   104.4657\n",
      "Epoch :  5 --batch : 49 --- Loss :    44.1150\n",
      "Epoch :  5 --batch : 50 --- Loss :    32.3638\n",
      "Epoch :  5 --batch : 51 --- Loss :    44.5535\n",
      "Epoch :  5 --batch : 52 --- Loss :    86.4719\n",
      "Epoch :  5 --batch : 53 --- Loss :    97.8794\n",
      "Epoch :  5 --batch : 54 --- Loss :    56.6037\n",
      "Epoch :  5 --batch : 55 --- Loss :    32.5020\n",
      "Epoch :  5 --batch : 56 --- Loss :    67.0651\n",
      "Epoch :  5 --batch : 57 --- Loss :    66.3255\n",
      "Epoch :  5 --batch : 58 --- Loss :    98.7631\n",
      "Epoch :  5 --batch : 59 --- Loss :    51.2682\n",
      "Epoch :  5 --batch : 60 --- Loss :    68.8289\n",
      "Epoch :  5 --batch : 61 --- Loss :    72.4431\n",
      "Epoch :  5 --batch : 62 --- Loss :    52.6582\n",
      "Epoch :  5 --batch : 63 --- Loss :   107.0442\n",
      "Epoch :  5 --batch : 64 --- Loss :    60.4627\n",
      "Testing Accuracy after epoch 5 : 0.9010000050067901\n",
      "Epoch :  6 --batch : 1 --- Loss :   134.2064\n",
      "Epoch :  6 --batch : 2 --- Loss :   102.0274\n",
      "Epoch :  6 --batch : 3 --- Loss :    92.4271\n",
      "Epoch :  6 --batch : 4 --- Loss :   119.4332\n",
      "Epoch :  6 --batch : 5 --- Loss :    99.9043\n",
      "Epoch :  6 --batch : 6 --- Loss :    80.5805\n",
      "Epoch :  6 --batch : 7 --- Loss :    87.5121\n",
      "Epoch :  6 --batch : 8 --- Loss :    64.1064\n",
      "Epoch :  6 --batch : 9 --- Loss :    74.9633\n",
      "Epoch :  6 --batch : 10 --- Loss :    98.8172\n",
      "Epoch :  6 --batch : 11 --- Loss :    63.2284\n",
      "Epoch :  6 --batch : 12 --- Loss :   107.7832\n",
      "Epoch :  6 --batch : 13 --- Loss :    42.2017\n",
      "Epoch :  6 --batch : 14 --- Loss :    83.1895\n",
      "Epoch :  6 --batch : 15 --- Loss :    59.0862\n",
      "Epoch :  6 --batch : 16 --- Loss :    44.3240\n",
      "Epoch :  6 --batch : 17 --- Loss :    67.1668\n",
      "Epoch :  6 --batch : 18 --- Loss :    72.5106\n",
      "Epoch :  6 --batch : 19 --- Loss :    71.9581\n",
      "Epoch :  6 --batch : 20 --- Loss :    76.2673\n",
      "Epoch :  6 --batch : 21 --- Loss :   107.4867\n",
      "Epoch :  6 --batch : 22 --- Loss :    62.5895\n",
      "Epoch :  6 --batch : 23 --- Loss :    46.3331\n",
      "Epoch :  6 --batch : 24 --- Loss :    74.7676\n",
      "Epoch :  6 --batch : 25 --- Loss :   112.4827\n",
      "Epoch :  6 --batch : 26 --- Loss :   109.3388\n",
      "Epoch :  6 --batch : 27 --- Loss :    99.0827\n",
      "Epoch :  6 --batch : 28 --- Loss :   101.4556\n",
      "Epoch :  6 --batch : 29 --- Loss :    47.1877\n",
      "Epoch :  6 --batch : 30 --- Loss :    50.1626\n",
      "Epoch :  6 --batch : 31 --- Loss :    67.2200\n",
      "Epoch :  6 --batch : 32 --- Loss :    53.4288\n",
      "Epoch :  6 --batch : 33 --- Loss :    30.3483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  6 --batch : 34 --- Loss :    46.1000\n",
      "Epoch :  6 --batch : 35 --- Loss :    25.7924\n",
      "Epoch :  6 --batch : 36 --- Loss :    70.7742\n",
      "Epoch :  6 --batch : 37 --- Loss :    73.7456\n",
      "Epoch :  6 --batch : 38 --- Loss :    61.6466\n",
      "Epoch :  6 --batch : 39 --- Loss :    45.5113\n",
      "Epoch :  6 --batch : 40 --- Loss :    53.3570\n",
      "Epoch :  6 --batch : 41 --- Loss :    76.9157\n",
      "Epoch :  6 --batch : 42 --- Loss :    58.2873\n",
      "Epoch :  6 --batch : 43 --- Loss :    40.3445\n",
      "Epoch :  6 --batch : 44 --- Loss :    70.5060\n",
      "Epoch :  6 --batch : 45 --- Loss :    50.8888\n",
      "Epoch :  6 --batch : 46 --- Loss :    62.6941\n",
      "Epoch :  6 --batch : 47 --- Loss :    60.4961\n",
      "Epoch :  6 --batch : 48 --- Loss :    72.3237\n",
      "Epoch :  6 --batch : 49 --- Loss :    35.7877\n",
      "Epoch :  6 --batch : 50 --- Loss :    21.0121\n",
      "Epoch :  6 --batch : 51 --- Loss :    25.0647\n",
      "Epoch :  6 --batch : 52 --- Loss :    49.8112\n",
      "Epoch :  6 --batch : 53 --- Loss :   121.8275\n",
      "Epoch :  6 --batch : 54 --- Loss :    48.5026\n",
      "Epoch :  6 --batch : 55 --- Loss :    33.4900\n",
      "Epoch :  6 --batch : 56 --- Loss :    62.0921\n",
      "Epoch :  6 --batch : 57 --- Loss :    49.7826\n",
      "Epoch :  6 --batch : 58 --- Loss :    92.6446\n",
      "Epoch :  6 --batch : 59 --- Loss :    39.8035\n",
      "Epoch :  6 --batch : 60 --- Loss :    62.7392\n",
      "Epoch :  6 --batch : 61 --- Loss :    61.5228\n",
      "Epoch :  6 --batch : 62 --- Loss :    52.9667\n",
      "Epoch :  6 --batch : 63 --- Loss :    69.6624\n",
      "Epoch :  6 --batch : 64 --- Loss :    35.7696\n",
      "Testing Accuracy after epoch 6 : 0.9330000013113022\n",
      "Epoch :  7 --batch : 1 --- Loss :   134.2143\n",
      "Epoch :  7 --batch : 2 --- Loss :    92.4505\n",
      "Epoch :  7 --batch : 3 --- Loss :    62.2018\n",
      "Epoch :  7 --batch : 4 --- Loss :    67.5398\n",
      "Epoch :  7 --batch : 5 --- Loss :    44.6112\n",
      "Epoch :  7 --batch : 6 --- Loss :    53.1385\n",
      "Epoch :  7 --batch : 7 --- Loss :    55.9209\n",
      "Epoch :  7 --batch : 8 --- Loss :    36.3445\n",
      "Epoch :  7 --batch : 9 --- Loss :    47.3833\n",
      "Epoch :  7 --batch : 10 --- Loss :    43.2007\n",
      "Epoch :  7 --batch : 11 --- Loss :    38.8115\n",
      "Epoch :  7 --batch : 12 --- Loss :    75.4012\n",
      "Epoch :  7 --batch : 13 --- Loss :    25.1796\n",
      "Epoch :  7 --batch : 14 --- Loss :    55.1747\n",
      "Epoch :  7 --batch : 15 --- Loss :    36.0004\n",
      "Epoch :  7 --batch : 16 --- Loss :    34.6203\n",
      "Epoch :  7 --batch : 17 --- Loss :    87.3583\n",
      "Epoch :  7 --batch : 18 --- Loss :    58.7401\n",
      "Epoch :  7 --batch : 19 --- Loss :    52.6962\n",
      "Epoch :  7 --batch : 20 --- Loss :    51.1806\n",
      "Epoch :  7 --batch : 21 --- Loss :    91.4907\n",
      "Epoch :  7 --batch : 22 --- Loss :    43.2960\n",
      "Epoch :  7 --batch : 23 --- Loss :    42.0751\n",
      "Epoch :  7 --batch : 24 --- Loss :    64.6848\n",
      "Epoch :  7 --batch : 25 --- Loss :    36.7576\n",
      "Epoch :  7 --batch : 26 --- Loss :    46.4921\n",
      "Epoch :  7 --batch : 27 --- Loss :    72.9736\n",
      "Epoch :  7 --batch : 28 --- Loss :    36.8980\n",
      "Epoch :  7 --batch : 29 --- Loss :    39.2162\n",
      "Epoch :  7 --batch : 30 --- Loss :    30.0146\n",
      "Epoch :  7 --batch : 31 --- Loss :    65.8335\n",
      "Epoch :  7 --batch : 32 --- Loss :    37.7153\n",
      "Epoch :  7 --batch : 33 --- Loss :    35.9705\n",
      "Epoch :  7 --batch : 34 --- Loss :    41.0555\n",
      "Epoch :  7 --batch : 35 --- Loss :    32.1909\n",
      "Epoch :  7 --batch : 36 --- Loss :    70.0457\n",
      "Epoch :  7 --batch : 37 --- Loss :    70.7320\n",
      "Epoch :  7 --batch : 38 --- Loss :    27.8188\n",
      "Epoch :  7 --batch : 39 --- Loss :    33.9163\n",
      "Epoch :  7 --batch : 40 --- Loss :    29.3627\n",
      "Epoch :  7 --batch : 41 --- Loss :    79.6243\n",
      "Epoch :  7 --batch : 42 --- Loss :    55.8100\n",
      "Epoch :  7 --batch : 43 --- Loss :    49.8817\n",
      "Epoch :  7 --batch : 44 --- Loss :    71.2400\n",
      "Epoch :  7 --batch : 45 --- Loss :    22.8141\n",
      "Epoch :  7 --batch : 46 --- Loss :    36.5468\n",
      "Epoch :  7 --batch : 47 --- Loss :    48.0537\n",
      "Epoch :  7 --batch : 48 --- Loss :    57.7854\n",
      "Epoch :  7 --batch : 49 --- Loss :    47.9950\n",
      "Epoch :  7 --batch : 50 --- Loss :    27.3937\n",
      "Epoch :  7 --batch : 51 --- Loss :    28.8525\n",
      "Epoch :  7 --batch : 52 --- Loss :    53.5301\n",
      "Epoch :  7 --batch : 53 --- Loss :    63.2765\n",
      "Epoch :  7 --batch : 54 --- Loss :    22.5563\n",
      "Epoch :  7 --batch : 55 --- Loss :    19.8179\n",
      "Epoch :  7 --batch : 56 --- Loss :    51.4641\n",
      "Epoch :  7 --batch : 57 --- Loss :    52.7387\n",
      "Epoch :  7 --batch : 58 --- Loss :    63.1966\n",
      "Epoch :  7 --batch : 59 --- Loss :    25.2603\n",
      "Epoch :  7 --batch : 60 --- Loss :    33.3522\n",
      "Epoch :  7 --batch : 61 --- Loss :    33.5643\n",
      "Epoch :  7 --batch : 62 --- Loss :    32.4278\n",
      "Epoch :  7 --batch : 63 --- Loss :    68.2766\n",
      "Epoch :  7 --batch : 64 --- Loss :    40.2505\n",
      "Testing Accuracy after epoch 7 : 0.9370000034570694\n",
      "Epoch :  8 --batch : 1 --- Loss :   117.3251\n",
      "Epoch :  8 --batch : 2 --- Loss :    72.8386\n",
      "Epoch :  8 --batch : 3 --- Loss :    37.9175\n",
      "Epoch :  8 --batch : 4 --- Loss :    44.9601\n",
      "Epoch :  8 --batch : 5 --- Loss :    45.3792\n",
      "Epoch :  8 --batch : 6 --- Loss :    52.9491\n",
      "Epoch :  8 --batch : 7 --- Loss :    54.1061\n",
      "Epoch :  8 --batch : 8 --- Loss :    30.4629\n",
      "Epoch :  8 --batch : 9 --- Loss :    25.5626\n",
      "Epoch :  8 --batch : 10 --- Loss :    41.3014\n",
      "Epoch :  8 --batch : 11 --- Loss :    40.9326\n",
      "Epoch :  8 --batch : 12 --- Loss :    57.2938\n",
      "Epoch :  8 --batch : 13 --- Loss :    24.2792\n",
      "Epoch :  8 --batch : 14 --- Loss :    40.5447\n",
      "Epoch :  8 --batch : 15 --- Loss :    42.3349\n",
      "Epoch :  8 --batch : 16 --- Loss :    24.2390\n",
      "Epoch :  8 --batch : 17 --- Loss :    62.0457\n",
      "Epoch :  8 --batch : 18 --- Loss :    60.4119\n",
      "Epoch :  8 --batch : 19 --- Loss :    60.3281\n",
      "Epoch :  8 --batch : 20 --- Loss :    63.6957\n",
      "Epoch :  8 --batch : 21 --- Loss :    64.5647\n",
      "Epoch :  8 --batch : 22 --- Loss :    35.1466\n",
      "Epoch :  8 --batch : 23 --- Loss :    33.0977\n",
      "Epoch :  8 --batch : 24 --- Loss :    44.6804\n",
      "Epoch :  8 --batch : 25 --- Loss :    44.2129\n",
      "Epoch :  8 --batch : 26 --- Loss :    40.6823\n",
      "Epoch :  8 --batch : 27 --- Loss :    33.7074\n",
      "Epoch :  8 --batch : 28 --- Loss :    31.3763\n",
      "Epoch :  8 --batch : 29 --- Loss :    27.9363\n",
      "Epoch :  8 --batch : 30 --- Loss :    19.8764\n",
      "Epoch :  8 --batch : 31 --- Loss :    56.2782\n",
      "Epoch :  8 --batch : 32 --- Loss :    37.2271\n",
      "Epoch :  8 --batch : 33 --- Loss :    22.4189\n",
      "Epoch :  8 --batch : 34 --- Loss :    29.3976\n",
      "Epoch :  8 --batch : 35 --- Loss :    13.7902\n",
      "Epoch :  8 --batch : 36 --- Loss :    32.1225\n",
      "Epoch :  8 --batch : 37 --- Loss :    51.6086\n",
      "Epoch :  8 --batch : 38 --- Loss :    35.5558\n",
      "Epoch :  8 --batch : 39 --- Loss :    19.4735\n",
      "Epoch :  8 --batch : 40 --- Loss :    18.3155\n",
      "Epoch :  8 --batch : 41 --- Loss :    56.5685\n",
      "Epoch :  8 --batch : 42 --- Loss :    35.4173\n",
      "Epoch :  8 --batch : 43 --- Loss :    35.1850\n",
      "Epoch :  8 --batch : 44 --- Loss :    57.0134\n",
      "Epoch :  8 --batch : 45 --- Loss :    25.2089\n",
      "Epoch :  8 --batch : 46 --- Loss :    31.3105\n",
      "Epoch :  8 --batch : 47 --- Loss :    34.8765\n",
      "Epoch :  8 --batch : 48 --- Loss :    45.4906\n",
      "Epoch :  8 --batch : 49 --- Loss :    24.7801\n",
      "Epoch :  8 --batch : 50 --- Loss :    24.7996\n",
      "Epoch :  8 --batch : 51 --- Loss :    25.9626\n",
      "Epoch :  8 --batch : 52 --- Loss :    38.3821\n",
      "Epoch :  8 --batch : 53 --- Loss :    44.5250\n",
      "Epoch :  8 --batch : 54 --- Loss :    30.5255\n",
      "Epoch :  8 --batch : 55 --- Loss :    27.6394\n",
      "Epoch :  8 --batch : 56 --- Loss :    48.9324\n",
      "Epoch :  8 --batch : 57 --- Loss :    44.5312\n",
      "Epoch :  8 --batch : 58 --- Loss :    62.3349\n",
      "Epoch :  8 --batch : 59 --- Loss :    29.5447\n",
      "Epoch :  8 --batch : 60 --- Loss :    27.2806\n",
      "Epoch :  8 --batch : 61 --- Loss :    20.2359\n",
      "Epoch :  8 --batch : 62 --- Loss :    25.6238\n",
      "Epoch :  8 --batch : 63 --- Loss :    36.7336\n",
      "Epoch :  8 --batch : 64 --- Loss :    25.5618\n",
      "Testing Accuracy after epoch 8 : 0.9520000010728836\n",
      "Epoch :  9 --batch : 1 --- Loss :   116.3661\n",
      "Epoch :  9 --batch : 2 --- Loss :    57.2808\n",
      "Epoch :  9 --batch : 3 --- Loss :    26.8569\n",
      "Epoch :  9 --batch : 4 --- Loss :    43.2203\n",
      "Epoch :  9 --batch : 5 --- Loss :    37.0981\n",
      "Epoch :  9 --batch : 6 --- Loss :    49.7038\n",
      "Epoch :  9 --batch : 7 --- Loss :    43.6193\n",
      "Epoch :  9 --batch : 8 --- Loss :    21.7291\n",
      "Epoch :  9 --batch : 9 --- Loss :    19.0540\n",
      "Epoch :  9 --batch : 10 --- Loss :    33.0139\n",
      "Epoch :  9 --batch : 11 --- Loss :    31.7570\n",
      "Epoch :  9 --batch : 12 --- Loss :    50.4495\n",
      "Epoch :  9 --batch : 13 --- Loss :    12.1610\n",
      "Epoch :  9 --batch : 14 --- Loss :    34.4085\n",
      "Epoch :  9 --batch : 15 --- Loss :    31.3523\n",
      "Epoch :  9 --batch : 16 --- Loss :    24.8256\n",
      "Epoch :  9 --batch : 17 --- Loss :    51.0319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  9 --batch : 18 --- Loss :    50.3489\n",
      "Epoch :  9 --batch : 19 --- Loss :    56.0146\n",
      "Epoch :  9 --batch : 20 --- Loss :    50.0206\n",
      "Epoch :  9 --batch : 21 --- Loss :    66.3080\n",
      "Epoch :  9 --batch : 22 --- Loss :    37.1684\n",
      "Epoch :  9 --batch : 23 --- Loss :    33.5964\n",
      "Epoch :  9 --batch : 24 --- Loss :    56.0866\n",
      "Epoch :  9 --batch : 25 --- Loss :    31.7812\n",
      "Epoch :  9 --batch : 26 --- Loss :    35.8332\n",
      "Epoch :  9 --batch : 27 --- Loss :    42.7052\n",
      "Epoch :  9 --batch : 28 --- Loss :    35.6516\n",
      "Epoch :  9 --batch : 29 --- Loss :    25.9004\n",
      "Epoch :  9 --batch : 30 --- Loss :    18.0213\n",
      "Epoch :  9 --batch : 31 --- Loss :    49.0523\n",
      "Epoch :  9 --batch : 32 --- Loss :    27.5188\n",
      "Epoch :  9 --batch : 33 --- Loss :    20.0493\n",
      "Epoch :  9 --batch : 34 --- Loss :    19.5399\n",
      "Epoch :  9 --batch : 35 --- Loss :    20.4194\n",
      "Epoch :  9 --batch : 36 --- Loss :    44.4429\n",
      "Epoch :  9 --batch : 37 --- Loss :    40.4728\n",
      "Epoch :  9 --batch : 38 --- Loss :    24.5212\n",
      "Epoch :  9 --batch : 39 --- Loss :    34.6487\n",
      "Epoch :  9 --batch : 40 --- Loss :    13.3616\n",
      "Epoch :  9 --batch : 41 --- Loss :    62.7113\n",
      "Epoch :  9 --batch : 42 --- Loss :    26.3443\n",
      "Epoch :  9 --batch : 43 --- Loss :    23.4569\n",
      "Epoch :  9 --batch : 44 --- Loss :    45.2069\n",
      "Epoch :  9 --batch : 45 --- Loss :    10.6399\n",
      "Epoch :  9 --batch : 46 --- Loss :    27.5487\n",
      "Epoch :  9 --batch : 47 --- Loss :    40.3019\n",
      "Epoch :  9 --batch : 48 --- Loss :    39.3086\n",
      "Epoch :  9 --batch : 49 --- Loss :    23.2813\n",
      "Epoch :  9 --batch : 50 --- Loss :    20.5553\n",
      "Epoch :  9 --batch : 51 --- Loss :    25.4600\n",
      "Epoch :  9 --batch : 52 --- Loss :    39.8730\n",
      "Epoch :  9 --batch : 53 --- Loss :    32.0326\n",
      "Epoch :  9 --batch : 54 --- Loss :    23.1286\n",
      "Epoch :  9 --batch : 55 --- Loss :    16.9432\n",
      "Epoch :  9 --batch : 56 --- Loss :    38.2318\n",
      "Epoch :  9 --batch : 57 --- Loss :    31.5192\n",
      "Epoch :  9 --batch : 58 --- Loss :    50.7841\n",
      "Epoch :  9 --batch : 59 --- Loss :    13.4441\n",
      "Epoch :  9 --batch : 60 --- Loss :    23.6316\n",
      "Epoch :  9 --batch : 61 --- Loss :    22.2246\n",
      "Epoch :  9 --batch : 62 --- Loss :    19.2244\n",
      "Epoch :  9 --batch : 63 --- Loss :    34.6966\n",
      "Epoch :  9 --batch : 64 --- Loss :    31.9922\n",
      "Testing Accuracy after epoch 9 : 0.9505000054836273\n",
      "Epoch : 10 --batch : 1 --- Loss :   113.1626\n",
      "Epoch : 10 --batch : 2 --- Loss :    63.0301\n",
      "Epoch : 10 --batch : 3 --- Loss :    32.6658\n",
      "Epoch : 10 --batch : 4 --- Loss :    45.6037\n",
      "Epoch : 10 --batch : 5 --- Loss :    36.4970\n",
      "Epoch : 10 --batch : 6 --- Loss :    55.1064\n",
      "Epoch : 10 --batch : 7 --- Loss :    39.4395\n",
      "Epoch : 10 --batch : 8 --- Loss :    21.9546\n",
      "Epoch : 10 --batch : 9 --- Loss :     9.8117\n",
      "Epoch : 10 --batch : 10 --- Loss :    34.8125\n",
      "Epoch : 10 --batch : 11 --- Loss :    37.2855\n",
      "Epoch : 10 --batch : 12 --- Loss :    43.7603\n",
      "Epoch : 10 --batch : 13 --- Loss :    11.0397\n",
      "Epoch : 10 --batch : 14 --- Loss :    33.1379\n",
      "Epoch : 10 --batch : 15 --- Loss :    25.7175\n",
      "Epoch : 10 --batch : 16 --- Loss :    18.5969\n",
      "Epoch : 10 --batch : 17 --- Loss :    42.1543\n",
      "Epoch : 10 --batch : 18 --- Loss :    29.5969\n",
      "Epoch : 10 --batch : 19 --- Loss :    45.5822\n",
      "Epoch : 10 --batch : 20 --- Loss :    29.4643\n",
      "Epoch : 10 --batch : 21 --- Loss :    54.6618\n",
      "Epoch : 10 --batch : 22 --- Loss :    24.6330\n",
      "Epoch : 10 --batch : 23 --- Loss :    34.9441\n",
      "Epoch : 10 --batch : 24 --- Loss :    60.8224\n",
      "Epoch : 10 --batch : 25 --- Loss :    26.5354\n",
      "Epoch : 10 --batch : 26 --- Loss :    37.5150\n",
      "Epoch : 10 --batch : 27 --- Loss :    30.5499\n",
      "Epoch : 10 --batch : 28 --- Loss :    21.2804\n",
      "Epoch : 10 --batch : 29 --- Loss :    26.5731\n",
      "Epoch : 10 --batch : 30 --- Loss :    18.3872\n",
      "Epoch : 10 --batch : 31 --- Loss :    49.8089\n",
      "Epoch : 10 --batch : 32 --- Loss :    31.2190\n",
      "Epoch : 10 --batch : 33 --- Loss :    19.7520\n",
      "Epoch : 10 --batch : 34 --- Loss :    15.0507\n",
      "Epoch : 10 --batch : 35 --- Loss :    17.6956\n",
      "Epoch : 10 --batch : 36 --- Loss :    20.1677\n",
      "Epoch : 10 --batch : 37 --- Loss :    39.4817\n",
      "Epoch : 10 --batch : 38 --- Loss :    13.5145\n",
      "Epoch : 10 --batch : 39 --- Loss :    17.7927\n",
      "Epoch : 10 --batch : 40 --- Loss :    10.1930\n",
      "Epoch : 10 --batch : 41 --- Loss :    55.0630\n",
      "Epoch : 10 --batch : 42 --- Loss :    24.2625\n",
      "Epoch : 10 --batch : 43 --- Loss :    22.0946\n",
      "Epoch : 10 --batch : 44 --- Loss :    44.3753\n",
      "Epoch : 10 --batch : 45 --- Loss :    13.6789\n",
      "Epoch : 10 --batch : 46 --- Loss :    29.9056\n",
      "Epoch : 10 --batch : 47 --- Loss :    19.4356\n",
      "Epoch : 10 --batch : 48 --- Loss :    28.2896\n",
      "Epoch : 10 --batch : 49 --- Loss :    22.1980\n",
      "Epoch : 10 --batch : 50 --- Loss :    20.2408\n",
      "Epoch : 10 --batch : 51 --- Loss :    25.6245\n",
      "Epoch : 10 --batch : 52 --- Loss :    39.1842\n",
      "Epoch : 10 --batch : 53 --- Loss :    28.2351\n",
      "Epoch : 10 --batch : 54 --- Loss :    17.1024\n",
      "Epoch : 10 --batch : 55 --- Loss :    16.7990\n",
      "Epoch : 10 --batch : 56 --- Loss :    42.7578\n",
      "Epoch : 10 --batch : 57 --- Loss :    27.9347\n",
      "Epoch : 10 --batch : 58 --- Loss :    60.4075\n",
      "Epoch : 10 --batch : 59 --- Loss :    12.0937\n",
      "Epoch : 10 --batch : 60 --- Loss :    16.9309\n",
      "Epoch : 10 --batch : 61 --- Loss :    14.2267\n",
      "Epoch : 10 --batch : 62 --- Loss :    22.5697\n",
      "Epoch : 10 --batch : 63 --- Loss :    27.7646\n",
      "Epoch : 10 --batch : 64 --- Loss :    34.1666\n",
      "Testing Accuracy after epoch 10 : 0.9485000044107437\n",
      "Epoch : 11 --batch : 1 --- Loss :   111.0010\n",
      "Epoch : 11 --batch : 2 --- Loss :    49.7346\n",
      "Epoch : 11 --batch : 3 --- Loss :    33.5432\n",
      "Epoch : 11 --batch : 4 --- Loss :    36.7257\n",
      "Epoch : 11 --batch : 5 --- Loss :    33.3753\n",
      "Epoch : 11 --batch : 6 --- Loss :    46.0468\n",
      "Epoch : 11 --batch : 7 --- Loss :    33.1547\n",
      "Epoch : 11 --batch : 8 --- Loss :    15.2225\n",
      "Epoch : 11 --batch : 9 --- Loss :    11.3751\n",
      "Epoch : 11 --batch : 10 --- Loss :    30.1904\n",
      "Epoch : 11 --batch : 11 --- Loss :    30.9084\n",
      "Epoch : 11 --batch : 12 --- Loss :    42.5693\n",
      "Epoch : 11 --batch : 13 --- Loss :    12.1351\n",
      "Epoch : 11 --batch : 14 --- Loss :    33.0692\n",
      "Epoch : 11 --batch : 15 --- Loss :    32.8755\n",
      "Epoch : 11 --batch : 16 --- Loss :    21.8166\n",
      "Epoch : 11 --batch : 17 --- Loss :    48.0052\n",
      "Epoch : 11 --batch : 18 --- Loss :    40.9062\n",
      "Epoch : 11 --batch : 19 --- Loss :    47.9955\n",
      "Epoch : 11 --batch : 20 --- Loss :    31.6237\n",
      "Epoch : 11 --batch : 21 --- Loss :    55.5447\n",
      "Epoch : 11 --batch : 22 --- Loss :    22.7174\n",
      "Epoch : 11 --batch : 23 --- Loss :    27.5301\n",
      "Epoch : 11 --batch : 24 --- Loss :    50.3326\n",
      "Epoch : 11 --batch : 25 --- Loss :    31.8922\n",
      "Epoch : 11 --batch : 26 --- Loss :    39.5192\n",
      "Epoch : 11 --batch : 27 --- Loss :    41.9602\n",
      "Epoch : 11 --batch : 28 --- Loss :    26.3282\n",
      "Epoch : 11 --batch : 29 --- Loss :    22.3611\n",
      "Epoch : 11 --batch : 30 --- Loss :    19.4740\n",
      "Epoch : 11 --batch : 31 --- Loss :    41.0323\n",
      "Epoch : 11 --batch : 32 --- Loss :    24.5391\n",
      "Epoch : 11 --batch : 33 --- Loss :    15.6871\n",
      "Epoch : 11 --batch : 34 --- Loss :    12.5262\n",
      "Epoch : 11 --batch : 35 --- Loss :    16.5184\n",
      "Epoch : 11 --batch : 36 --- Loss :    11.9387\n",
      "Epoch : 11 --batch : 37 --- Loss :    39.4590\n",
      "Epoch : 11 --batch : 38 --- Loss :    13.8795\n",
      "Epoch : 11 --batch : 39 --- Loss :    14.5214\n",
      "Epoch : 11 --batch : 40 --- Loss :    11.3737\n",
      "Epoch : 11 --batch : 41 --- Loss :    54.4397\n",
      "Epoch : 11 --batch : 42 --- Loss :    20.2493\n",
      "Epoch : 11 --batch : 43 --- Loss :    20.6740\n",
      "Epoch : 11 --batch : 44 --- Loss :    37.6546\n",
      "Epoch : 11 --batch : 45 --- Loss :    13.3893\n",
      "Epoch : 11 --batch : 46 --- Loss :    25.5308\n",
      "Epoch : 11 --batch : 47 --- Loss :    22.0990\n",
      "Epoch : 11 --batch : 48 --- Loss :    29.2872\n",
      "Epoch : 11 --batch : 49 --- Loss :    17.4009\n",
      "Epoch : 11 --batch : 50 --- Loss :    23.1518\n",
      "Epoch : 11 --batch : 51 --- Loss :    25.6814\n",
      "Epoch : 11 --batch : 52 --- Loss :    24.4356\n",
      "Epoch : 11 --batch : 53 --- Loss :    36.7369\n",
      "Epoch : 11 --batch : 54 --- Loss :    13.4098\n",
      "Epoch : 11 --batch : 55 --- Loss :    20.4931\n",
      "Epoch : 11 --batch : 56 --- Loss :    36.0556\n",
      "Epoch : 11 --batch : 57 --- Loss :    21.2752\n",
      "Epoch : 11 --batch : 58 --- Loss :    41.4877\n",
      "Epoch : 11 --batch : 59 --- Loss :    16.1771\n",
      "Epoch : 11 --batch : 60 --- Loss :    22.6079\n",
      "Epoch : 11 --batch : 61 --- Loss :    19.6610\n",
      "Epoch : 11 --batch : 62 --- Loss :    19.1026\n",
      "Epoch : 11 --batch : 63 --- Loss :    27.5176\n",
      "Epoch : 11 --batch : 64 --- Loss :    23.8299\n",
      "Testing Accuracy after epoch 11 : 0.95450000166893\n",
      "Epoch : 12 --batch : 1 --- Loss :   105.3195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 12 --batch : 2 --- Loss :    41.0283\n",
      "Epoch : 12 --batch : 3 --- Loss :    22.0141\n",
      "Epoch : 12 --batch : 4 --- Loss :    38.9280\n",
      "Epoch : 12 --batch : 5 --- Loss :    35.4495\n",
      "Epoch : 12 --batch : 6 --- Loss :    43.8487\n",
      "Epoch : 12 --batch : 7 --- Loss :    34.0771\n",
      "Epoch : 12 --batch : 8 --- Loss :    16.4884\n",
      "Epoch : 12 --batch : 9 --- Loss :    10.4192\n",
      "Epoch : 12 --batch : 10 --- Loss :    26.5997\n",
      "Epoch : 12 --batch : 11 --- Loss :    31.2943\n",
      "Epoch : 12 --batch : 12 --- Loss :    41.0804\n",
      "Epoch : 12 --batch : 13 --- Loss :    10.3539\n",
      "Epoch : 12 --batch : 14 --- Loss :    30.2175\n",
      "Epoch : 12 --batch : 15 --- Loss :    25.6306\n",
      "Epoch : 12 --batch : 16 --- Loss :    12.2258\n",
      "Epoch : 12 --batch : 17 --- Loss :    33.0541\n",
      "Epoch : 12 --batch : 18 --- Loss :    29.6159\n",
      "Epoch : 12 --batch : 19 --- Loss :    44.4187\n",
      "Epoch : 12 --batch : 20 --- Loss :    34.2880\n",
      "Epoch : 12 --batch : 21 --- Loss :    44.4627\n",
      "Epoch : 12 --batch : 22 --- Loss :    26.1424\n",
      "Epoch : 12 --batch : 23 --- Loss :    22.5811\n",
      "Epoch : 12 --batch : 24 --- Loss :    50.7962\n",
      "Epoch : 12 --batch : 25 --- Loss :    21.9387\n",
      "Epoch : 12 --batch : 26 --- Loss :    23.3198\n",
      "Epoch : 12 --batch : 27 --- Loss :    27.5156\n",
      "Epoch : 12 --batch : 28 --- Loss :    35.9243\n",
      "Epoch : 12 --batch : 29 --- Loss :    19.8353\n",
      "Epoch : 12 --batch : 30 --- Loss :    16.5937\n",
      "Epoch : 12 --batch : 31 --- Loss :    34.7979\n",
      "Epoch : 12 --batch : 32 --- Loss :    21.3101\n",
      "Epoch : 12 --batch : 33 --- Loss :    14.5872\n",
      "Epoch : 12 --batch : 34 --- Loss :    11.7983\n",
      "Epoch : 12 --batch : 35 --- Loss :    13.4721\n",
      "Epoch : 12 --batch : 36 --- Loss :    27.6673\n",
      "Epoch : 12 --batch : 37 --- Loss :    35.0092\n",
      "Epoch : 12 --batch : 38 --- Loss :    12.6013\n",
      "Epoch : 12 --batch : 39 --- Loss :    20.2226\n",
      "Epoch : 12 --batch : 40 --- Loss :    12.4288\n",
      "Epoch : 12 --batch : 41 --- Loss :    57.1798\n",
      "Epoch : 12 --batch : 42 --- Loss :    26.7892\n",
      "Epoch : 12 --batch : 43 --- Loss :    21.0887\n",
      "Epoch : 12 --batch : 44 --- Loss :    42.7879\n",
      "Epoch : 12 --batch : 45 --- Loss :    10.5067\n",
      "Epoch : 12 --batch : 46 --- Loss :    22.5464\n",
      "Epoch : 12 --batch : 47 --- Loss :    20.0888\n",
      "Epoch : 12 --batch : 48 --- Loss :    25.1146\n",
      "Epoch : 12 --batch : 49 --- Loss :    13.2480\n",
      "Epoch : 12 --batch : 50 --- Loss :    18.6747\n",
      "Epoch : 12 --batch : 51 --- Loss :    22.4937\n",
      "Epoch : 12 --batch : 52 --- Loss :    25.0950\n",
      "Epoch : 12 --batch : 53 --- Loss :    26.6765\n",
      "Epoch : 12 --batch : 54 --- Loss :    14.7847\n",
      "Epoch : 12 --batch : 55 --- Loss :    12.8902\n",
      "Epoch : 12 --batch : 56 --- Loss :    31.3082\n",
      "Epoch : 12 --batch : 57 --- Loss :    18.5912\n",
      "Epoch : 12 --batch : 58 --- Loss :    32.6004\n",
      "Epoch : 12 --batch : 59 --- Loss :    11.2286\n",
      "Epoch : 12 --batch : 60 --- Loss :    18.6267\n",
      "Epoch : 12 --batch : 61 --- Loss :    22.4541\n",
      "Epoch : 12 --batch : 62 --- Loss :    22.3407\n",
      "Epoch : 12 --batch : 63 --- Loss :    26.7113\n",
      "Epoch : 12 --batch : 64 --- Loss :    23.8420\n",
      "Testing Accuracy after epoch 12 : 0.9555000066757202\n",
      "Epoch : 13 --batch : 1 --- Loss :    91.7472\n",
      "Epoch : 13 --batch : 2 --- Loss :    27.7630\n",
      "Epoch : 13 --batch : 3 --- Loss :    24.6345\n",
      "Epoch : 13 --batch : 4 --- Loss :    38.6023\n",
      "Epoch : 13 --batch : 5 --- Loss :    28.0404\n",
      "Epoch : 13 --batch : 6 --- Loss :    41.3020\n",
      "Epoch : 13 --batch : 7 --- Loss :    27.5732\n",
      "Epoch : 13 --batch : 8 --- Loss :    10.5348\n",
      "Epoch : 13 --batch : 9 --- Loss :    15.9538\n",
      "Epoch : 13 --batch : 10 --- Loss :    23.2170\n",
      "Epoch : 13 --batch : 11 --- Loss :    26.7904\n",
      "Epoch : 13 --batch : 12 --- Loss :    45.0540\n",
      "Epoch : 13 --batch : 13 --- Loss :     5.5589\n",
      "Epoch : 13 --batch : 14 --- Loss :    27.6607\n",
      "Epoch : 13 --batch : 15 --- Loss :    30.3344\n",
      "Epoch : 13 --batch : 16 --- Loss :    19.9560\n",
      "Epoch : 13 --batch : 17 --- Loss :    46.0191\n",
      "Epoch : 13 --batch : 18 --- Loss :    25.5492\n",
      "Epoch : 13 --batch : 19 --- Loss :    40.7653\n",
      "Epoch : 13 --batch : 20 --- Loss :    32.8964\n",
      "Epoch : 13 --batch : 21 --- Loss :    41.3156\n",
      "Epoch : 13 --batch : 22 --- Loss :    20.1235\n",
      "Epoch : 13 --batch : 23 --- Loss :    21.0171\n",
      "Epoch : 13 --batch : 24 --- Loss :    45.6507\n",
      "Epoch : 13 --batch : 25 --- Loss :    19.1466\n",
      "Epoch : 13 --batch : 26 --- Loss :    21.4541\n",
      "Epoch : 13 --batch : 27 --- Loss :    33.3511\n",
      "Epoch : 13 --batch : 28 --- Loss :    22.7160\n",
      "Epoch : 13 --batch : 29 --- Loss :    20.7010\n",
      "Epoch : 13 --batch : 30 --- Loss :    17.5637\n",
      "Epoch : 13 --batch : 31 --- Loss :    36.1704\n",
      "Epoch : 13 --batch : 32 --- Loss :    23.3913\n",
      "Epoch : 13 --batch : 33 --- Loss :    14.5091\n",
      "Epoch : 13 --batch : 34 --- Loss :    10.0803\n",
      "Epoch : 13 --batch : 35 --- Loss :    13.4861\n",
      "Epoch : 13 --batch : 36 --- Loss :    12.2250\n",
      "Epoch : 13 --batch : 37 --- Loss :    28.9149\n",
      "Epoch : 13 --batch : 38 --- Loss :    12.8189\n",
      "Epoch : 13 --batch : 39 --- Loss :    16.7171\n",
      "Epoch : 13 --batch : 40 --- Loss :    12.6744\n",
      "Epoch : 13 --batch : 41 --- Loss :    49.2798\n",
      "Epoch : 13 --batch : 42 --- Loss :    16.2975\n",
      "Epoch : 13 --batch : 43 --- Loss :    15.1582\n",
      "Epoch : 13 --batch : 44 --- Loss :    31.6218\n",
      "Epoch : 13 --batch : 45 --- Loss :     3.8355\n",
      "Epoch : 13 --batch : 46 --- Loss :    20.3987\n",
      "Epoch : 13 --batch : 47 --- Loss :    17.1578\n",
      "Epoch : 13 --batch : 48 --- Loss :    21.5800\n",
      "Epoch : 13 --batch : 49 --- Loss :    15.7104\n",
      "Epoch : 13 --batch : 50 --- Loss :    16.7821\n",
      "Epoch : 13 --batch : 51 --- Loss :    19.4759\n",
      "Epoch : 13 --batch : 52 --- Loss :    30.3535\n",
      "Epoch : 13 --batch : 53 --- Loss :    30.2072\n",
      "Epoch : 13 --batch : 54 --- Loss :    10.6244\n",
      "Epoch : 13 --batch : 55 --- Loss :    18.6317\n",
      "Epoch : 13 --batch : 56 --- Loss :    28.9449\n",
      "Epoch : 13 --batch : 57 --- Loss :    14.7967\n",
      "Epoch : 13 --batch : 58 --- Loss :    38.7286\n",
      "Epoch : 13 --batch : 59 --- Loss :    13.0023\n",
      "Epoch : 13 --batch : 60 --- Loss :    13.4368\n",
      "Epoch : 13 --batch : 61 --- Loss :     9.1577\n",
      "Epoch : 13 --batch : 62 --- Loss :    11.7803\n",
      "Epoch : 13 --batch : 63 --- Loss :    20.6323\n",
      "Epoch : 13 --batch : 64 --- Loss :    22.1063\n",
      "Testing Accuracy after epoch 13 : 0.9575000077486038\n",
      "Epoch : 14 --batch : 1 --- Loss :    84.0870\n",
      "Epoch : 14 --batch : 2 --- Loss :    39.8133\n",
      "Epoch : 14 --batch : 3 --- Loss :    32.7769\n",
      "Epoch : 14 --batch : 4 --- Loss :    36.6389\n",
      "Epoch : 14 --batch : 5 --- Loss :    22.9273\n",
      "Epoch : 14 --batch : 6 --- Loss :    36.1733\n",
      "Epoch : 14 --batch : 7 --- Loss :    26.2136\n",
      "Epoch : 14 --batch : 8 --- Loss :    12.4653\n",
      "Epoch : 14 --batch : 9 --- Loss :    12.6538\n",
      "Epoch : 14 --batch : 10 --- Loss :    16.9514\n",
      "Epoch : 14 --batch : 11 --- Loss :    22.1598\n",
      "Epoch : 14 --batch : 12 --- Loss :    34.3954\n",
      "Epoch : 14 --batch : 13 --- Loss :     2.8033\n",
      "Epoch : 14 --batch : 14 --- Loss :    20.7354\n",
      "Epoch : 14 --batch : 15 --- Loss :    20.3439\n",
      "Epoch : 14 --batch : 16 --- Loss :    14.2973\n",
      "Epoch : 14 --batch : 17 --- Loss :    44.1948\n",
      "Epoch : 14 --batch : 18 --- Loss :    37.8355\n",
      "Epoch : 14 --batch : 19 --- Loss :    31.7907\n",
      "Epoch : 14 --batch : 20 --- Loss :    22.2317\n",
      "Epoch : 14 --batch : 21 --- Loss :    32.0680\n",
      "Epoch : 14 --batch : 22 --- Loss :    24.4858\n",
      "Epoch : 14 --batch : 23 --- Loss :    20.6924\n",
      "Epoch : 14 --batch : 24 --- Loss :    30.6998\n",
      "Epoch : 14 --batch : 25 --- Loss :    16.6933\n",
      "Epoch : 14 --batch : 26 --- Loss :    20.3811\n",
      "Epoch : 14 --batch : 27 --- Loss :    31.2581\n",
      "Epoch : 14 --batch : 28 --- Loss :    23.5297\n",
      "Epoch : 14 --batch : 29 --- Loss :    18.9026\n",
      "Epoch : 14 --batch : 30 --- Loss :    15.7295\n",
      "Epoch : 14 --batch : 31 --- Loss :    25.7848\n",
      "Epoch : 14 --batch : 32 --- Loss :    22.9920\n",
      "Epoch : 14 --batch : 33 --- Loss :    13.9896\n",
      "Epoch : 14 --batch : 34 --- Loss :    10.1764\n",
      "Epoch : 14 --batch : 35 --- Loss :    13.8405\n",
      "Epoch : 14 --batch : 36 --- Loss :     7.2022\n",
      "Epoch : 14 --batch : 37 --- Loss :    29.4389\n",
      "Epoch : 14 --batch : 38 --- Loss :    10.8864\n",
      "Epoch : 14 --batch : 39 --- Loss :    21.0487\n",
      "Epoch : 14 --batch : 40 --- Loss :     8.8694\n",
      "Epoch : 14 --batch : 41 --- Loss :    37.3313\n",
      "Epoch : 14 --batch : 42 --- Loss :    15.8860\n",
      "Epoch : 14 --batch : 43 --- Loss :    13.6081\n",
      "Epoch : 14 --batch : 44 --- Loss :    30.3981\n",
      "Epoch : 14 --batch : 45 --- Loss :     4.1880\n",
      "Epoch : 14 --batch : 46 --- Loss :    21.9917\n",
      "Epoch : 14 --batch : 47 --- Loss :    16.3327\n",
      "Epoch : 14 --batch : 48 --- Loss :    21.2885\n",
      "Epoch : 14 --batch : 49 --- Loss :    14.4923\n",
      "Epoch : 14 --batch : 50 --- Loss :    11.9251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 14 --batch : 51 --- Loss :    18.2008\n",
      "Epoch : 14 --batch : 52 --- Loss :    25.0926\n",
      "Epoch : 14 --batch : 53 --- Loss :    20.6474\n",
      "Epoch : 14 --batch : 54 --- Loss :     6.5577\n",
      "Epoch : 14 --batch : 55 --- Loss :     9.9160\n",
      "Epoch : 14 --batch : 56 --- Loss :    24.7678\n",
      "Epoch : 14 --batch : 57 --- Loss :     9.9547\n",
      "Epoch : 14 --batch : 58 --- Loss :    24.4639\n",
      "Epoch : 14 --batch : 59 --- Loss :    16.0395\n",
      "Epoch : 14 --batch : 60 --- Loss :    12.8976\n",
      "Epoch : 14 --batch : 61 --- Loss :    20.9344\n",
      "Epoch : 14 --batch : 62 --- Loss :    30.0658\n",
      "Epoch : 14 --batch : 63 --- Loss :    26.9472\n",
      "Epoch : 14 --batch : 64 --- Loss :    18.2666\n",
      "Testing Accuracy after epoch 14 : 0.9600000023841858\n",
      "Epoch : 15 --batch : 1 --- Loss :    79.9108\n",
      "Epoch : 15 --batch : 2 --- Loss :    25.6085\n",
      "Epoch : 15 --batch : 3 --- Loss :    19.6654\n",
      "Epoch : 15 --batch : 4 --- Loss :    32.3199\n",
      "Epoch : 15 --batch : 5 --- Loss :    18.6398\n",
      "Epoch : 15 --batch : 6 --- Loss :    32.3925\n",
      "Epoch : 15 --batch : 7 --- Loss :    23.8693\n",
      "Epoch : 15 --batch : 8 --- Loss :     8.1636\n",
      "Epoch : 15 --batch : 9 --- Loss :     9.6664\n",
      "Epoch : 15 --batch : 10 --- Loss :    21.4261\n",
      "Epoch : 15 --batch : 11 --- Loss :    18.0623\n",
      "Epoch : 15 --batch : 12 --- Loss :    42.6190\n",
      "Epoch : 15 --batch : 13 --- Loss :     2.0258\n",
      "Epoch : 15 --batch : 14 --- Loss :    18.4518\n",
      "Epoch : 15 --batch : 15 --- Loss :    16.9836\n",
      "Epoch : 15 --batch : 16 --- Loss :    13.2045\n",
      "Epoch : 15 --batch : 17 --- Loss :    31.1053\n",
      "Epoch : 15 --batch : 18 --- Loss :    25.5785\n",
      "Epoch : 15 --batch : 19 --- Loss :    28.7502\n",
      "Epoch : 15 --batch : 20 --- Loss :    16.1882\n",
      "Epoch : 15 --batch : 21 --- Loss :    28.3707\n",
      "Epoch : 15 --batch : 22 --- Loss :    18.3478\n",
      "Epoch : 15 --batch : 23 --- Loss :    23.4098\n",
      "Epoch : 15 --batch : 24 --- Loss :    30.3934\n",
      "Epoch : 15 --batch : 25 --- Loss :    18.4704\n",
      "Epoch : 15 --batch : 26 --- Loss :    27.7327\n",
      "Epoch : 15 --batch : 27 --- Loss :    24.1266\n",
      "Epoch : 15 --batch : 28 --- Loss :    19.8765\n",
      "Epoch : 15 --batch : 29 --- Loss :    13.6718\n",
      "Epoch : 15 --batch : 30 --- Loss :    15.7269\n",
      "Epoch : 15 --batch : 31 --- Loss :    32.4775\n",
      "Epoch : 15 --batch : 32 --- Loss :    22.9199\n",
      "Epoch : 15 --batch : 33 --- Loss :    15.3183\n",
      "Epoch : 15 --batch : 34 --- Loss :     5.4655\n",
      "Epoch : 15 --batch : 35 --- Loss :    13.0284\n",
      "Epoch : 15 --batch : 36 --- Loss :     8.9487\n",
      "Epoch : 15 --batch : 37 --- Loss :    29.1289\n",
      "Epoch : 15 --batch : 38 --- Loss :     6.2793\n",
      "Epoch : 15 --batch : 39 --- Loss :    22.6695\n",
      "Epoch : 15 --batch : 40 --- Loss :     8.2860\n",
      "Epoch : 15 --batch : 41 --- Loss :    33.8973\n",
      "Epoch : 15 --batch : 42 --- Loss :    18.5026\n",
      "Epoch : 15 --batch : 43 --- Loss :    14.9256\n",
      "Epoch : 15 --batch : 44 --- Loss :    30.8355\n",
      "Epoch : 15 --batch : 45 --- Loss :     5.4822\n",
      "Epoch : 15 --batch : 46 --- Loss :    15.4407\n",
      "Epoch : 15 --batch : 47 --- Loss :    25.4037\n",
      "Epoch : 15 --batch : 48 --- Loss :    19.3290\n",
      "Epoch : 15 --batch : 49 --- Loss :    12.7845\n",
      "Epoch : 15 --batch : 50 --- Loss :    11.8367\n",
      "Epoch : 15 --batch : 51 --- Loss :    16.2071\n",
      "Epoch : 15 --batch : 52 --- Loss :    22.4735\n",
      "Epoch : 15 --batch : 53 --- Loss :    21.8550\n",
      "Epoch : 15 --batch : 54 --- Loss :     5.0524\n",
      "Epoch : 15 --batch : 55 --- Loss :    10.4206\n",
      "Epoch : 15 --batch : 56 --- Loss :    22.7188\n",
      "Epoch : 15 --batch : 57 --- Loss :    10.3054\n",
      "Epoch : 15 --batch : 58 --- Loss :    22.8102\n",
      "Epoch : 15 --batch : 59 --- Loss :    19.1891\n",
      "Epoch : 15 --batch : 60 --- Loss :    11.0423\n",
      "Epoch : 15 --batch : 61 --- Loss :     9.6019\n",
      "Epoch : 15 --batch : 62 --- Loss :    10.4930\n",
      "Epoch : 15 --batch : 63 --- Loss :    14.2202\n",
      "Epoch : 15 --batch : 64 --- Loss :    18.8078\n",
      "Testing Accuracy after epoch 15 : 0.9610000044107437\n",
      "Epoch : 16 --batch : 1 --- Loss :    75.2703\n",
      "Epoch : 16 --batch : 2 --- Loss :    21.5764\n",
      "Epoch : 16 --batch : 3 --- Loss :    16.4622\n",
      "Epoch : 16 --batch : 4 --- Loss :    29.8684\n",
      "Epoch : 16 --batch : 5 --- Loss :    16.2726\n",
      "Epoch : 16 --batch : 6 --- Loss :    30.9780\n",
      "Epoch : 16 --batch : 7 --- Loss :    20.3008\n",
      "Epoch : 16 --batch : 8 --- Loss :     9.8933\n",
      "Epoch : 16 --batch : 9 --- Loss :    11.2233\n",
      "Epoch : 16 --batch : 10 --- Loss :    17.4553\n",
      "Epoch : 16 --batch : 11 --- Loss :    22.1844\n",
      "Epoch : 16 --batch : 12 --- Loss :    42.9777\n",
      "Epoch : 16 --batch : 13 --- Loss :     0.9872\n",
      "Epoch : 16 --batch : 14 --- Loss :    18.2091\n",
      "Epoch : 16 --batch : 15 --- Loss :    19.4757\n",
      "Epoch : 16 --batch : 16 --- Loss :    14.3604\n",
      "Epoch : 16 --batch : 17 --- Loss :    25.0722\n",
      "Epoch : 16 --batch : 18 --- Loss :    28.0040\n",
      "Epoch : 16 --batch : 19 --- Loss :    25.4794\n",
      "Epoch : 16 --batch : 20 --- Loss :    23.9455\n",
      "Epoch : 16 --batch : 21 --- Loss :    32.4317\n",
      "Epoch : 16 --batch : 22 --- Loss :    19.0983\n",
      "Epoch : 16 --batch : 23 --- Loss :    20.1438\n",
      "Epoch : 16 --batch : 24 --- Loss :    26.1179\n",
      "Epoch : 16 --batch : 25 --- Loss :    15.6318\n",
      "Epoch : 16 --batch : 26 --- Loss :    22.1222\n",
      "Epoch : 16 --batch : 27 --- Loss :    21.2249\n",
      "Epoch : 16 --batch : 28 --- Loss :    21.4098\n",
      "Epoch : 16 --batch : 29 --- Loss :    15.8678\n",
      "Epoch : 16 --batch : 30 --- Loss :    14.2078\n",
      "Epoch : 16 --batch : 31 --- Loss :    24.9060\n",
      "Epoch : 16 --batch : 32 --- Loss :    22.5247\n",
      "Epoch : 16 --batch : 33 --- Loss :    13.5696\n",
      "Epoch : 16 --batch : 34 --- Loss :     4.3754\n",
      "Epoch : 16 --batch : 35 --- Loss :    12.3026\n",
      "Epoch : 16 --batch : 36 --- Loss :     8.7876\n",
      "Epoch : 16 --batch : 37 --- Loss :    31.6261\n",
      "Epoch : 16 --batch : 38 --- Loss :     5.0252\n",
      "Epoch : 16 --batch : 39 --- Loss :    15.4376\n",
      "Epoch : 16 --batch : 40 --- Loss :     9.5001\n",
      "Epoch : 16 --batch : 41 --- Loss :    33.4038\n",
      "Epoch : 16 --batch : 42 --- Loss :    15.6671\n",
      "Epoch : 16 --batch : 43 --- Loss :    10.5173\n",
      "Epoch : 16 --batch : 44 --- Loss :    33.6684\n",
      "Epoch : 16 --batch : 45 --- Loss :     2.6592\n",
      "Epoch : 16 --batch : 46 --- Loss :    20.5264\n",
      "Epoch : 16 --batch : 47 --- Loss :    18.4594\n",
      "Epoch : 16 --batch : 48 --- Loss :    27.4050\n",
      "Epoch : 16 --batch : 49 --- Loss :    17.3725\n",
      "Epoch : 16 --batch : 50 --- Loss :    13.2097\n",
      "Epoch : 16 --batch : 51 --- Loss :    13.5756\n",
      "Epoch : 16 --batch : 52 --- Loss :    26.1167\n",
      "Epoch : 16 --batch : 53 --- Loss :    10.3966\n",
      "Epoch : 16 --batch : 54 --- Loss :     6.3806\n",
      "Epoch : 16 --batch : 55 --- Loss :    10.4285\n",
      "Epoch : 16 --batch : 56 --- Loss :    20.4111\n",
      "Epoch : 16 --batch : 57 --- Loss :     5.1662\n",
      "Epoch : 16 --batch : 58 --- Loss :    21.2674\n",
      "Epoch : 16 --batch : 59 --- Loss :    10.9247\n",
      "Epoch : 16 --batch : 60 --- Loss :     8.2609\n",
      "Epoch : 16 --batch : 61 --- Loss :    10.7270\n",
      "Epoch : 16 --batch : 62 --- Loss :    11.0650\n",
      "Epoch : 16 --batch : 63 --- Loss :    15.4867\n",
      "Epoch : 16 --batch : 64 --- Loss :    13.6459\n",
      "Testing Accuracy after epoch 16 : 0.964000004529953\n",
      "Epoch : 17 --batch : 1 --- Loss :    75.3654\n",
      "Epoch : 17 --batch : 2 --- Loss :    20.4029\n",
      "Epoch : 17 --batch : 3 --- Loss :    16.7375\n",
      "Epoch : 17 --batch : 4 --- Loss :    28.3295\n",
      "Epoch : 17 --batch : 5 --- Loss :    12.7817\n",
      "Epoch : 17 --batch : 6 --- Loss :    25.1431\n",
      "Epoch : 17 --batch : 7 --- Loss :    20.5648\n",
      "Epoch : 17 --batch : 8 --- Loss :     7.5626\n",
      "Epoch : 17 --batch : 9 --- Loss :     7.3522\n",
      "Epoch : 17 --batch : 10 --- Loss :    15.9221\n",
      "Epoch : 17 --batch : 11 --- Loss :    10.4504\n",
      "Epoch : 17 --batch : 12 --- Loss :    31.4938\n",
      "Epoch : 17 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 17 --batch : 14 --- Loss :    15.7060\n",
      "Epoch : 17 --batch : 15 --- Loss :    16.5433\n",
      "Epoch : 17 --batch : 16 --- Loss :    14.3504\n",
      "Epoch : 17 --batch : 17 --- Loss :    20.1198\n",
      "Epoch : 17 --batch : 18 --- Loss :    28.2453\n",
      "Epoch : 17 --batch : 19 --- Loss :    24.5088\n",
      "Epoch : 17 --batch : 20 --- Loss :    20.4002\n",
      "Epoch : 17 --batch : 21 --- Loss :    31.7978\n",
      "Epoch : 17 --batch : 22 --- Loss :    22.7419\n",
      "Epoch : 17 --batch : 23 --- Loss :    20.4566\n",
      "Epoch : 17 --batch : 24 --- Loss :    28.1070\n",
      "Epoch : 17 --batch : 25 --- Loss :    14.7912\n",
      "Epoch : 17 --batch : 26 --- Loss :    18.2016\n",
      "Epoch : 17 --batch : 27 --- Loss :    27.3453\n",
      "Epoch : 17 --batch : 28 --- Loss :    22.1765\n",
      "Epoch : 17 --batch : 29 --- Loss :    15.8355\n",
      "Epoch : 17 --batch : 30 --- Loss :    13.4484\n",
      "Epoch : 17 --batch : 31 --- Loss :    19.2365\n",
      "Epoch : 17 --batch : 32 --- Loss :    23.6737\n",
      "Epoch : 17 --batch : 33 --- Loss :    14.2527\n",
      "Epoch : 17 --batch : 34 --- Loss :     2.6725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 17 --batch : 35 --- Loss :    11.0372\n",
      "Epoch : 17 --batch : 36 --- Loss :     6.7887\n",
      "Epoch : 17 --batch : 37 --- Loss :    27.5630\n",
      "Epoch : 17 --batch : 38 --- Loss :     3.9539\n",
      "Epoch : 17 --batch : 39 --- Loss :    14.1496\n",
      "Epoch : 17 --batch : 40 --- Loss :     7.5737\n",
      "Epoch : 17 --batch : 41 --- Loss :    29.3986\n",
      "Epoch : 17 --batch : 42 --- Loss :    13.3202\n",
      "Epoch : 17 --batch : 43 --- Loss :     9.8301\n",
      "Epoch : 17 --batch : 44 --- Loss :    33.2307\n",
      "Epoch : 17 --batch : 45 --- Loss :     4.8674\n",
      "Epoch : 17 --batch : 46 --- Loss :    13.4608\n",
      "Epoch : 17 --batch : 47 --- Loss :    17.6167\n",
      "Epoch : 17 --batch : 48 --- Loss :    18.1138\n",
      "Epoch : 17 --batch : 49 --- Loss :    12.5647\n",
      "Epoch : 17 --batch : 50 --- Loss :    10.2465\n",
      "Epoch : 17 --batch : 51 --- Loss :    12.9876\n",
      "Epoch : 17 --batch : 52 --- Loss :    30.1585\n",
      "Epoch : 17 --batch : 53 --- Loss :    13.8440\n",
      "Epoch : 17 --batch : 54 --- Loss :     4.8318\n",
      "Epoch : 17 --batch : 55 --- Loss :    12.1389\n",
      "Epoch : 17 --batch : 56 --- Loss :    17.1541\n",
      "Epoch : 17 --batch : 57 --- Loss :     5.1794\n",
      "Epoch : 17 --batch : 58 --- Loss :    20.3353\n",
      "Epoch : 17 --batch : 59 --- Loss :    10.3876\n",
      "Epoch : 17 --batch : 60 --- Loss :     6.4716\n",
      "Epoch : 17 --batch : 61 --- Loss :     8.4237\n",
      "Epoch : 17 --batch : 62 --- Loss :    14.5022\n",
      "Epoch : 17 --batch : 63 --- Loss :    23.5544\n",
      "Epoch : 17 --batch : 64 --- Loss :    16.7798\n",
      "Testing Accuracy after epoch 17 : 0.9605000048875809\n",
      "Epoch : 18 --batch : 1 --- Loss :    72.0848\n",
      "Epoch : 18 --batch : 2 --- Loss :    19.5728\n",
      "Epoch : 18 --batch : 3 --- Loss :    15.8669\n",
      "Epoch : 18 --batch : 4 --- Loss :    25.6993\n",
      "Epoch : 18 --batch : 5 --- Loss :    13.6779\n",
      "Epoch : 18 --batch : 6 --- Loss :    21.5576\n",
      "Epoch : 18 --batch : 7 --- Loss :    22.2644\n",
      "Epoch : 18 --batch : 8 --- Loss :     6.6035\n",
      "Epoch : 18 --batch : 9 --- Loss :     7.5214\n",
      "Epoch : 18 --batch : 10 --- Loss :    10.5761\n",
      "Epoch : 18 --batch : 11 --- Loss :     7.6412\n",
      "Epoch : 18 --batch : 12 --- Loss :    26.4783\n",
      "Epoch : 18 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 18 --batch : 14 --- Loss :     9.2200\n",
      "Epoch : 18 --batch : 15 --- Loss :     7.5623\n",
      "Epoch : 18 --batch : 16 --- Loss :    12.6934\n",
      "Epoch : 18 --batch : 17 --- Loss :    17.1616\n",
      "Epoch : 18 --batch : 18 --- Loss :    24.5240\n",
      "Epoch : 18 --batch : 19 --- Loss :    19.6649\n",
      "Epoch : 18 --batch : 20 --- Loss :    18.4852\n",
      "Epoch : 18 --batch : 21 --- Loss :    34.9905\n",
      "Epoch : 18 --batch : 22 --- Loss :    15.6570\n",
      "Epoch : 18 --batch : 23 --- Loss :    22.9996\n",
      "Epoch : 18 --batch : 24 --- Loss :    21.4067\n",
      "Epoch : 18 --batch : 25 --- Loss :    11.5539\n",
      "Epoch : 18 --batch : 26 --- Loss :    14.2920\n",
      "Epoch : 18 --batch : 27 --- Loss :    27.0851\n",
      "Epoch : 18 --batch : 28 --- Loss :    27.9877\n",
      "Epoch : 18 --batch : 29 --- Loss :    13.2694\n",
      "Epoch : 18 --batch : 30 --- Loss :    14.5978\n",
      "Epoch : 18 --batch : 31 --- Loss :    15.5254\n",
      "Epoch : 18 --batch : 32 --- Loss :    24.7890\n",
      "Epoch : 18 --batch : 33 --- Loss :    13.8408\n",
      "Epoch : 18 --batch : 34 --- Loss :     1.7183\n",
      "Epoch : 18 --batch : 35 --- Loss :    11.4473\n",
      "Epoch : 18 --batch : 36 --- Loss :     6.9333\n",
      "Epoch : 18 --batch : 37 --- Loss :    22.0687\n",
      "Epoch : 18 --batch : 38 --- Loss :     2.4341\n",
      "Epoch : 18 --batch : 39 --- Loss :    14.2676\n",
      "Epoch : 18 --batch : 40 --- Loss :     8.5279\n",
      "Epoch : 18 --batch : 41 --- Loss :    40.8450\n",
      "Epoch : 18 --batch : 42 --- Loss :    13.4890\n",
      "Epoch : 18 --batch : 43 --- Loss :     9.4076\n",
      "Epoch : 18 --batch : 44 --- Loss :    27.9115\n",
      "Epoch : 18 --batch : 45 --- Loss :     3.8267\n",
      "Epoch : 18 --batch : 46 --- Loss :    14.3312\n",
      "Epoch : 18 --batch : 47 --- Loss :    17.6388\n",
      "Epoch : 18 --batch : 48 --- Loss :    28.8731\n",
      "Epoch : 18 --batch : 49 --- Loss :    20.9035\n",
      "Epoch : 18 --batch : 50 --- Loss :     9.9348\n",
      "Epoch : 18 --batch : 51 --- Loss :    11.8717\n",
      "Epoch : 18 --batch : 52 --- Loss :    21.2232\n",
      "Epoch : 18 --batch : 53 --- Loss :    11.4265\n",
      "Epoch : 18 --batch : 54 --- Loss :     5.4449\n",
      "Epoch : 18 --batch : 55 --- Loss :     7.0040\n",
      "Epoch : 18 --batch : 56 --- Loss :    18.0454\n",
      "Epoch : 18 --batch : 57 --- Loss :     2.2191\n",
      "Epoch : 18 --batch : 58 --- Loss :    28.1996\n",
      "Epoch : 18 --batch : 59 --- Loss :    13.1665\n",
      "Epoch : 18 --batch : 60 --- Loss :     4.7807\n",
      "Epoch : 18 --batch : 61 --- Loss :    12.5285\n",
      "Epoch : 18 --batch : 62 --- Loss :    10.3060\n",
      "Epoch : 18 --batch : 63 --- Loss :    14.2869\n",
      "Epoch : 18 --batch : 64 --- Loss :    15.7163\n",
      "Testing Accuracy after epoch 18 : 0.9610000044107437\n",
      "Epoch : 19 --batch : 1 --- Loss :    65.6519\n",
      "Epoch : 19 --batch : 2 --- Loss :    15.3346\n",
      "Epoch : 19 --batch : 3 --- Loss :    12.1643\n",
      "Epoch : 19 --batch : 4 --- Loss :    23.0852\n",
      "Epoch : 19 --batch : 5 --- Loss :    10.7347\n",
      "Epoch : 19 --batch : 6 --- Loss :    18.8024\n",
      "Epoch : 19 --batch : 7 --- Loss :    20.1271\n",
      "Epoch : 19 --batch : 8 --- Loss :     6.1621\n",
      "Epoch : 19 --batch : 9 --- Loss :    12.0804\n",
      "Epoch : 19 --batch : 10 --- Loss :     9.0064\n",
      "Epoch : 19 --batch : 11 --- Loss :     8.8600\n",
      "Epoch : 19 --batch : 12 --- Loss :    28.4056\n",
      "Epoch : 19 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 19 --batch : 14 --- Loss :    16.2743\n",
      "Epoch : 19 --batch : 15 --- Loss :    19.3300\n",
      "Epoch : 19 --batch : 16 --- Loss :    20.0063\n",
      "Epoch : 19 --batch : 17 --- Loss :    31.9816\n",
      "Epoch : 19 --batch : 18 --- Loss :    32.2577\n",
      "Epoch : 19 --batch : 19 --- Loss :    24.6703\n",
      "Epoch : 19 --batch : 20 --- Loss :    21.8431\n",
      "Epoch : 19 --batch : 21 --- Loss :    31.2440\n",
      "Epoch : 19 --batch : 22 --- Loss :    13.7536\n",
      "Epoch : 19 --batch : 23 --- Loss :    17.4293\n",
      "Epoch : 19 --batch : 24 --- Loss :    21.0154\n",
      "Epoch : 19 --batch : 25 --- Loss :    12.6865\n",
      "Epoch : 19 --batch : 26 --- Loss :    10.8830\n",
      "Epoch : 19 --batch : 27 --- Loss :    20.2217\n",
      "Epoch : 19 --batch : 28 --- Loss :    23.7358\n",
      "Epoch : 19 --batch : 29 --- Loss :    12.0622\n",
      "Epoch : 19 --batch : 30 --- Loss :    13.2513\n",
      "Epoch : 19 --batch : 31 --- Loss :    17.4929\n",
      "Epoch : 19 --batch : 32 --- Loss :    25.1229\n",
      "Epoch : 19 --batch : 33 --- Loss :    14.7936\n",
      "Epoch : 19 --batch : 34 --- Loss :     8.8049\n",
      "Epoch : 19 --batch : 35 --- Loss :    11.3321\n",
      "Epoch : 19 --batch : 36 --- Loss :     6.5173\n",
      "Epoch : 19 --batch : 37 --- Loss :    24.9669\n",
      "Epoch : 19 --batch : 38 --- Loss :     3.4361\n",
      "Epoch : 19 --batch : 39 --- Loss :    16.4413\n",
      "Epoch : 19 --batch : 40 --- Loss :     7.3413\n",
      "Epoch : 19 --batch : 41 --- Loss :    23.5466\n",
      "Epoch : 19 --batch : 42 --- Loss :    12.5245\n",
      "Epoch : 19 --batch : 43 --- Loss :    11.2949\n",
      "Epoch : 19 --batch : 44 --- Loss :    25.8893\n",
      "Epoch : 19 --batch : 45 --- Loss :     3.2523\n",
      "Epoch : 19 --batch : 46 --- Loss :    12.3637\n",
      "Epoch : 19 --batch : 47 --- Loss :    15.6086\n",
      "Epoch : 19 --batch : 48 --- Loss :    16.2337\n",
      "Epoch : 19 --batch : 49 --- Loss :    20.0916\n",
      "Epoch : 19 --batch : 50 --- Loss :     9.5238\n",
      "Epoch : 19 --batch : 51 --- Loss :    10.9510\n",
      "Epoch : 19 --batch : 52 --- Loss :    20.6341\n",
      "Epoch : 19 --batch : 53 --- Loss :     9.1092\n",
      "Epoch : 19 --batch : 54 --- Loss :    14.4131\n",
      "Epoch : 19 --batch : 55 --- Loss :    10.0881\n",
      "Epoch : 19 --batch : 56 --- Loss :    17.0101\n",
      "Epoch : 19 --batch : 57 --- Loss :     4.0564\n",
      "Epoch : 19 --batch : 58 --- Loss :    19.8602\n",
      "Epoch : 19 --batch : 59 --- Loss :     9.6132\n",
      "Epoch : 19 --batch : 60 --- Loss :     2.9782\n",
      "Epoch : 19 --batch : 61 --- Loss :     4.4395\n",
      "Epoch : 19 --batch : 62 --- Loss :     9.5161\n",
      "Epoch : 19 --batch : 63 --- Loss :    11.8027\n",
      "Epoch : 19 --batch : 64 --- Loss :    10.3364\n",
      "Testing Accuracy after epoch 19 : 0.9675000071525574\n",
      "Epoch : 20 --batch : 1 --- Loss :    62.4353\n",
      "Epoch : 20 --batch : 2 --- Loss :    13.9950\n",
      "Epoch : 20 --batch : 3 --- Loss :    10.3592\n",
      "Epoch : 20 --batch : 4 --- Loss :    25.3541\n",
      "Epoch : 20 --batch : 5 --- Loss :    10.2709\n",
      "Epoch : 20 --batch : 6 --- Loss :    16.4640\n",
      "Epoch : 20 --batch : 7 --- Loss :    16.8464\n",
      "Epoch : 20 --batch : 8 --- Loss :     7.4393\n",
      "Epoch : 20 --batch : 9 --- Loss :     7.6172\n",
      "Epoch : 20 --batch : 10 --- Loss :    11.4962\n",
      "Epoch : 20 --batch : 11 --- Loss :     8.1352\n",
      "Epoch : 20 --batch : 12 --- Loss :    29.7240\n",
      "Epoch : 20 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 20 --batch : 14 --- Loss :    10.8239\n",
      "Epoch : 20 --batch : 15 --- Loss :     8.2337\n",
      "Epoch : 20 --batch : 16 --- Loss :    11.0955\n",
      "Epoch : 20 --batch : 17 --- Loss :    18.6417\n",
      "Epoch : 20 --batch : 18 --- Loss :    28.3900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20 --batch : 19 --- Loss :    23.0394\n",
      "Epoch : 20 --batch : 20 --- Loss :    25.0556\n",
      "Epoch : 20 --batch : 21 --- Loss :    42.1431\n",
      "Epoch : 20 --batch : 22 --- Loss :    13.7640\n",
      "Epoch : 20 --batch : 23 --- Loss :    18.0355\n",
      "Epoch : 20 --batch : 24 --- Loss :    17.4375\n",
      "Epoch : 20 --batch : 25 --- Loss :    16.1107\n",
      "Epoch : 20 --batch : 26 --- Loss :    15.9994\n",
      "Epoch : 20 --batch : 27 --- Loss :    20.9054\n",
      "Epoch : 20 --batch : 28 --- Loss :    20.1852\n",
      "Epoch : 20 --batch : 29 --- Loss :    12.9881\n",
      "Epoch : 20 --batch : 30 --- Loss :    12.9847\n",
      "Epoch : 20 --batch : 31 --- Loss :    15.8032\n",
      "Epoch : 20 --batch : 32 --- Loss :    23.7711\n",
      "Epoch : 20 --batch : 33 --- Loss :    13.7496\n",
      "Epoch : 20 --batch : 34 --- Loss :     1.2473\n",
      "Epoch : 20 --batch : 35 --- Loss :    11.2223\n",
      "Epoch : 20 --batch : 36 --- Loss :     8.5538\n",
      "Epoch : 20 --batch : 37 --- Loss :    19.5554\n",
      "Epoch : 20 --batch : 38 --- Loss :     4.8395\n",
      "Epoch : 20 --batch : 39 --- Loss :    14.0372\n",
      "Epoch : 20 --batch : 40 --- Loss :     7.2112\n",
      "Epoch : 20 --batch : 41 --- Loss :    28.1960\n",
      "Epoch : 20 --batch : 42 --- Loss :    15.2275\n",
      "Epoch : 20 --batch : 43 --- Loss :    11.9332\n",
      "Epoch : 20 --batch : 44 --- Loss :    31.7371\n",
      "Epoch : 20 --batch : 45 --- Loss :     5.1115\n",
      "Epoch : 20 --batch : 46 --- Loss :    12.6803\n",
      "Epoch : 20 --batch : 47 --- Loss :    15.3210\n",
      "Epoch : 20 --batch : 48 --- Loss :    12.0980\n",
      "Epoch : 20 --batch : 49 --- Loss :    12.1036\n",
      "Epoch : 20 --batch : 50 --- Loss :     8.1184\n",
      "Epoch : 20 --batch : 51 --- Loss :    11.3590\n",
      "Epoch : 20 --batch : 52 --- Loss :    19.6741\n",
      "Epoch : 20 --batch : 53 --- Loss :     8.3314\n",
      "Epoch : 20 --batch : 54 --- Loss :    11.1328\n",
      "Epoch : 20 --batch : 55 --- Loss :    11.2457\n",
      "Epoch : 20 --batch : 56 --- Loss :    15.1652\n",
      "Epoch : 20 --batch : 57 --- Loss :     6.9343\n",
      "Epoch : 20 --batch : 58 --- Loss :    20.0294\n",
      "Epoch : 20 --batch : 59 --- Loss :     9.1059\n",
      "Epoch : 20 --batch : 60 --- Loss :     2.6923\n",
      "Epoch : 20 --batch : 61 --- Loss :    12.7810\n",
      "Epoch : 20 --batch : 62 --- Loss :    12.6856\n",
      "Epoch : 20 --batch : 63 --- Loss :    14.6241\n",
      "Epoch : 20 --batch : 64 --- Loss :    10.6136\n",
      "Testing Accuracy after epoch 20 : 0.9680000066757202\n",
      "Epoch : 21 --batch : 1 --- Loss :    62.4909\n",
      "Epoch : 21 --batch : 2 --- Loss :    13.5089\n",
      "Epoch : 21 --batch : 3 --- Loss :     8.5391\n",
      "Epoch : 21 --batch : 4 --- Loss :    22.0164\n",
      "Epoch : 21 --batch : 5 --- Loss :     7.4278\n",
      "Epoch : 21 --batch : 6 --- Loss :    17.4362\n",
      "Epoch : 21 --batch : 7 --- Loss :    16.5169\n",
      "Epoch : 21 --batch : 8 --- Loss :     6.9611\n",
      "Epoch : 21 --batch : 9 --- Loss :     6.9658\n",
      "Epoch : 21 --batch : 10 --- Loss :     7.5384\n",
      "Epoch : 21 --batch : 11 --- Loss :     4.2458\n",
      "Epoch : 21 --batch : 12 --- Loss :    24.5974\n",
      "Epoch : 21 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 21 --batch : 14 --- Loss :    11.7002\n",
      "Epoch : 21 --batch : 15 --- Loss :    12.5091\n",
      "Epoch : 21 --batch : 16 --- Loss :    12.5487\n",
      "Epoch : 21 --batch : 17 --- Loss :    20.2858\n",
      "Epoch : 21 --batch : 18 --- Loss :    18.6176\n",
      "Epoch : 21 --batch : 19 --- Loss :    24.2175\n",
      "Epoch : 21 --batch : 20 --- Loss :    27.8431\n",
      "Epoch : 21 --batch : 21 --- Loss :    34.8207\n",
      "Epoch : 21 --batch : 22 --- Loss :    16.6035\n",
      "Epoch : 21 --batch : 23 --- Loss :    17.7595\n",
      "Epoch : 21 --batch : 24 --- Loss :    20.5584\n",
      "Epoch : 21 --batch : 25 --- Loss :    11.7018\n",
      "Epoch : 21 --batch : 26 --- Loss :     7.5750\n",
      "Epoch : 21 --batch : 27 --- Loss :    18.4628\n",
      "Epoch : 21 --batch : 28 --- Loss :    21.0502\n",
      "Epoch : 21 --batch : 29 --- Loss :    12.4488\n",
      "Epoch : 21 --batch : 30 --- Loss :    12.9337\n",
      "Epoch : 21 --batch : 31 --- Loss :    16.5302\n",
      "Epoch : 21 --batch : 32 --- Loss :    24.0802\n",
      "Epoch : 21 --batch : 33 --- Loss :    14.3198\n",
      "Epoch : 21 --batch : 34 --- Loss :     3.6790\n",
      "Epoch : 21 --batch : 35 --- Loss :    12.0250\n",
      "Epoch : 21 --batch : 36 --- Loss :     5.8520\n",
      "Epoch : 21 --batch : 37 --- Loss :    18.3056\n",
      "Epoch : 21 --batch : 38 --- Loss :     0.8160\n",
      "Epoch : 21 --batch : 39 --- Loss :     6.2344\n",
      "Epoch : 21 --batch : 40 --- Loss :     5.2396\n",
      "Epoch : 21 --batch : 41 --- Loss :    22.3500\n",
      "Epoch : 21 --batch : 42 --- Loss :    14.8250\n",
      "Epoch : 21 --batch : 43 --- Loss :     8.8878\n",
      "Epoch : 21 --batch : 44 --- Loss :    23.1700\n",
      "Epoch : 21 --batch : 45 --- Loss :     4.2494\n",
      "Epoch : 21 --batch : 46 --- Loss :    11.6326\n",
      "Epoch : 21 --batch : 47 --- Loss :    16.5732\n",
      "Epoch : 21 --batch : 48 --- Loss :    13.1462\n",
      "Epoch : 21 --batch : 49 --- Loss :    10.2292\n",
      "Epoch : 21 --batch : 50 --- Loss :    11.5216\n",
      "Epoch : 21 --batch : 51 --- Loss :    12.3691\n",
      "Epoch : 21 --batch : 52 --- Loss :    22.9726\n",
      "Epoch : 21 --batch : 53 --- Loss :     8.0935\n",
      "Epoch : 21 --batch : 54 --- Loss :     3.5114\n",
      "Epoch : 21 --batch : 55 --- Loss :    10.4180\n",
      "Epoch : 21 --batch : 56 --- Loss :    17.0226\n",
      "Epoch : 21 --batch : 57 --- Loss :     2.3615\n",
      "Epoch : 21 --batch : 58 --- Loss :    21.3244\n",
      "Epoch : 21 --batch : 59 --- Loss :     8.9906\n",
      "Epoch : 21 --batch : 60 --- Loss :     1.9606\n",
      "Epoch : 21 --batch : 61 --- Loss :     6.7245\n",
      "Epoch : 21 --batch : 62 --- Loss :     7.9213\n",
      "Epoch : 21 --batch : 63 --- Loss :    14.1987\n",
      "Epoch : 21 --batch : 64 --- Loss :    12.7264\n",
      "Testing Accuracy after epoch 21 : 0.9645000040531159\n",
      "Epoch : 22 --batch : 1 --- Loss :    59.0509\n",
      "Epoch : 22 --batch : 2 --- Loss :    15.7516\n",
      "Epoch : 22 --batch : 3 --- Loss :    15.0632\n",
      "Epoch : 22 --batch : 4 --- Loss :    23.8550\n",
      "Epoch : 22 --batch : 5 --- Loss :     6.8510\n",
      "Epoch : 22 --batch : 6 --- Loss :    16.2660\n",
      "Epoch : 22 --batch : 7 --- Loss :    15.7306\n",
      "Epoch : 22 --batch : 8 --- Loss :     6.5008\n",
      "Epoch : 22 --batch : 9 --- Loss :     8.6343\n",
      "Epoch : 22 --batch : 10 --- Loss :     8.5059\n",
      "Epoch : 22 --batch : 11 --- Loss :     4.4883\n",
      "Epoch : 22 --batch : 12 --- Loss :    24.1129\n",
      "Epoch : 22 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 22 --batch : 14 --- Loss :     6.2860\n",
      "Epoch : 22 --batch : 15 --- Loss :     4.3267\n",
      "Epoch : 22 --batch : 16 --- Loss :    11.3516\n",
      "Epoch : 22 --batch : 17 --- Loss :    17.8672\n",
      "Epoch : 22 --batch : 18 --- Loss :    18.1503\n",
      "Epoch : 22 --batch : 19 --- Loss :    16.9056\n",
      "Epoch : 22 --batch : 20 --- Loss :    15.5273\n",
      "Epoch : 22 --batch : 21 --- Loss :    32.7038\n",
      "Epoch : 22 --batch : 22 --- Loss :    11.1438\n",
      "Epoch : 22 --batch : 23 --- Loss :    17.7070\n",
      "Epoch : 22 --batch : 24 --- Loss :    17.8846\n",
      "Epoch : 22 --batch : 25 --- Loss :    10.3975\n",
      "Epoch : 22 --batch : 26 --- Loss :    14.0298\n",
      "Epoch : 22 --batch : 27 --- Loss :    22.4910\n",
      "Epoch : 22 --batch : 28 --- Loss :    20.5495\n",
      "Epoch : 22 --batch : 29 --- Loss :    13.7964\n",
      "Epoch : 22 --batch : 30 --- Loss :    15.1906\n",
      "Epoch : 22 --batch : 31 --- Loss :    20.9134\n",
      "Epoch : 22 --batch : 32 --- Loss :    24.2456\n",
      "Epoch : 22 --batch : 33 --- Loss :    15.8494\n",
      "Epoch : 22 --batch : 34 --- Loss :     8.0241\n",
      "Epoch : 22 --batch : 35 --- Loss :    11.4065\n",
      "Epoch : 22 --batch : 36 --- Loss :     6.0611\n",
      "Epoch : 22 --batch : 37 --- Loss :    20.7294\n",
      "Epoch : 22 --batch : 38 --- Loss :     3.6601\n",
      "Epoch : 22 --batch : 39 --- Loss :    12.9584\n",
      "Epoch : 22 --batch : 40 --- Loss :     7.5243\n",
      "Epoch : 22 --batch : 41 --- Loss :    25.1154\n",
      "Epoch : 22 --batch : 42 --- Loss :    12.6244\n",
      "Epoch : 22 --batch : 43 --- Loss :     9.1299\n",
      "Epoch : 22 --batch : 44 --- Loss :    28.7396\n",
      "Epoch : 22 --batch : 45 --- Loss :     3.9037\n",
      "Epoch : 22 --batch : 46 --- Loss :    11.2470\n",
      "Epoch : 22 --batch : 47 --- Loss :    13.0955\n",
      "Epoch : 22 --batch : 48 --- Loss :    13.3511\n",
      "Epoch : 22 --batch : 49 --- Loss :    10.0240\n",
      "Epoch : 22 --batch : 50 --- Loss :     8.4573\n",
      "Epoch : 22 --batch : 51 --- Loss :     9.9550\n",
      "Epoch : 22 --batch : 52 --- Loss :    17.2293\n",
      "Epoch : 22 --batch : 53 --- Loss :     7.7225\n",
      "Epoch : 22 --batch : 54 --- Loss :     3.1856\n",
      "Epoch : 22 --batch : 55 --- Loss :    11.0237\n",
      "Epoch : 22 --batch : 56 --- Loss :    13.4672\n",
      "Epoch : 22 --batch : 57 --- Loss :     3.8941\n",
      "Epoch : 22 --batch : 58 --- Loss :    20.6812\n",
      "Epoch : 22 --batch : 59 --- Loss :     9.1861\n",
      "Epoch : 22 --batch : 60 --- Loss :     1.4592\n",
      "Epoch : 22 --batch : 61 --- Loss :     7.1105\n",
      "Epoch : 22 --batch : 62 --- Loss :     8.4613\n",
      "Epoch : 22 --batch : 63 --- Loss :    15.6222\n",
      "Epoch : 22 --batch : 64 --- Loss :     9.8989\n",
      "Testing Accuracy after epoch 22 : 0.9690000057220459\n",
      "Epoch : 23 --batch : 1 --- Loss :    60.7592\n",
      "Epoch : 23 --batch : 2 --- Loss :    22.8975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 23 --batch : 3 --- Loss :    13.4107\n",
      "Epoch : 23 --batch : 4 --- Loss :    23.0038\n",
      "Epoch : 23 --batch : 5 --- Loss :     6.6978\n",
      "Epoch : 23 --batch : 6 --- Loss :    13.5346\n",
      "Epoch : 23 --batch : 7 --- Loss :    14.2701\n",
      "Epoch : 23 --batch : 8 --- Loss :     6.7470\n",
      "Epoch : 23 --batch : 9 --- Loss :     8.0659\n",
      "Epoch : 23 --batch : 10 --- Loss :     6.8273\n",
      "Epoch : 23 --batch : 11 --- Loss :     4.3015\n",
      "Epoch : 23 --batch : 12 --- Loss :    25.7935\n",
      "Epoch : 23 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 23 --batch : 14 --- Loss :    10.9736\n",
      "Epoch : 23 --batch : 15 --- Loss :    12.5810\n",
      "Epoch : 23 --batch : 16 --- Loss :    10.0305\n",
      "Epoch : 23 --batch : 17 --- Loss :    15.3282\n",
      "Epoch : 23 --batch : 18 --- Loss :    16.9777\n",
      "Epoch : 23 --batch : 19 --- Loss :    15.4946\n",
      "Epoch : 23 --batch : 20 --- Loss :    16.3518\n",
      "Epoch : 23 --batch : 21 --- Loss :    28.1067\n",
      "Epoch : 23 --batch : 22 --- Loss :    14.4277\n",
      "Epoch : 23 --batch : 23 --- Loss :    14.2523\n",
      "Epoch : 23 --batch : 24 --- Loss :    23.3403\n",
      "Epoch : 23 --batch : 25 --- Loss :    16.1231\n",
      "Epoch : 23 --batch : 26 --- Loss :    10.9631\n",
      "Epoch : 23 --batch : 27 --- Loss :    23.3547\n",
      "Epoch : 23 --batch : 28 --- Loss :    24.7746\n",
      "Epoch : 23 --batch : 29 --- Loss :    11.5309\n",
      "Epoch : 23 --batch : 30 --- Loss :    11.7087\n",
      "Epoch : 23 --batch : 31 --- Loss :    15.8971\n",
      "Epoch : 23 --batch : 32 --- Loss :    22.3580\n",
      "Epoch : 23 --batch : 33 --- Loss :    15.1961\n",
      "Epoch : 23 --batch : 34 --- Loss :     3.6543\n",
      "Epoch : 23 --batch : 35 --- Loss :     9.8142\n",
      "Epoch : 23 --batch : 36 --- Loss :     6.2331\n",
      "Epoch : 23 --batch : 37 --- Loss :    14.7656\n",
      "Epoch : 23 --batch : 38 --- Loss :     2.1790\n",
      "Epoch : 23 --batch : 39 --- Loss :     8.1183\n",
      "Epoch : 23 --batch : 40 --- Loss :     5.1069\n",
      "Epoch : 23 --batch : 41 --- Loss :    24.0532\n",
      "Epoch : 23 --batch : 42 --- Loss :    12.9096\n",
      "Epoch : 23 --batch : 43 --- Loss :     9.2721\n",
      "Epoch : 23 --batch : 44 --- Loss :    24.5412\n",
      "Epoch : 23 --batch : 45 --- Loss :     3.0141\n",
      "Epoch : 23 --batch : 46 --- Loss :    11.9490\n",
      "Epoch : 23 --batch : 47 --- Loss :    16.7369\n",
      "Epoch : 23 --batch : 48 --- Loss :    12.3138\n",
      "Epoch : 23 --batch : 49 --- Loss :    11.1549\n",
      "Epoch : 23 --batch : 50 --- Loss :     8.9543\n",
      "Epoch : 23 --batch : 51 --- Loss :     8.9710\n",
      "Epoch : 23 --batch : 52 --- Loss :    15.7701\n",
      "Epoch : 23 --batch : 53 --- Loss :     7.1214\n",
      "Epoch : 23 --batch : 54 --- Loss :     4.8174\n",
      "Epoch : 23 --batch : 55 --- Loss :     7.9727\n",
      "Epoch : 23 --batch : 56 --- Loss :    13.9314\n",
      "Epoch : 23 --batch : 57 --- Loss :     0.8958\n",
      "Epoch : 23 --batch : 58 --- Loss :    15.2127\n",
      "Epoch : 23 --batch : 59 --- Loss :     8.4593\n",
      "Epoch : 23 --batch : 60 --- Loss :     2.1701\n",
      "Epoch : 23 --batch : 61 --- Loss :    10.6033\n",
      "Epoch : 23 --batch : 62 --- Loss :     9.0556\n",
      "Epoch : 23 --batch : 63 --- Loss :    14.8056\n",
      "Epoch : 23 --batch : 64 --- Loss :    11.3199\n",
      "Testing Accuracy after epoch 23 : 0.9665000051259994\n",
      "Epoch : 24 --batch : 1 --- Loss :    63.1044\n",
      "Epoch : 24 --batch : 2 --- Loss :    16.1941\n",
      "Epoch : 24 --batch : 3 --- Loss :    12.3369\n",
      "Epoch : 24 --batch : 4 --- Loss :    26.6832\n",
      "Epoch : 24 --batch : 5 --- Loss :     6.4007\n",
      "Epoch : 24 --batch : 6 --- Loss :    12.9734\n",
      "Epoch : 24 --batch : 7 --- Loss :    12.5586\n",
      "Epoch : 24 --batch : 8 --- Loss :     8.1026\n",
      "Epoch : 24 --batch : 9 --- Loss :     8.4454\n",
      "Epoch : 24 --batch : 10 --- Loss :     8.2188\n",
      "Epoch : 24 --batch : 11 --- Loss :     8.8232\n",
      "Epoch : 24 --batch : 12 --- Loss :    22.7607\n",
      "Epoch : 24 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 24 --batch : 14 --- Loss :     7.1315\n",
      "Epoch : 24 --batch : 15 --- Loss :     6.0887\n",
      "Epoch : 24 --batch : 16 --- Loss :     8.2419\n",
      "Epoch : 24 --batch : 17 --- Loss :    13.8004\n",
      "Epoch : 24 --batch : 18 --- Loss :    15.5762\n",
      "Epoch : 24 --batch : 19 --- Loss :    16.5247\n",
      "Epoch : 24 --batch : 20 --- Loss :    12.5188\n",
      "Epoch : 24 --batch : 21 --- Loss :    27.2647\n",
      "Epoch : 24 --batch : 22 --- Loss :    13.2984\n",
      "Epoch : 24 --batch : 23 --- Loss :    17.0453\n",
      "Epoch : 24 --batch : 24 --- Loss :    15.1681\n",
      "Epoch : 24 --batch : 25 --- Loss :     9.5985\n",
      "Epoch : 24 --batch : 26 --- Loss :     9.8429\n",
      "Epoch : 24 --batch : 27 --- Loss :    24.6373\n",
      "Epoch : 24 --batch : 28 --- Loss :    16.8288\n",
      "Epoch : 24 --batch : 29 --- Loss :    11.4703\n",
      "Epoch : 24 --batch : 30 --- Loss :    12.4798\n",
      "Epoch : 24 --batch : 31 --- Loss :    17.4353\n",
      "Epoch : 24 --batch : 32 --- Loss :    22.7467\n",
      "Epoch : 24 --batch : 33 --- Loss :    14.8153\n",
      "Epoch : 24 --batch : 34 --- Loss :     2.6434\n",
      "Epoch : 24 --batch : 35 --- Loss :    10.1334\n",
      "Epoch : 24 --batch : 36 --- Loss :     7.1746\n",
      "Epoch : 24 --batch : 37 --- Loss :    13.9563\n",
      "Epoch : 24 --batch : 38 --- Loss :     0.7070\n",
      "Epoch : 24 --batch : 39 --- Loss :     7.9331\n",
      "Epoch : 24 --batch : 40 --- Loss :     5.5910\n",
      "Epoch : 24 --batch : 41 --- Loss :    22.3913\n",
      "Epoch : 24 --batch : 42 --- Loss :    10.1620\n",
      "Epoch : 24 --batch : 43 --- Loss :     9.8229\n",
      "Epoch : 24 --batch : 44 --- Loss :    24.2280\n",
      "Epoch : 24 --batch : 45 --- Loss :     1.8276\n",
      "Epoch : 24 --batch : 46 --- Loss :    11.8085\n",
      "Epoch : 24 --batch : 47 --- Loss :    14.0043\n",
      "Epoch : 24 --batch : 48 --- Loss :    13.0064\n",
      "Epoch : 24 --batch : 49 --- Loss :    11.5819\n",
      "Epoch : 24 --batch : 50 --- Loss :     8.4879\n",
      "Epoch : 24 --batch : 51 --- Loss :    10.4318\n",
      "Epoch : 24 --batch : 52 --- Loss :    15.1908\n",
      "Epoch : 24 --batch : 53 --- Loss :     8.1686\n",
      "Epoch : 24 --batch : 54 --- Loss :     1.5472\n",
      "Epoch : 24 --batch : 55 --- Loss :     6.1558\n",
      "Epoch : 24 --batch : 56 --- Loss :    14.5604\n",
      "Epoch : 24 --batch : 57 --- Loss :     0.2429\n",
      "Epoch : 24 --batch : 58 --- Loss :    14.2464\n",
      "Epoch : 24 --batch : 59 --- Loss :     9.0955\n",
      "Epoch : 24 --batch : 60 --- Loss :     2.1077\n",
      "Epoch : 24 --batch : 61 --- Loss :     5.1992\n",
      "Epoch : 24 --batch : 62 --- Loss :     8.1941\n",
      "Epoch : 24 --batch : 63 --- Loss :    11.0327\n",
      "Epoch : 24 --batch : 64 --- Loss :     7.9318\n",
      "Testing Accuracy after epoch 24 : 0.970500010251999\n",
      "Epoch : 25 --batch : 1 --- Loss :    52.8368\n",
      "Epoch : 25 --batch : 2 --- Loss :    12.1436\n",
      "Epoch : 25 --batch : 3 --- Loss :     8.5187\n",
      "Epoch : 25 --batch : 4 --- Loss :    27.9035\n",
      "Epoch : 25 --batch : 5 --- Loss :     6.5383\n",
      "Epoch : 25 --batch : 6 --- Loss :    14.4879\n",
      "Epoch : 25 --batch : 7 --- Loss :    12.4746\n",
      "Epoch : 25 --batch : 8 --- Loss :     5.8253\n",
      "Epoch : 25 --batch : 9 --- Loss :     8.0039\n",
      "Epoch : 25 --batch : 10 --- Loss :     7.0942\n",
      "Epoch : 25 --batch : 11 --- Loss :     2.1339\n",
      "Epoch : 25 --batch : 12 --- Loss :    22.6302\n",
      "Epoch : 25 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 25 --batch : 14 --- Loss :     4.6703\n",
      "Epoch : 25 --batch : 15 --- Loss :     2.9317\n",
      "Epoch : 25 --batch : 16 --- Loss :     7.8406\n",
      "Epoch : 25 --batch : 17 --- Loss :    11.6220\n",
      "Epoch : 25 --batch : 18 --- Loss :    15.0170\n",
      "Epoch : 25 --batch : 19 --- Loss :    15.4353\n",
      "Epoch : 25 --batch : 20 --- Loss :    13.7012\n",
      "Epoch : 25 --batch : 21 --- Loss :    26.7240\n",
      "Epoch : 25 --batch : 22 --- Loss :    10.6471\n",
      "Epoch : 25 --batch : 23 --- Loss :    15.3720\n",
      "Epoch : 25 --batch : 24 --- Loss :    13.1188\n",
      "Epoch : 25 --batch : 25 --- Loss :    12.1816\n",
      "Epoch : 25 --batch : 26 --- Loss :     5.7360\n",
      "Epoch : 25 --batch : 27 --- Loss :    16.3376\n",
      "Epoch : 25 --batch : 28 --- Loss :    15.8478\n",
      "Epoch : 25 --batch : 29 --- Loss :    10.9263\n",
      "Epoch : 25 --batch : 30 --- Loss :    12.8391\n",
      "Epoch : 25 --batch : 31 --- Loss :    13.2391\n",
      "Epoch : 25 --batch : 32 --- Loss :    23.5171\n",
      "Epoch : 25 --batch : 33 --- Loss :     6.0500\n",
      "Epoch : 25 --batch : 34 --- Loss :     6.8339\n",
      "Epoch : 25 --batch : 35 --- Loss :    11.0569\n",
      "Epoch : 25 --batch : 36 --- Loss :     5.9977\n",
      "Epoch : 25 --batch : 37 --- Loss :    13.5505\n",
      "Epoch : 25 --batch : 38 --- Loss :     1.8242\n",
      "Epoch : 25 --batch : 39 --- Loss :    10.3973\n",
      "Epoch : 25 --batch : 40 --- Loss :     5.8459\n",
      "Epoch : 25 --batch : 41 --- Loss :    21.5764\n",
      "Epoch : 25 --batch : 42 --- Loss :     9.9233\n",
      "Epoch : 25 --batch : 43 --- Loss :     6.4370\n",
      "Epoch : 25 --batch : 44 --- Loss :    22.7220\n",
      "Epoch : 25 --batch : 45 --- Loss :     3.8702\n",
      "Epoch : 25 --batch : 46 --- Loss :    10.6119\n",
      "Epoch : 25 --batch : 47 --- Loss :    10.5607\n",
      "Epoch : 25 --batch : 48 --- Loss :    12.5751\n",
      "Epoch : 25 --batch : 49 --- Loss :    10.8820\n",
      "Epoch : 25 --batch : 50 --- Loss :     7.0642\n",
      "Epoch : 25 --batch : 51 --- Loss :     8.8012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 25 --batch : 52 --- Loss :    14.9964\n",
      "Epoch : 25 --batch : 53 --- Loss :     9.4793\n",
      "Epoch : 25 --batch : 54 --- Loss :     4.5509\n",
      "Epoch : 25 --batch : 55 --- Loss :     6.0892\n",
      "Epoch : 25 --batch : 56 --- Loss :    13.1147\n",
      "Epoch : 25 --batch : 57 --- Loss :     0.8940\n",
      "Epoch : 25 --batch : 58 --- Loss :    14.3060\n",
      "Epoch : 25 --batch : 59 --- Loss :     8.2748\n",
      "Epoch : 25 --batch : 60 --- Loss :     1.9267\n",
      "Epoch : 25 --batch : 61 --- Loss :     4.7159\n",
      "Epoch : 25 --batch : 62 --- Loss :    10.0880\n",
      "Epoch : 25 --batch : 63 --- Loss :    13.4242\n",
      "Epoch : 25 --batch : 64 --- Loss :     6.0413\n",
      "Testing Accuracy after epoch 25 : 0.969500008225441\n",
      "Epoch : 26 --batch : 1 --- Loss :    49.4212\n",
      "Epoch : 26 --batch : 2 --- Loss :    13.5361\n",
      "Epoch : 26 --batch : 3 --- Loss :     5.7621\n",
      "Epoch : 26 --batch : 4 --- Loss :    24.2896\n",
      "Epoch : 26 --batch : 5 --- Loss :     6.3085\n",
      "Epoch : 26 --batch : 6 --- Loss :    11.9770\n",
      "Epoch : 26 --batch : 7 --- Loss :    10.4402\n",
      "Epoch : 26 --batch : 8 --- Loss :     4.7040\n",
      "Epoch : 26 --batch : 9 --- Loss :     6.2743\n",
      "Epoch : 26 --batch : 10 --- Loss :     4.1581\n",
      "Epoch : 26 --batch : 11 --- Loss :     4.5814\n",
      "Epoch : 26 --batch : 12 --- Loss :    25.3620\n",
      "Epoch : 26 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 26 --batch : 14 --- Loss :     6.1383\n",
      "Epoch : 26 --batch : 15 --- Loss :     7.5387\n",
      "Epoch : 26 --batch : 16 --- Loss :     8.3502\n",
      "Epoch : 26 --batch : 17 --- Loss :    18.4335\n",
      "Epoch : 26 --batch : 18 --- Loss :    21.4409\n",
      "Epoch : 26 --batch : 19 --- Loss :    15.0682\n",
      "Epoch : 26 --batch : 20 --- Loss :    15.7534\n",
      "Epoch : 26 --batch : 21 --- Loss :    25.8931\n",
      "Epoch : 26 --batch : 22 --- Loss :     8.0677\n",
      "Epoch : 26 --batch : 23 --- Loss :    13.0711\n",
      "Epoch : 26 --batch : 24 --- Loss :    21.9876\n",
      "Epoch : 26 --batch : 25 --- Loss :    13.8542\n",
      "Epoch : 26 --batch : 26 --- Loss :    13.9918\n",
      "Epoch : 26 --batch : 27 --- Loss :    18.6529\n",
      "Epoch : 26 --batch : 28 --- Loss :    19.8317\n",
      "Epoch : 26 --batch : 29 --- Loss :    12.1451\n",
      "Epoch : 26 --batch : 30 --- Loss :    14.7447\n",
      "Epoch : 26 --batch : 31 --- Loss :    16.3996\n",
      "Epoch : 26 --batch : 32 --- Loss :    22.0794\n",
      "Epoch : 26 --batch : 33 --- Loss :    11.1132\n",
      "Epoch : 26 --batch : 34 --- Loss :     3.1033\n",
      "Epoch : 26 --batch : 35 --- Loss :     9.3086\n",
      "Epoch : 26 --batch : 36 --- Loss :     6.4316\n",
      "Epoch : 26 --batch : 37 --- Loss :    21.5585\n",
      "Epoch : 26 --batch : 38 --- Loss :     4.2902\n",
      "Epoch : 26 --batch : 39 --- Loss :     8.6317\n",
      "Epoch : 26 --batch : 40 --- Loss :     4.1717\n",
      "Epoch : 26 --batch : 41 --- Loss :    20.0729\n",
      "Epoch : 26 --batch : 42 --- Loss :    12.0104\n",
      "Epoch : 26 --batch : 43 --- Loss :     6.6245\n",
      "Epoch : 26 --batch : 44 --- Loss :    20.8980\n",
      "Epoch : 26 --batch : 45 --- Loss :     2.0765\n",
      "Epoch : 26 --batch : 46 --- Loss :     9.3325\n",
      "Epoch : 26 --batch : 47 --- Loss :    18.9897\n",
      "Epoch : 26 --batch : 48 --- Loss :    11.5489\n",
      "Epoch : 26 --batch : 49 --- Loss :     8.6485\n",
      "Epoch : 26 --batch : 50 --- Loss :     8.1868\n",
      "Epoch : 26 --batch : 51 --- Loss :     9.2495\n",
      "Epoch : 26 --batch : 52 --- Loss :    22.9891\n",
      "Epoch : 26 --batch : 53 --- Loss :     8.0528\n",
      "Epoch : 26 --batch : 54 --- Loss :     0.7891\n",
      "Epoch : 26 --batch : 55 --- Loss :    10.9308\n",
      "Epoch : 26 --batch : 56 --- Loss :    14.0545\n",
      "Epoch : 26 --batch : 57 --- Loss :     2.8421\n",
      "Epoch : 26 --batch : 58 --- Loss :    12.6422\n",
      "Epoch : 26 --batch : 59 --- Loss :     7.9996\n",
      "Epoch : 26 --batch : 60 --- Loss :     2.6534\n",
      "Epoch : 26 --batch : 61 --- Loss :     9.5020\n",
      "Epoch : 26 --batch : 62 --- Loss :     7.6847\n",
      "Epoch : 26 --batch : 63 --- Loss :    11.1704\n",
      "Epoch : 26 --batch : 64 --- Loss :     7.8072\n",
      "Testing Accuracy after epoch 26 : 0.9730000048875809\n",
      "Epoch : 27 --batch : 1 --- Loss :    51.0645\n",
      "Epoch : 27 --batch : 2 --- Loss :    13.8935\n",
      "Epoch : 27 --batch : 3 --- Loss :     8.7380\n",
      "Epoch : 27 --batch : 4 --- Loss :    24.4053\n",
      "Epoch : 27 --batch : 5 --- Loss :     6.9170\n",
      "Epoch : 27 --batch : 6 --- Loss :    12.7098\n",
      "Epoch : 27 --batch : 7 --- Loss :    12.6658\n",
      "Epoch : 27 --batch : 8 --- Loss :     5.1762\n",
      "Epoch : 27 --batch : 9 --- Loss :     7.3882\n",
      "Epoch : 27 --batch : 10 --- Loss :     3.8392\n",
      "Epoch : 27 --batch : 11 --- Loss :     4.8906\n",
      "Epoch : 27 --batch : 12 --- Loss :    25.7317\n",
      "Epoch : 27 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 27 --batch : 14 --- Loss :     5.9316\n",
      "Epoch : 27 --batch : 15 --- Loss :     5.1035\n",
      "Epoch : 27 --batch : 16 --- Loss :     8.4749\n",
      "Epoch : 27 --batch : 17 --- Loss :    20.6995\n",
      "Epoch : 27 --batch : 18 --- Loss :    16.7205\n",
      "Epoch : 27 --batch : 19 --- Loss :    18.7124\n",
      "Epoch : 27 --batch : 20 --- Loss :    16.8402\n",
      "Epoch : 27 --batch : 21 --- Loss :    30.5980\n",
      "Epoch : 27 --batch : 22 --- Loss :     8.1005\n",
      "Epoch : 27 --batch : 23 --- Loss :    14.3742\n",
      "Epoch : 27 --batch : 24 --- Loss :    14.7885\n",
      "Epoch : 27 --batch : 25 --- Loss :     9.2795\n",
      "Epoch : 27 --batch : 26 --- Loss :     8.6613\n",
      "Epoch : 27 --batch : 27 --- Loss :    22.1462\n",
      "Epoch : 27 --batch : 28 --- Loss :    17.7644\n",
      "Epoch : 27 --batch : 29 --- Loss :    10.2213\n",
      "Epoch : 27 --batch : 30 --- Loss :    10.8590\n",
      "Epoch : 27 --batch : 31 --- Loss :    13.0577\n",
      "Epoch : 27 --batch : 32 --- Loss :    21.3166\n",
      "Epoch : 27 --batch : 33 --- Loss :     8.8345\n",
      "Epoch : 27 --batch : 34 --- Loss :     3.7559\n",
      "Epoch : 27 --batch : 35 --- Loss :     8.7135\n",
      "Epoch : 27 --batch : 36 --- Loss :     5.6882\n",
      "Epoch : 27 --batch : 37 --- Loss :    13.0645\n",
      "Epoch : 27 --batch : 38 --- Loss :     3.2221\n",
      "Epoch : 27 --batch : 39 --- Loss :     9.7309\n",
      "Epoch : 27 --batch : 40 --- Loss :     5.7111\n",
      "Epoch : 27 --batch : 41 --- Loss :    17.0088\n",
      "Epoch : 27 --batch : 42 --- Loss :    11.9347\n",
      "Epoch : 27 --batch : 43 --- Loss :     7.4127\n",
      "Epoch : 27 --batch : 44 --- Loss :    23.3019\n",
      "Epoch : 27 --batch : 45 --- Loss :     3.5142\n",
      "Epoch : 27 --batch : 46 --- Loss :     7.9546\n",
      "Epoch : 27 --batch : 47 --- Loss :    14.2992\n",
      "Epoch : 27 --batch : 48 --- Loss :    11.1748\n",
      "Epoch : 27 --batch : 49 --- Loss :     9.5908\n",
      "Epoch : 27 --batch : 50 --- Loss :     7.2592\n",
      "Epoch : 27 --batch : 51 --- Loss :     9.0722\n",
      "Epoch : 27 --batch : 52 --- Loss :    15.5384\n",
      "Epoch : 27 --batch : 53 --- Loss :     6.9227\n",
      "Epoch : 27 --batch : 54 --- Loss :     3.1953\n",
      "Epoch : 27 --batch : 55 --- Loss :    15.1663\n",
      "Epoch : 27 --batch : 56 --- Loss :    13.4078\n",
      "Epoch : 27 --batch : 57 --- Loss :     2.4635\n",
      "Epoch : 27 --batch : 58 --- Loss :    13.6320\n",
      "Epoch : 27 --batch : 59 --- Loss :     7.5446\n",
      "Epoch : 27 --batch : 60 --- Loss :     4.4837\n",
      "Epoch : 27 --batch : 61 --- Loss :     5.5769\n",
      "Epoch : 27 --batch : 62 --- Loss :     6.8909\n",
      "Epoch : 27 --batch : 63 --- Loss :    10.5590\n",
      "Epoch : 27 --batch : 64 --- Loss :     8.9327\n",
      "Testing Accuracy after epoch 27 : 0.972500005364418\n",
      "Epoch : 28 --batch : 1 --- Loss :    49.3347\n",
      "Epoch : 28 --batch : 2 --- Loss :    11.7720\n",
      "Epoch : 28 --batch : 3 --- Loss :     6.9004\n",
      "Epoch : 28 --batch : 4 --- Loss :    26.6148\n",
      "Epoch : 28 --batch : 5 --- Loss :     5.9429\n",
      "Epoch : 28 --batch : 6 --- Loss :    13.7595\n",
      "Epoch : 28 --batch : 7 --- Loss :    11.9825\n",
      "Epoch : 28 --batch : 8 --- Loss :     5.1255\n",
      "Epoch : 28 --batch : 9 --- Loss :     7.2773\n",
      "Epoch : 28 --batch : 10 --- Loss :     6.9350\n",
      "Epoch : 28 --batch : 11 --- Loss :     5.3095\n",
      "Epoch : 28 --batch : 12 --- Loss :    21.4653\n",
      "Epoch : 28 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 28 --batch : 14 --- Loss :    10.7767\n",
      "Epoch : 28 --batch : 15 --- Loss :    13.7171\n",
      "Epoch : 28 --batch : 16 --- Loss :    12.1721\n",
      "Epoch : 28 --batch : 17 --- Loss :    19.4885\n",
      "Epoch : 28 --batch : 18 --- Loss :    17.1345\n",
      "Epoch : 28 --batch : 19 --- Loss :    14.6088\n",
      "Epoch : 28 --batch : 20 --- Loss :    17.9934\n",
      "Epoch : 28 --batch : 21 --- Loss :    28.5002\n",
      "Epoch : 28 --batch : 22 --- Loss :     9.9782\n",
      "Epoch : 28 --batch : 23 --- Loss :    13.3271\n",
      "Epoch : 28 --batch : 24 --- Loss :    10.4462\n",
      "Epoch : 28 --batch : 25 --- Loss :    10.6582\n",
      "Epoch : 28 --batch : 26 --- Loss :    12.6434\n",
      "Epoch : 28 --batch : 27 --- Loss :    21.5521\n",
      "Epoch : 28 --batch : 28 --- Loss :    16.9049\n",
      "Epoch : 28 --batch : 29 --- Loss :    11.8227\n",
      "Epoch : 28 --batch : 30 --- Loss :    10.3056\n",
      "Epoch : 28 --batch : 31 --- Loss :    12.2490\n",
      "Epoch : 28 --batch : 32 --- Loss :    23.7905\n",
      "Epoch : 28 --batch : 33 --- Loss :    14.5921\n",
      "Epoch : 28 --batch : 34 --- Loss :     2.0719\n",
      "Epoch : 28 --batch : 35 --- Loss :     9.0889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 28 --batch : 36 --- Loss :     5.5416\n",
      "Epoch : 28 --batch : 37 --- Loss :    15.5845\n",
      "Epoch : 28 --batch : 38 --- Loss :     0.8267\n",
      "Epoch : 28 --batch : 39 --- Loss :     6.5982\n",
      "Epoch : 28 --batch : 40 --- Loss :     4.5715\n",
      "Epoch : 28 --batch : 41 --- Loss :    15.5694\n",
      "Epoch : 28 --batch : 42 --- Loss :    11.2593\n",
      "Epoch : 28 --batch : 43 --- Loss :     6.9654\n",
      "Epoch : 28 --batch : 44 --- Loss :    16.1390\n",
      "Epoch : 28 --batch : 45 --- Loss :     1.7650\n",
      "Epoch : 28 --batch : 46 --- Loss :     8.0739\n",
      "Epoch : 28 --batch : 47 --- Loss :    11.2936\n",
      "Epoch : 28 --batch : 48 --- Loss :    10.1193\n",
      "Epoch : 28 --batch : 49 --- Loss :    11.6362\n",
      "Epoch : 28 --batch : 50 --- Loss :     8.4934\n",
      "Epoch : 28 --batch : 51 --- Loss :    10.1656\n",
      "Epoch : 28 --batch : 52 --- Loss :    22.3855\n",
      "Epoch : 28 --batch : 53 --- Loss :     5.4570\n",
      "Epoch : 28 --batch : 54 --- Loss :     0.6856\n",
      "Epoch : 28 --batch : 55 --- Loss :    11.4377\n",
      "Epoch : 28 --batch : 56 --- Loss :    14.2677\n",
      "Epoch : 28 --batch : 57 --- Loss :     9.0836\n",
      "Epoch : 28 --batch : 58 --- Loss :    19.1369\n",
      "Epoch : 28 --batch : 59 --- Loss :     7.5843\n",
      "Epoch : 28 --batch : 60 --- Loss :     1.6313\n",
      "Epoch : 28 --batch : 61 --- Loss :     6.4542\n",
      "Epoch : 28 --batch : 62 --- Loss :     6.5562\n",
      "Epoch : 28 --batch : 63 --- Loss :     8.6836\n",
      "Epoch : 28 --batch : 64 --- Loss :     6.6778\n",
      "Testing Accuracy after epoch 28 : 0.9715000092983246\n",
      "Epoch : 29 --batch : 1 --- Loss :    50.4145\n",
      "Epoch : 29 --batch : 2 --- Loss :    13.1091\n",
      "Epoch : 29 --batch : 3 --- Loss :     3.6997\n",
      "Epoch : 29 --batch : 4 --- Loss :    15.2892\n",
      "Epoch : 29 --batch : 5 --- Loss :     5.8591\n",
      "Epoch : 29 --batch : 6 --- Loss :    13.6602\n",
      "Epoch : 29 --batch : 7 --- Loss :     9.1601\n",
      "Epoch : 29 --batch : 8 --- Loss :     5.7363\n",
      "Epoch : 29 --batch : 9 --- Loss :     6.7709\n",
      "Epoch : 29 --batch : 10 --- Loss :     4.1964\n",
      "Epoch : 29 --batch : 11 --- Loss :     7.4025\n",
      "Epoch : 29 --batch : 12 --- Loss :    21.6159\n",
      "Epoch : 29 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 29 --batch : 14 --- Loss :     7.9544\n",
      "Epoch : 29 --batch : 15 --- Loss :     2.4818\n",
      "Epoch : 29 --batch : 16 --- Loss :     9.7752\n",
      "Epoch : 29 --batch : 17 --- Loss :    20.1908\n",
      "Epoch : 29 --batch : 18 --- Loss :    18.0080\n",
      "Epoch : 29 --batch : 19 --- Loss :    15.4574\n",
      "Epoch : 29 --batch : 20 --- Loss :     9.1736\n",
      "Epoch : 29 --batch : 21 --- Loss :    25.1205\n",
      "Epoch : 29 --batch : 22 --- Loss :     7.9601\n",
      "Epoch : 29 --batch : 23 --- Loss :    14.0494\n",
      "Epoch : 29 --batch : 24 --- Loss :    12.1806\n",
      "Epoch : 29 --batch : 25 --- Loss :     8.5282\n",
      "Epoch : 29 --batch : 26 --- Loss :    15.6100\n",
      "Epoch : 29 --batch : 27 --- Loss :    15.1831\n",
      "Epoch : 29 --batch : 28 --- Loss :    16.6405\n",
      "Epoch : 29 --batch : 29 --- Loss :    12.0467\n",
      "Epoch : 29 --batch : 30 --- Loss :    10.1481\n",
      "Epoch : 29 --batch : 31 --- Loss :    11.3499\n",
      "Epoch : 29 --batch : 32 --- Loss :    20.1133\n",
      "Epoch : 29 --batch : 33 --- Loss :    11.6991\n",
      "Epoch : 29 --batch : 34 --- Loss :     2.2832\n",
      "Epoch : 29 --batch : 35 --- Loss :     9.0506\n",
      "Epoch : 29 --batch : 36 --- Loss :     5.2283\n",
      "Epoch : 29 --batch : 37 --- Loss :    16.2419\n",
      "Epoch : 29 --batch : 38 --- Loss :     2.4517\n",
      "Epoch : 29 --batch : 39 --- Loss :     7.4428\n",
      "Epoch : 29 --batch : 40 --- Loss :     4.4948\n",
      "Epoch : 29 --batch : 41 --- Loss :    15.8108\n",
      "Epoch : 29 --batch : 42 --- Loss :    12.7046\n",
      "Epoch : 29 --batch : 43 --- Loss :     7.2379\n",
      "Epoch : 29 --batch : 44 --- Loss :    19.7599\n",
      "Epoch : 29 --batch : 45 --- Loss :     0.8220\n",
      "Epoch : 29 --batch : 46 --- Loss :     8.9333\n",
      "Epoch : 29 --batch : 47 --- Loss :     9.8862\n",
      "Epoch : 29 --batch : 48 --- Loss :     9.4148\n",
      "Epoch : 29 --batch : 49 --- Loss :     9.1581\n",
      "Epoch : 29 --batch : 50 --- Loss :     7.5089\n",
      "Epoch : 29 --batch : 51 --- Loss :     8.0805\n",
      "Epoch : 29 --batch : 52 --- Loss :    11.2716\n",
      "Epoch : 29 --batch : 53 --- Loss :     7.1185\n",
      "Epoch : 29 --batch : 54 --- Loss :     2.4215\n",
      "Epoch : 29 --batch : 55 --- Loss :     5.0444\n",
      "Epoch : 29 --batch : 56 --- Loss :    10.4468\n",
      "Epoch : 29 --batch : 57 --- Loss :     0.0000\n",
      "Epoch : 29 --batch : 58 --- Loss :    11.8778\n",
      "Epoch : 29 --batch : 59 --- Loss :     7.4094\n",
      "Epoch : 29 --batch : 60 --- Loss :     1.8309\n",
      "Epoch : 29 --batch : 61 --- Loss :     2.7496\n",
      "Epoch : 29 --batch : 62 --- Loss :     5.2116\n",
      "Epoch : 29 --batch : 63 --- Loss :     6.8871\n",
      "Epoch : 29 --batch : 64 --- Loss :     7.4974\n",
      "Testing Accuracy after epoch 29 : 0.9735000103712081\n",
      "Epoch : 30 --batch : 1 --- Loss :    47.7503\n",
      "Epoch : 30 --batch : 2 --- Loss :    12.8526\n",
      "Epoch : 30 --batch : 3 --- Loss :     9.1490\n",
      "Epoch : 30 --batch : 4 --- Loss :    15.4295\n",
      "Epoch : 30 --batch : 5 --- Loss :     5.9998\n",
      "Epoch : 30 --batch : 6 --- Loss :     8.7707\n",
      "Epoch : 30 --batch : 7 --- Loss :    10.7681\n",
      "Epoch : 30 --batch : 8 --- Loss :     4.9795\n",
      "Epoch : 30 --batch : 9 --- Loss :     6.5971\n",
      "Epoch : 30 --batch : 10 --- Loss :     2.0634\n",
      "Epoch : 30 --batch : 11 --- Loss :     3.6969\n",
      "Epoch : 30 --batch : 12 --- Loss :    20.9719\n",
      "Epoch : 30 --batch : 13 --- Loss :     0.0000\n",
      "Epoch : 30 --batch : 14 --- Loss :     5.3568\n",
      "Epoch : 30 --batch : 15 --- Loss :     6.9127\n",
      "Epoch : 30 --batch : 16 --- Loss :     8.0074\n",
      "Epoch : 30 --batch : 17 --- Loss :    16.1516\n",
      "Epoch : 30 --batch : 18 --- Loss :    15.8019\n",
      "Epoch : 30 --batch : 19 --- Loss :    14.7565\n",
      "Epoch : 30 --batch : 20 --- Loss :    13.3297\n",
      "Epoch : 30 --batch : 21 --- Loss :    21.5054\n",
      "Epoch : 30 --batch : 22 --- Loss :     8.9713\n",
      "Epoch : 30 --batch : 23 --- Loss :    13.3724\n",
      "Epoch : 30 --batch : 24 --- Loss :    13.3929\n",
      "Epoch : 30 --batch : 25 --- Loss :     6.6668\n",
      "Epoch : 30 --batch : 26 --- Loss :    11.5626\n",
      "Epoch : 30 --batch : 27 --- Loss :    14.2339\n",
      "Epoch : 30 --batch : 28 --- Loss :    14.3101\n",
      "Epoch : 30 --batch : 29 --- Loss :    11.9831\n",
      "Epoch : 30 --batch : 30 --- Loss :     9.1921\n",
      "Epoch : 30 --batch : 31 --- Loss :    10.9633\n",
      "Epoch : 30 --batch : 32 --- Loss :    20.1270\n",
      "Epoch : 30 --batch : 33 --- Loss :     9.3144\n",
      "Epoch : 30 --batch : 34 --- Loss :     0.0000\n",
      "Epoch : 30 --batch : 35 --- Loss :     8.4324\n",
      "Epoch : 30 --batch : 36 --- Loss :     5.5260\n",
      "Epoch : 30 --batch : 37 --- Loss :    11.8362\n",
      "Epoch : 30 --batch : 38 --- Loss :     1.0091\n",
      "Epoch : 30 --batch : 39 --- Loss :    10.8256\n",
      "Epoch : 30 --batch : 40 --- Loss :     6.6879\n",
      "Epoch : 30 --batch : 41 --- Loss :    16.0761\n",
      "Epoch : 30 --batch : 42 --- Loss :    10.7228\n",
      "Epoch : 30 --batch : 43 --- Loss :     5.6613\n",
      "Epoch : 30 --batch : 44 --- Loss :    19.6074\n",
      "Epoch : 30 --batch : 45 --- Loss :     1.4467\n",
      "Epoch : 30 --batch : 46 --- Loss :     8.0584\n",
      "Epoch : 30 --batch : 47 --- Loss :     8.7757\n",
      "Epoch : 30 --batch : 48 --- Loss :     9.6212\n",
      "Epoch : 30 --batch : 49 --- Loss :     8.5973\n",
      "Epoch : 30 --batch : 50 --- Loss :     7.1792\n",
      "Epoch : 30 --batch : 51 --- Loss :     8.3163\n",
      "Epoch : 30 --batch : 52 --- Loss :    12.3292\n",
      "Epoch : 30 --batch : 53 --- Loss :     4.5607\n",
      "Epoch : 30 --batch : 54 --- Loss :     0.7101\n",
      "Epoch : 30 --batch : 55 --- Loss :     4.0473\n",
      "Epoch : 30 --batch : 56 --- Loss :    12.0811\n",
      "Epoch : 30 --batch : 57 --- Loss :     2.7634\n",
      "Epoch : 30 --batch : 58 --- Loss :    17.9593\n",
      "Epoch : 30 --batch : 59 --- Loss :     7.1381\n",
      "Epoch : 30 --batch : 60 --- Loss :     4.1351\n",
      "Epoch : 30 --batch : 61 --- Loss :     6.2619\n",
      "Epoch : 30 --batch : 62 --- Loss :     9.8621\n",
      "Epoch : 30 --batch : 63 --- Loss :     9.2256\n",
      "Epoch : 30 --batch : 64 --- Loss :     6.6916\n",
      "Testing Accuracy after epoch 30 : 0.9735000103712081\n",
      "Model saved in file: model/my_model\n"
     ]
    }
   ],
   "source": [
    "#CNN model constructed for training of data\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 30\n",
    "n_classes = 2\n",
    "dropout = 0.80\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32],mean=0.0,stddev=1.0)),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64],mean=0.0,stddev=1.0)),\n",
    "    'wd1': tf.Variable(tf.random_normal([32*18*64, 1024],mean=0.0,stddev=1.0)),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes],mean=0.0,stddev=1.0))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 128*72*1 to 64*36*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 64*36*32 to 32*18*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Fully connected layer - 32*18*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 2\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None,128,72,1])\n",
    "y = tf.placeholder(tf.float32, [None,n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model--output for each image\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# Launch the graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #batches in form of matrix\n",
    "        for i in range(0,64) :\n",
    "            batch_x = X_train_batch[i]\n",
    "            batch_y = Y_train_batch[i]\n",
    "        \n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "            print('Epoch : {:>2} --batch : {} --- Loss : {:>10.4f}'.format(epoch + 1,i+1,loss))\n",
    "        \n",
    "        test_acc_array=np.array([])\n",
    "        for i in range(0,20) :\n",
    "            batch_x_test = X_test_batch[i]\n",
    "            batch_y_test = Y_test_batch[i]\n",
    "            \n",
    "            test_acc = sess.run(accuracy, feed_dict={\n",
    "              x:batch_x_test,\n",
    "              y:batch_y_test,\n",
    "              keep_prob: 1.0})\n",
    "            \n",
    "            test_acc_array=np.append(test_acc_array,test_acc,axis=None)\n",
    "    \n",
    "        print('Testing Accuracy after epoch {} : {}'.format(epoch+1,np.mean(test_acc_array)))\n",
    "\n",
    "    weights_updated = {\n",
    "        'wc1_updated': tf.Variable(weights['wc1'],name='wc1_updated'),\n",
    "        'wc2_updated': tf.Variable(weights['wc2'],name='wc2_updated'),\n",
    "        'wd1_updated': tf.Variable(weights['wd1'],name='wd1_updated'),\n",
    "        'w_out_updated': tf.Variable(weights['out'],name='w_out_updated')}\n",
    "\n",
    "    biases_updated = {\n",
    "        'bc1_updated': tf.Variable(biases['bc1'],name='bc1_updated'),\n",
    "        'bc2_updated': tf.Variable(biases['bc2'],name='bc2_updated'),\n",
    "        'bd1_updated': tf.Variable(biases['bd1'],name='bd1_updated'),\n",
    "        'b_out_updated': tf.Variable(biases['out'],name='b_out_updated')}\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #model to be saved in file\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"model/my_model\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a136f505f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot of accuracy against epochs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x=np.arange(1,31)\n",
    "y=np.array([0.8250,0.8745,0.8915,0.8930,0.9010,0.9330,0.9370,0.9520,0.9505,0.9485,0.9545,0.9555,0.9575,0.9600,0.9610,0.9640,\n",
    "            0.9605,0.9610,0.9675,0.9680,0.9645,0.9690,0.9665,0.9705,0.9695,0.9730,0.9725,0.9715,0.9735,0.9735\n",
    "           ])\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Epoch vs Accuracy \")\n",
    "plt.ylim([0.8,1])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
