{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDIAN SIGN LANGUAGE RECOGNITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indian Sign Language(ISL) forms the most widely accepted communication method used by the deaf and mute communities of India. The vocabulary of ISL is formed by various hand gestures, facial expressions and certain body movements. The fact being that India is home to more than 12 million deaf people, a way to lessen the communication gap between the deaf community and ordinary people is essential.\n",
    " \n",
    "That said, we need a model that could overcome many difficulties posed by the recognition process. But technically speaking, no public dataset, little distortion from signer to signer, size of hands, color of skin, noise in background everything poses a difficulty while training a model. Furthermore, to make an effective model, data that encapsulates all these changes must be formed from scratch.\n",
    " \n",
    "Although there are several problems those hinder the project, the solution to the problem has been tried by many researchers, both in the field of computer vision and machine learning. And I wish to contribute to the cause myself using the knowledge I have gained through this nanodegree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindata=pd.read_csv('traindata.csv')\n",
    "traindata=traindata.drop(['Unnamed: 0'],axis=1)\n",
    "labeldata=pd.read_csv('labeldata.csv')\n",
    "labeldata=labeldata.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9765 images in the dataset\n",
      "There are 15 classes for the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} images in the dataset\".format(traindata.shape[0]))\n",
    "print(\"There are {} classes for the dataset\".format(labeldata.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8765, 3072)\n",
      "(8765, 15)\n",
      "(1000, 3072)\n",
      "(1000, 15)\n",
      "There are 8765 images in the training dataset\n",
      "There are 1000 images in the testing dataset\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(traindata,labeldata, test_size=0.1024, random_state=100)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "print(\"There are {} images in the training dataset\".format(X_train.shape[0]))\n",
    "print(\"There are {} images in the testing dataset\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give the idea of how the captured images after skin detection and largest contour detection looks, lets see some random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAD8CAYAAADkM2ZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCA5AkDi/wFnQ53jVt37JwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23102ececf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#change the image_number and you can plot any image that you want to visualize\n",
    "\n",
    "image_number = 4200\n",
    "import matplotlib.pyplot as plt\n",
    "img1 = traindata[image_number:image_number+1]\n",
    "img1 = img1.values.reshape(48,64)\n",
    "plt.imshow(img1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1753 images in each batch\n"
     ]
    }
   ],
   "source": [
    "#Since my laptop doesn't have much RAM, I have divided the training dataset into 5 batches\n",
    "\n",
    "print(\"There are {} images in each batch\".format(int(len(X_train)/5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually splitting each batch into 1750 images each\n",
    "\n",
    "\n",
    "len_X_train = int(len(X_train)/5)-3\n",
    "\n",
    "X_train_1 = X_train[:len_X_train]\n",
    "Y_train_1 = Y_train[:len_X_train]\n",
    "\n",
    "X_train_2 = X_train[len_X_train : len_X_train*2]\n",
    "Y_train_2 = Y_train[len_X_train : len_X_train*2]\n",
    "\n",
    "X_train_3 = X_train[len_X_train*2 : len_X_train*3]\n",
    "Y_train_3 = Y_train[len_X_train*2 : len_X_train*3]\n",
    "\n",
    "X_train_4 = X_train[len_X_train*3 : len_X_train*4]\n",
    "Y_train_4 = Y_train[len_X_train*3 : len_X_train*4]\n",
    "\n",
    "X_train_5 = X_train[len_X_train*4 :]\n",
    "Y_train_5 = Y_train[len_X_train*4 :]\n",
    "\n",
    "\n",
    "#modifying the data to fit into the CNN\n",
    "\n",
    "X_train_1=X_train_1.as_matrix()\n",
    "Y_train_1=Y_train_1.as_matrix()\n",
    "\n",
    "X_train_2=X_train_2.as_matrix()\n",
    "Y_train_2=Y_train_2.as_matrix()\n",
    "\n",
    "X_train_3=X_train_3.as_matrix()\n",
    "Y_train_3=Y_train_3.as_matrix()\n",
    "\n",
    "X_train_4=X_train_4.as_matrix()\n",
    "Y_train_4=Y_train_4.as_matrix()\n",
    "\n",
    "X_train_5=X_train_5.as_matrix()\n",
    "Y_train_5=Y_train_5.as_matrix()\n",
    "\n",
    "X_test=X_test.as_matrix()\n",
    "Y_test=Y_test.as_matrix()\n",
    "\n",
    "X_train_1=X_train_1.reshape(1750,64,48,1)\n",
    "X_train_2=X_train_2.reshape(1750,64,48,1)\n",
    "X_train_3=X_train_3.reshape(1750,64,48,1)\n",
    "X_train_4=X_train_4.reshape(1750,64,48,1)\n",
    "X_train_5=X_train_5.reshape(1765,64,48,1)\n",
    "X_test=X_test.reshape(1000,64,48,1)\n",
    "\n",
    "\n",
    "#this function returns a training batch.\n",
    "#it takes in paramater batch_number and returns the same batch\n",
    "\n",
    "def X_batch(batch_number) :\n",
    "    if batch_number == 1 :\n",
    "        return X_train_1\n",
    "    if batch_number == 2 :\n",
    "        return X_train_2\n",
    "    if batch_number == 3 :\n",
    "        return X_train_3\n",
    "    if batch_number == 4 :\n",
    "        return X_train_4\n",
    "    if batch_number == 5 :\n",
    "        return X_train_5\n",
    "\n",
    "    \n",
    "#same as the previous funtion except that it returns the labels for the data batches.\n",
    "    \n",
    "def Y_batch(batch_number) :\n",
    "    if batch_number == 1 :\n",
    "        return Y_train_1\n",
    "    if batch_number == 2 :\n",
    "        return Y_train_2\n",
    "    if batch_number == 3 :\n",
    "        return Y_train_3\n",
    "    if batch_number == 4 :\n",
    "        return Y_train_4\n",
    "    if batch_number == 5 :\n",
    "        return Y_train_5    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the perfect results, we need confusion metrics but the problem with confusion metrics in sklearn is that it takes in input two 1D arrays but our labels for CNN are of shape (n,15) since there are 15 classes. To deal with this problem, we should matrix multiply the labels of shape (n,15) to a matrix to return us a 1D array. The code for same is in the session of tensorflow. The matrix we should multiply the labels with is declared below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "multiplier_matrix = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN that I will be using will have 2 convolutional layers and a fully connected layer that shall further join to the output layer. I have used maxpooling for the CNN and used a dropout of 80% to prevent overfitting. All the required weights, biases and different functions of the CNN are declared in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 --batch : 1 --- Loss : 114282.5781\n",
      "Epoch :  1 --batch : 2 --- Loss : 77334.7578\n",
      "Epoch :  1 --batch : 3 --- Loss : 43883.4844\n",
      "Epoch :  1 --batch : 4 --- Loss : 41293.7539\n",
      "Epoch :  1 --batch : 5 --- Loss : 18561.9746\n",
      "Testing Accuracy after epoch 0 : 0.3370000123977661\n",
      "Predicted   0    1   3  4    5    6   7   8  9   11  12   13   All\n",
      "Actual                                                            \n",
      "0          44    0   0  0    0   26   0   0  0    0   0    0    70\n",
      "1           0   67   0  0    0    0   0   0  0    0   0    1    68\n",
      "2           2    3   0  0   16    0   1   0  0   14   0   27    63\n",
      "3           0    0   3  0   44   10   2   1  0    0   1    0    61\n",
      "4           1    9   0  1    0    0   0   0  0    0   0   50    61\n",
      "5           0    0   0  0   49    8   0   0  0    0   0    0    57\n",
      "6           0    0   0  0    5   57   0   0  0    0   0    0    62\n",
      "7           2    1   0  0    9   24   9   0  0   12   0    6    63\n",
      "8           0   11   0  0    4    8   2   1  0    3   0   30    59\n",
      "9           2    9   0  0    1    3   0   0  2    0   0   62    79\n",
      "10          0    0   0  0    0    0   0   0  0    6   0   64    70\n",
      "11          1    1   7  0    9    6   0   0  0   41   1    3    69\n",
      "12          3    0   2  0   10   16   1   1  0   16  31    2    82\n",
      "13          1   30   0  0    0    0   0   0  0    1   0   32    64\n",
      "14         27    0   0  0    0   11   0   8  0   16   2    8    72\n",
      "All        83  131  12  1  147  169  15  11  2  109  35  285  1000\n",
      "Epoch :  2 --batch : 1 --- Loss : 12234.7090\n",
      "Epoch :  2 --batch : 2 --- Loss :  9691.1211\n",
      "Epoch :  2 --batch : 3 --- Loss :  5085.0742\n",
      "Epoch :  2 --batch : 4 --- Loss :  2137.9446\n",
      "Epoch :  2 --batch : 5 --- Loss :  2115.7751\n",
      "Testing Accuracy after epoch 1 : 0.824999988079071\n",
      "Epoch :  3 --batch : 1 --- Loss :  1595.1794\n",
      "Epoch :  3 --batch : 2 --- Loss :  1685.7540\n",
      "Epoch :  3 --batch : 3 --- Loss :  1426.9285\n",
      "Epoch :  3 --batch : 4 --- Loss :  1174.6125\n",
      "Epoch :  3 --batch : 5 --- Loss :  1113.3384\n",
      "Testing Accuracy after epoch 2 : 0.8859999775886536\n",
      "Epoch :  4 --batch : 1 --- Loss :  1167.1857\n",
      "Epoch :  4 --batch : 2 --- Loss :   936.0040\n",
      "Epoch :  4 --batch : 3 --- Loss :  1242.7012\n",
      "Epoch :  4 --batch : 4 --- Loss :   742.8452\n",
      "Epoch :  4 --batch : 5 --- Loss :   859.7283\n",
      "Testing Accuracy after epoch 3 : 0.9089999794960022\n",
      "Epoch :  5 --batch : 1 --- Loss :   725.1962\n",
      "Epoch :  5 --batch : 2 --- Loss :   631.6275\n",
      "Epoch :  5 --batch : 3 --- Loss :   741.3117\n",
      "Epoch :  5 --batch : 4 --- Loss :   467.4880\n",
      "Epoch :  5 --batch : 5 --- Loss :   578.6722\n",
      "Testing Accuracy after epoch 4 : 0.9390000104904175\n",
      "Epoch :  6 --batch : 1 --- Loss :   515.2753\n",
      "Epoch :  6 --batch : 2 --- Loss :   462.7148\n",
      "Epoch :  6 --batch : 3 --- Loss :   518.5179\n",
      "Epoch :  6 --batch : 4 --- Loss :   366.0996\n",
      "Epoch :  6 --batch : 5 --- Loss :   450.7512\n",
      "Testing Accuracy after epoch 5 : 0.9490000009536743\n",
      "Epoch :  7 --batch : 1 --- Loss :   405.3545\n",
      "Epoch :  7 --batch : 2 --- Loss :   375.0736\n",
      "Epoch :  7 --batch : 3 --- Loss :   454.9827\n",
      "Epoch :  7 --batch : 4 --- Loss :   282.5343\n",
      "Epoch :  7 --batch : 5 --- Loss :   344.2074\n",
      "Testing Accuracy after epoch 6 : 0.953000009059906\n",
      "Epoch :  8 --batch : 1 --- Loss :   336.8718\n",
      "Epoch :  8 --batch : 2 --- Loss :   333.8748\n",
      "Epoch :  8 --batch : 3 --- Loss :   369.0998\n",
      "Epoch :  8 --batch : 4 --- Loss :   233.6753\n",
      "Epoch :  8 --batch : 5 --- Loss :   289.9829\n",
      "Testing Accuracy after epoch 7 : 0.9589999914169312\n",
      "Epoch :  9 --batch : 1 --- Loss :   277.9425\n",
      "Epoch :  9 --batch : 2 --- Loss :   265.2087\n",
      "Epoch :  9 --batch : 3 --- Loss :   304.0631\n",
      "Epoch :  9 --batch : 4 --- Loss :   207.8437\n",
      "Epoch :  9 --batch : 5 --- Loss :   265.4728\n",
      "Testing Accuracy after epoch 8 : 0.9629999995231628\n",
      "Epoch : 10 --batch : 1 --- Loss :   241.6788\n",
      "Epoch : 10 --batch : 2 --- Loss :   213.3403\n",
      "Epoch : 10 --batch : 3 --- Loss :   253.8004\n",
      "Epoch : 10 --batch : 4 --- Loss :   179.5245\n",
      "Epoch : 10 --batch : 5 --- Loss :   218.5742\n",
      "Testing Accuracy after epoch 9 : 0.9649999737739563\n",
      "Epoch : 11 --batch : 1 --- Loss :   210.8211\n",
      "Epoch : 11 --batch : 2 --- Loss :   186.4371\n",
      "Epoch : 11 --batch : 3 --- Loss :   231.2499\n",
      "Epoch : 11 --batch : 4 --- Loss :   157.5471\n",
      "Epoch : 11 --batch : 5 --- Loss :   204.5352\n",
      "Testing Accuracy after epoch 10 : 0.9700000286102295\n",
      "Epoch : 12 --batch : 1 --- Loss :   180.0066\n",
      "Epoch : 12 --batch : 2 --- Loss :   166.4979\n",
      "Epoch : 12 --batch : 3 --- Loss :   211.3255\n",
      "Epoch : 12 --batch : 4 --- Loss :   140.6353\n",
      "Epoch : 12 --batch : 5 --- Loss :   181.9672\n",
      "Testing Accuracy after epoch 11 : 0.9710000157356262\n",
      "Epoch : 13 --batch : 1 --- Loss :   161.6580\n",
      "Epoch : 13 --batch : 2 --- Loss :   148.3171\n",
      "Epoch : 13 --batch : 3 --- Loss :   196.1947\n",
      "Epoch : 13 --batch : 4 --- Loss :   122.2519\n",
      "Epoch : 13 --batch : 5 --- Loss :   148.0984\n",
      "Testing Accuracy after epoch 12 : 0.9729999899864197\n",
      "Epoch : 14 --batch : 1 --- Loss :   146.6469\n",
      "Epoch : 14 --batch : 2 --- Loss :   130.9598\n",
      "Epoch : 14 --batch : 3 --- Loss :   180.3102\n",
      "Epoch : 14 --batch : 4 --- Loss :   111.5159\n",
      "Epoch : 14 --batch : 5 --- Loss :   128.9115\n",
      "Testing Accuracy after epoch 13 : 0.972000002861023\n",
      "Epoch : 15 --batch : 1 --- Loss :   133.7298\n",
      "Epoch : 15 --batch : 2 --- Loss :   120.7839\n",
      "Epoch : 15 --batch : 3 --- Loss :   166.8804\n",
      "Epoch : 15 --batch : 4 --- Loss :   104.5155\n",
      "Epoch : 15 --batch : 5 --- Loss :   117.1179\n",
      "Testing Accuracy after epoch 14 : 0.9729999899864197\n",
      "Epoch : 16 --batch : 1 --- Loss :   113.6602\n",
      "Epoch : 16 --batch : 2 --- Loss :   103.2562\n",
      "Epoch : 16 --batch : 3 --- Loss :   154.3327\n",
      "Epoch : 16 --batch : 4 --- Loss :    99.6449\n",
      "Epoch : 16 --batch : 5 --- Loss :   106.1639\n",
      "Testing Accuracy after epoch 15 : 0.9729999899864197\n",
      "Epoch : 17 --batch : 1 --- Loss :   113.5066\n",
      "Epoch : 17 --batch : 2 --- Loss :    92.2780\n",
      "Epoch : 17 --batch : 3 --- Loss :   143.0921\n",
      "Epoch : 17 --batch : 4 --- Loss :    85.8768\n",
      "Epoch : 17 --batch : 5 --- Loss :    95.5364\n",
      "Testing Accuracy after epoch 16 : 0.9750000238418579\n",
      "Epoch : 18 --batch : 1 --- Loss :    90.5999\n",
      "Epoch : 18 --batch : 2 --- Loss :    81.2808\n",
      "Epoch : 18 --batch : 3 --- Loss :   130.8034\n",
      "Epoch : 18 --batch : 4 --- Loss :    75.7265\n",
      "Epoch : 18 --batch : 5 --- Loss :    90.4874\n",
      "Testing Accuracy after epoch 17 : 0.9779999852180481\n",
      "Epoch : 19 --batch : 1 --- Loss :    93.6001\n",
      "Epoch : 19 --batch : 2 --- Loss :    86.2326\n",
      "Epoch : 19 --batch : 3 --- Loss :   120.6287\n",
      "Epoch : 19 --batch : 4 --- Loss :    71.8186\n",
      "Epoch : 19 --batch : 5 --- Loss :    83.2246\n",
      "Testing Accuracy after epoch 18 : 0.9789999723434448\n",
      "Epoch : 20 --batch : 1 --- Loss :    83.8798\n",
      "Epoch : 20 --batch : 2 --- Loss :    68.0970\n",
      "Epoch : 20 --batch : 3 --- Loss :   111.3609\n",
      "Epoch : 20 --batch : 4 --- Loss :    69.1125\n",
      "Epoch : 20 --batch : 5 --- Loss :    67.2567\n",
      "Testing Accuracy after epoch 19 : 0.9819999933242798\n",
      "Epoch : 21 --batch : 1 --- Loss :    75.1903\n",
      "Epoch : 21 --batch : 2 --- Loss :    62.3515\n",
      "Epoch : 21 --batch : 3 --- Loss :   105.6334\n",
      "Epoch : 21 --batch : 4 --- Loss :    66.1842\n",
      "Epoch : 21 --batch : 5 --- Loss :    66.0690\n",
      "Testing Accuracy after epoch 20 : 0.984000027179718\n",
      "Epoch : 22 --batch : 1 --- Loss :    69.9830\n",
      "Epoch : 22 --batch : 2 --- Loss :    62.8178\n",
      "Epoch : 22 --batch : 3 --- Loss :    99.2186\n",
      "Epoch : 22 --batch : 4 --- Loss :    57.9270\n",
      "Epoch : 22 --batch : 5 --- Loss :    63.2323\n",
      "Testing Accuracy after epoch 21 : 0.984000027179718\n",
      "Epoch : 23 --batch : 1 --- Loss :    64.0846\n",
      "Epoch : 23 --batch : 2 --- Loss :    51.1013\n",
      "Epoch : 23 --batch : 3 --- Loss :    84.7486\n",
      "Epoch : 23 --batch : 4 --- Loss :    51.1178\n",
      "Epoch : 23 --batch : 5 --- Loss :    61.7894\n",
      "Testing Accuracy after epoch 22 : 0.984000027179718\n",
      "Epoch : 24 --batch : 1 --- Loss :    57.3989\n",
      "Epoch : 24 --batch : 2 --- Loss :    46.5743\n",
      "Epoch : 24 --batch : 3 --- Loss :    85.3842\n",
      "Epoch : 24 --batch : 4 --- Loss :    51.4926\n",
      "Epoch : 24 --batch : 5 --- Loss :    53.5418\n",
      "Testing Accuracy after epoch 23 : 0.9879999756813049\n",
      "Epoch : 25 --batch : 1 --- Loss :    55.4748\n",
      "Epoch : 25 --batch : 2 --- Loss :    41.7456\n",
      "Epoch : 25 --batch : 3 --- Loss :    81.5205\n",
      "Epoch : 25 --batch : 4 --- Loss :    48.2455\n",
      "Epoch : 25 --batch : 5 --- Loss :    51.4895\n",
      "Testing Accuracy after epoch 24 : 0.9900000095367432\n",
      "Epoch : 26 --batch : 1 --- Loss :    50.7641\n",
      "Epoch : 26 --batch : 2 --- Loss :    38.1010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 26 --batch : 3 --- Loss :    69.4712\n",
      "Epoch : 26 --batch : 4 --- Loss :    42.2192\n",
      "Epoch : 26 --batch : 5 --- Loss :    48.2713\n",
      "Testing Accuracy after epoch 25 : 0.9879999756813049\n",
      "Epoch : 27 --batch : 1 --- Loss :    46.4539\n",
      "Epoch : 27 --batch : 2 --- Loss :    33.2424\n",
      "Epoch : 27 --batch : 3 --- Loss :    73.9766\n",
      "Epoch : 27 --batch : 4 --- Loss :    41.4011\n",
      "Epoch : 27 --batch : 5 --- Loss :    46.4777\n",
      "Testing Accuracy after epoch 26 : 0.9890000224113464\n",
      "Epoch : 28 --batch : 1 --- Loss :    44.6374\n",
      "Epoch : 28 --batch : 2 --- Loss :    28.7090\n",
      "Epoch : 28 --batch : 3 --- Loss :    76.2431\n",
      "Epoch : 28 --batch : 4 --- Loss :    40.9669\n",
      "Epoch : 28 --batch : 5 --- Loss :    46.1214\n",
      "Testing Accuracy after epoch 27 : 0.9900000095367432\n",
      "Epoch : 29 --batch : 1 --- Loss :    39.5668\n",
      "Epoch : 29 --batch : 2 --- Loss :    30.9898\n",
      "Epoch : 29 --batch : 3 --- Loss :    67.3691\n",
      "Epoch : 29 --batch : 4 --- Loss :    38.8827\n",
      "Epoch : 29 --batch : 5 --- Loss :    43.6868\n",
      "Testing Accuracy after epoch 28 : 0.9909999966621399\n",
      "Epoch : 30 --batch : 1 --- Loss :    39.6384\n",
      "Epoch : 30 --batch : 2 --- Loss :    29.2678\n",
      "Epoch : 30 --batch : 3 --- Loss :    66.0121\n",
      "Epoch : 30 --batch : 4 --- Loss :    39.1682\n",
      "Epoch : 30 --batch : 5 --- Loss :    42.3936\n",
      "Testing Accuracy after epoch 29 : 0.9900000095367432\n",
      "Epoch : 31 --batch : 1 --- Loss :    39.9536\n",
      "Epoch : 31 --batch : 2 --- Loss :    27.2095\n",
      "Epoch : 31 --batch : 3 --- Loss :    61.4028\n",
      "Epoch : 31 --batch : 4 --- Loss :    36.9780\n",
      "Epoch : 31 --batch : 5 --- Loss :    41.3415\n",
      "Testing Accuracy after epoch 30 : 0.9909999966621399\n",
      "Epoch : 32 --batch : 1 --- Loss :    38.3132\n",
      "Epoch : 32 --batch : 2 --- Loss :    27.0966\n",
      "Epoch : 32 --batch : 3 --- Loss :    59.9762\n",
      "Epoch : 32 --batch : 4 --- Loss :    35.2350\n",
      "Epoch : 32 --batch : 5 --- Loss :    37.8018\n",
      "Testing Accuracy after epoch 31 : 0.9909999966621399\n",
      "Epoch : 33 --batch : 1 --- Loss :    36.3516\n",
      "Epoch : 33 --batch : 2 --- Loss :    23.2279\n",
      "Epoch : 33 --batch : 3 --- Loss :    63.8028\n",
      "Epoch : 33 --batch : 4 --- Loss :    32.4110\n",
      "Epoch : 33 --batch : 5 --- Loss :    38.3526\n",
      "Testing Accuracy after epoch 32 : 0.9909999966621399\n",
      "Epoch : 34 --batch : 1 --- Loss :    34.9238\n",
      "Epoch : 34 --batch : 2 --- Loss :    19.8437\n",
      "Epoch : 34 --batch : 3 --- Loss :    53.7869\n",
      "Epoch : 34 --batch : 4 --- Loss :    31.5041\n",
      "Epoch : 34 --batch : 5 --- Loss :    34.8455\n",
      "Testing Accuracy after epoch 33 : 0.9919999837875366\n",
      "Epoch : 35 --batch : 1 --- Loss :    32.3123\n",
      "Epoch : 35 --batch : 2 --- Loss :    18.3877\n",
      "Epoch : 35 --batch : 3 --- Loss :    52.5685\n",
      "Epoch : 35 --batch : 4 --- Loss :    29.9516\n",
      "Epoch : 35 --batch : 5 --- Loss :    31.9861\n",
      "Testing Accuracy after epoch 34 : 0.9919999837875366\n",
      "Epoch : 36 --batch : 1 --- Loss :    31.8668\n",
      "Epoch : 36 --batch : 2 --- Loss :    17.5915\n",
      "Epoch : 36 --batch : 3 --- Loss :    48.1572\n",
      "Epoch : 36 --batch : 4 --- Loss :    28.1743\n",
      "Epoch : 36 --batch : 5 --- Loss :    31.4243\n",
      "Testing Accuracy after epoch 35 : 0.9909999966621399\n",
      "Epoch : 37 --batch : 1 --- Loss :    28.8335\n",
      "Epoch : 37 --batch : 2 --- Loss :    17.1382\n",
      "Epoch : 37 --batch : 3 --- Loss :    47.2637\n",
      "Epoch : 37 --batch : 4 --- Loss :    25.5028\n",
      "Epoch : 37 --batch : 5 --- Loss :    29.2241\n",
      "Testing Accuracy after epoch 36 : 0.9909999966621399\n",
      "Epoch : 38 --batch : 1 --- Loss :    27.6454\n",
      "Epoch : 38 --batch : 2 --- Loss :    18.3766\n",
      "Epoch : 38 --batch : 3 --- Loss :    42.9343\n",
      "Epoch : 38 --batch : 4 --- Loss :    25.3294\n",
      "Epoch : 38 --batch : 5 --- Loss :    29.6672\n",
      "Testing Accuracy after epoch 37 : 0.9919999837875366\n",
      "Epoch : 39 --batch : 1 --- Loss :    26.7591\n",
      "Epoch : 39 --batch : 2 --- Loss :    15.1782\n",
      "Epoch : 39 --batch : 3 --- Loss :    43.5726\n",
      "Epoch : 39 --batch : 4 --- Loss :    24.7148\n",
      "Epoch : 39 --batch : 5 --- Loss :    29.1755\n",
      "Testing Accuracy after epoch 38 : 0.9919999837875366\n",
      "Epoch : 40 --batch : 1 --- Loss :    24.4651\n",
      "Epoch : 40 --batch : 2 --- Loss :    14.2212\n",
      "Epoch : 40 --batch : 3 --- Loss :    40.5320\n",
      "Epoch : 40 --batch : 4 --- Loss :    24.1146\n",
      "Epoch : 40 --batch : 5 --- Loss :    28.4499\n",
      "Testing Accuracy after epoch 39 : 0.9919999837875366\n",
      "Epoch : 41 --batch : 1 --- Loss :    22.6070\n",
      "Epoch : 41 --batch : 2 --- Loss :    12.2103\n",
      "Epoch : 41 --batch : 3 --- Loss :    38.1523\n",
      "Epoch : 41 --batch : 4 --- Loss :    23.0814\n",
      "Epoch : 41 --batch : 5 --- Loss :    27.4047\n",
      "Testing Accuracy after epoch 40 : 0.9929999709129333\n",
      "Epoch : 42 --batch : 1 --- Loss :    24.0718\n",
      "Epoch : 42 --batch : 2 --- Loss :    13.6259\n",
      "Epoch : 42 --batch : 3 --- Loss :    37.9687\n",
      "Epoch : 42 --batch : 4 --- Loss :    22.4426\n",
      "Epoch : 42 --batch : 5 --- Loss :    26.8070\n",
      "Testing Accuracy after epoch 41 : 0.9940000176429749\n",
      "Epoch : 43 --batch : 1 --- Loss :    22.6730\n",
      "Epoch : 43 --batch : 2 --- Loss :    12.7233\n",
      "Epoch : 43 --batch : 3 --- Loss :    35.0879\n",
      "Epoch : 43 --batch : 4 --- Loss :    20.4611\n",
      "Epoch : 43 --batch : 5 --- Loss :    27.2246\n",
      "Testing Accuracy after epoch 42 : 0.9940000176429749\n",
      "Epoch : 44 --batch : 1 --- Loss :    19.0647\n",
      "Epoch : 44 --batch : 2 --- Loss :    12.7620\n",
      "Epoch : 44 --batch : 3 --- Loss :    32.3804\n",
      "Epoch : 44 --batch : 4 --- Loss :    19.4684\n",
      "Epoch : 44 --batch : 5 --- Loss :    24.0754\n",
      "Testing Accuracy after epoch 43 : 0.9929999709129333\n",
      "Epoch : 45 --batch : 1 --- Loss :    18.8236\n",
      "Epoch : 45 --batch : 2 --- Loss :    10.9570\n",
      "Epoch : 45 --batch : 3 --- Loss :    32.2901\n",
      "Epoch : 45 --batch : 4 --- Loss :    19.5726\n",
      "Epoch : 45 --batch : 5 --- Loss :    22.8332\n",
      "Testing Accuracy after epoch 44 : 0.9940000176429749\n",
      "Epoch : 46 --batch : 1 --- Loss :    18.8251\n",
      "Epoch : 46 --batch : 2 --- Loss :     9.4528\n",
      "Epoch : 46 --batch : 3 --- Loss :    31.6378\n",
      "Epoch : 46 --batch : 4 --- Loss :    19.1434\n",
      "Epoch : 46 --batch : 5 --- Loss :    22.6309\n",
      "Testing Accuracy after epoch 45 : 0.9940000176429749\n",
      "Epoch : 47 --batch : 1 --- Loss :    18.5370\n",
      "Epoch : 47 --batch : 2 --- Loss :     8.8765\n",
      "Epoch : 47 --batch : 3 --- Loss :    29.3766\n",
      "Epoch : 47 --batch : 4 --- Loss :    19.5485\n",
      "Epoch : 47 --batch : 5 --- Loss :    22.4943\n",
      "Testing Accuracy after epoch 46 : 0.9940000176429749\n",
      "Epoch : 48 --batch : 1 --- Loss :    18.3284\n",
      "Epoch : 48 --batch : 2 --- Loss :     7.5907\n",
      "Epoch : 48 --batch : 3 --- Loss :    27.0490\n",
      "Epoch : 48 --batch : 4 --- Loss :    19.7661\n",
      "Epoch : 48 --batch : 5 --- Loss :    20.5591\n",
      "Testing Accuracy after epoch 47 : 0.9940000176429749\n",
      "Epoch : 49 --batch : 1 --- Loss :    18.7484\n",
      "Epoch : 49 --batch : 2 --- Loss :     8.2491\n",
      "Epoch : 49 --batch : 3 --- Loss :    26.1014\n",
      "Epoch : 49 --batch : 4 --- Loss :    18.6263\n",
      "Epoch : 49 --batch : 5 --- Loss :    23.8373\n",
      "Testing Accuracy after epoch 48 : 0.9959999918937683\n",
      "Epoch : 50 --batch : 1 --- Loss :    19.0766\n",
      "Epoch : 50 --batch : 2 --- Loss :     8.8750\n",
      "Epoch : 50 --batch : 3 --- Loss :    27.9848\n",
      "Epoch : 50 --batch : 4 --- Loss :    18.3614\n",
      "Epoch : 50 --batch : 5 --- Loss :    22.4985\n",
      "Testing Accuracy after epoch 49 : 0.9959999918937683\n",
      "Epoch : 51 --batch : 1 --- Loss :    17.0814\n",
      "Epoch : 51 --batch : 2 --- Loss :     7.3563\n",
      "Epoch : 51 --batch : 3 --- Loss :    25.2218\n",
      "Epoch : 51 --batch : 4 --- Loss :    16.2665\n",
      "Epoch : 51 --batch : 5 --- Loss :    21.3437\n",
      "Testing Accuracy after epoch 50 : 0.9950000047683716\n",
      "Epoch : 52 --batch : 1 --- Loss :    15.7929\n",
      "Epoch : 52 --batch : 2 --- Loss :     8.0624\n",
      "Epoch : 52 --batch : 3 --- Loss :    23.3712\n",
      "Epoch : 52 --batch : 4 --- Loss :    14.6435\n",
      "Epoch : 52 --batch : 5 --- Loss :    18.9573\n",
      "Testing Accuracy after epoch 51 : 0.9950000047683716\n",
      "Epoch : 53 --batch : 1 --- Loss :    16.1514\n",
      "Epoch : 53 --batch : 2 --- Loss :     6.7822\n",
      "Epoch : 53 --batch : 3 --- Loss :    22.2481\n",
      "Epoch : 53 --batch : 4 --- Loss :    15.1854\n",
      "Epoch : 53 --batch : 5 --- Loss :    20.5194\n",
      "Testing Accuracy after epoch 52 : 0.9959999918937683\n",
      "Epoch : 54 --batch : 1 --- Loss :    16.0882\n",
      "Epoch : 54 --batch : 2 --- Loss :     6.8449\n",
      "Epoch : 54 --batch : 3 --- Loss :    19.1610\n",
      "Epoch : 54 --batch : 4 --- Loss :    13.8744\n",
      "Epoch : 54 --batch : 5 --- Loss :    19.4159\n",
      "Testing Accuracy after epoch 53 : 0.9959999918937683\n",
      "Epoch : 55 --batch : 1 --- Loss :    16.2652\n",
      "Epoch : 55 --batch : 2 --- Loss :     6.5122\n",
      "Epoch : 55 --batch : 3 --- Loss :    19.6903\n",
      "Epoch : 55 --batch : 4 --- Loss :    14.5641\n",
      "Epoch : 55 --batch : 5 --- Loss :    17.5238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy after epoch 54 : 0.9959999918937683\n",
      "Epoch : 56 --batch : 1 --- Loss :    16.0668\n",
      "Epoch : 56 --batch : 2 --- Loss :     5.3269\n",
      "Epoch : 56 --batch : 3 --- Loss :    16.2239\n",
      "Epoch : 56 --batch : 4 --- Loss :    13.8942\n",
      "Epoch : 56 --batch : 5 --- Loss :    16.3904\n",
      "Testing Accuracy after epoch 55 : 0.9950000047683716\n",
      "Epoch : 57 --batch : 1 --- Loss :    15.2442\n",
      "Epoch : 57 --batch : 2 --- Loss :     4.1328\n",
      "Epoch : 57 --batch : 3 --- Loss :    13.8452\n",
      "Epoch : 57 --batch : 4 --- Loss :    12.6987\n",
      "Epoch : 57 --batch : 5 --- Loss :    16.1712\n",
      "Testing Accuracy after epoch 56 : 0.9959999918937683\n",
      "Epoch : 58 --batch : 1 --- Loss :    14.6752\n",
      "Epoch : 58 --batch : 2 --- Loss :     4.5340\n",
      "Epoch : 58 --batch : 3 --- Loss :    13.8993\n",
      "Epoch : 58 --batch : 4 --- Loss :    12.2961\n",
      "Epoch : 58 --batch : 5 --- Loss :    15.4654\n",
      "Testing Accuracy after epoch 57 : 0.9959999918937683\n",
      "Epoch : 59 --batch : 1 --- Loss :    12.6953\n",
      "Epoch : 59 --batch : 2 --- Loss :     5.2429\n",
      "Epoch : 59 --batch : 3 --- Loss :    14.1809\n",
      "Epoch : 59 --batch : 4 --- Loss :    10.0644\n",
      "Epoch : 59 --batch : 5 --- Loss :    14.8174\n",
      "Testing Accuracy after epoch 58 : 0.9959999918937683\n",
      "Epoch : 60 --batch : 1 --- Loss :    11.7525\n",
      "Epoch : 60 --batch : 2 --- Loss :     4.3865\n",
      "Epoch : 60 --batch : 3 --- Loss :    14.5288\n",
      "Epoch : 60 --batch : 4 --- Loss :     8.5675\n",
      "Epoch : 60 --batch : 5 --- Loss :    15.7385\n",
      "Testing Accuracy after epoch 59 : 0.9959999918937683\n",
      "Epoch : 61 --batch : 1 --- Loss :    11.2659\n",
      "Epoch : 61 --batch : 2 --- Loss :     3.3115\n",
      "Epoch : 61 --batch : 3 --- Loss :    12.3958\n",
      "Epoch : 61 --batch : 4 --- Loss :     8.4755\n",
      "Epoch : 61 --batch : 5 --- Loss :    15.0360\n",
      "Testing Accuracy after epoch 60 : 0.9959999918937683\n",
      "Epoch : 62 --batch : 1 --- Loss :    11.7729\n",
      "Epoch : 62 --batch : 2 --- Loss :     2.9596\n",
      "Epoch : 62 --batch : 3 --- Loss :    11.5750\n",
      "Epoch : 62 --batch : 4 --- Loss :     9.4532\n",
      "Epoch : 62 --batch : 5 --- Loss :    14.2853\n",
      "Testing Accuracy after epoch 61 : 0.9950000047683716\n",
      "Epoch : 63 --batch : 1 --- Loss :    10.9794\n",
      "Epoch : 63 --batch : 2 --- Loss :     4.3265\n",
      "Epoch : 63 --batch : 3 --- Loss :    12.0488\n",
      "Epoch : 63 --batch : 4 --- Loss :    10.2910\n",
      "Epoch : 63 --batch : 5 --- Loss :    13.1546\n",
      "Testing Accuracy after epoch 62 : 0.9950000047683716\n",
      "Epoch : 64 --batch : 1 --- Loss :    10.5454\n",
      "Epoch : 64 --batch : 2 --- Loss :     3.9666\n",
      "Epoch : 64 --batch : 3 --- Loss :    11.7640\n",
      "Epoch : 64 --batch : 4 --- Loss :     9.0118\n",
      "Epoch : 64 --batch : 5 --- Loss :    12.8140\n",
      "Testing Accuracy after epoch 63 : 0.9950000047683716\n",
      "Epoch : 65 --batch : 1 --- Loss :    10.2429\n",
      "Epoch : 65 --batch : 2 --- Loss :     3.9402\n",
      "Epoch : 65 --batch : 3 --- Loss :     9.5185\n",
      "Epoch : 65 --batch : 4 --- Loss :     8.1852\n",
      "Epoch : 65 --batch : 5 --- Loss :    12.6922\n",
      "Testing Accuracy after epoch 64 : 0.9950000047683716\n",
      "Epoch : 66 --batch : 1 --- Loss :     8.9260\n",
      "Epoch : 66 --batch : 2 --- Loss :     3.1686\n",
      "Epoch : 66 --batch : 3 --- Loss :     9.5062\n",
      "Epoch : 66 --batch : 4 --- Loss :     9.2327\n",
      "Epoch : 66 --batch : 5 --- Loss :    11.9697\n",
      "Testing Accuracy after epoch 65 : 0.9959999918937683\n",
      "Epoch : 67 --batch : 1 --- Loss :     8.7145\n",
      "Epoch : 67 --batch : 2 --- Loss :     3.3440\n",
      "Epoch : 67 --batch : 3 --- Loss :     8.8564\n",
      "Epoch : 67 --batch : 4 --- Loss :     9.1597\n",
      "Epoch : 67 --batch : 5 --- Loss :    12.1549\n",
      "Testing Accuracy after epoch 66 : 0.9959999918937683\n",
      "Epoch : 68 --batch : 1 --- Loss :     8.7450\n",
      "Epoch : 68 --batch : 2 --- Loss :     2.1909\n",
      "Epoch : 68 --batch : 3 --- Loss :     7.5282\n",
      "Epoch : 68 --batch : 4 --- Loss :     8.1210\n",
      "Epoch : 68 --batch : 5 --- Loss :    11.6279\n",
      "Testing Accuracy after epoch 67 : 0.9959999918937683\n",
      "Epoch : 69 --batch : 1 --- Loss :     8.2984\n",
      "Epoch : 69 --batch : 2 --- Loss :     3.1783\n",
      "Epoch : 69 --batch : 3 --- Loss :    10.7475\n",
      "Epoch : 69 --batch : 4 --- Loss :     7.7721\n",
      "Epoch : 69 --batch : 5 --- Loss :    11.5058\n",
      "Testing Accuracy after epoch 68 : 0.9959999918937683\n",
      "Epoch : 70 --batch : 1 --- Loss :     7.7611\n",
      "Epoch : 70 --batch : 2 --- Loss :     3.3018\n",
      "Epoch : 70 --batch : 3 --- Loss :     9.0899\n",
      "Epoch : 70 --batch : 4 --- Loss :     8.0579\n",
      "Epoch : 70 --batch : 5 --- Loss :    11.9022\n",
      "Testing Accuracy after epoch 69 : 0.9959999918937683\n",
      "Epoch : 71 --batch : 1 --- Loss :     7.7565\n",
      "Epoch : 71 --batch : 2 --- Loss :     3.1183\n",
      "Epoch : 71 --batch : 3 --- Loss :     7.8330\n",
      "Epoch : 71 --batch : 4 --- Loss :     7.5273\n",
      "Epoch : 71 --batch : 5 --- Loss :    10.9445\n",
      "Testing Accuracy after epoch 70 : 0.9959999918937683\n",
      "Epoch : 72 --batch : 1 --- Loss :     8.4958\n",
      "Epoch : 72 --batch : 2 --- Loss :     2.2438\n",
      "Epoch : 72 --batch : 3 --- Loss :     5.7197\n",
      "Epoch : 72 --batch : 4 --- Loss :     7.6688\n",
      "Epoch : 72 --batch : 5 --- Loss :    10.0617\n",
      "Testing Accuracy after epoch 71 : 0.9950000047683716\n",
      "Epoch : 73 --batch : 1 --- Loss :     8.0551\n",
      "Epoch : 73 --batch : 2 --- Loss :     2.2351\n",
      "Epoch : 73 --batch : 3 --- Loss :     8.3857\n",
      "Epoch : 73 --batch : 4 --- Loss :     7.7545\n",
      "Epoch : 73 --batch : 5 --- Loss :    10.1974\n",
      "Testing Accuracy after epoch 72 : 0.9959999918937683\n",
      "Epoch : 74 --batch : 1 --- Loss :     7.7724\n",
      "Epoch : 74 --batch : 2 --- Loss :     2.9369\n",
      "Epoch : 74 --batch : 3 --- Loss :     6.2403\n",
      "Epoch : 74 --batch : 4 --- Loss :     8.2254\n",
      "Epoch : 74 --batch : 5 --- Loss :     9.0302\n",
      "Testing Accuracy after epoch 73 : 0.9959999918937683\n",
      "Epoch : 75 --batch : 1 --- Loss :     8.3309\n",
      "Epoch : 75 --batch : 2 --- Loss :     3.4777\n",
      "Epoch : 75 --batch : 3 --- Loss :     7.2722\n",
      "Epoch : 75 --batch : 4 --- Loss :     8.2033\n",
      "Epoch : 75 --batch : 5 --- Loss :     8.7012\n",
      "Testing Accuracy after epoch 74 : 0.9959999918937683\n",
      "Epoch : 76 --batch : 1 --- Loss :     7.9346\n",
      "Epoch : 76 --batch : 2 --- Loss :     2.4562\n",
      "Epoch : 76 --batch : 3 --- Loss :     5.4414\n",
      "Epoch : 76 --batch : 4 --- Loss :     7.5902\n",
      "Epoch : 76 --batch : 5 --- Loss :     8.9979\n",
      "Testing Accuracy after epoch 75 : 0.9959999918937683\n",
      "Epoch : 77 --batch : 1 --- Loss :     8.5287\n",
      "Epoch : 77 --batch : 2 --- Loss :     2.0773\n",
      "Epoch : 77 --batch : 3 --- Loss :     6.6715\n",
      "Epoch : 77 --batch : 4 --- Loss :     7.7339\n",
      "Epoch : 77 --batch : 5 --- Loss :     8.7212\n",
      "Testing Accuracy after epoch 76 : 0.9959999918937683\n",
      "Epoch : 78 --batch : 1 --- Loss :     7.4871\n",
      "Epoch : 78 --batch : 2 --- Loss :     2.7114\n",
      "Epoch : 78 --batch : 3 --- Loss :     4.8071\n",
      "Epoch : 78 --batch : 4 --- Loss :     7.4635\n",
      "Epoch : 78 --batch : 5 --- Loss :     8.7996\n",
      "Testing Accuracy after epoch 77 : 0.9959999918937683\n",
      "Epoch : 79 --batch : 1 --- Loss :     7.8267\n",
      "Epoch : 79 --batch : 2 --- Loss :     2.0791\n",
      "Epoch : 79 --batch : 3 --- Loss :     4.7320\n",
      "Epoch : 79 --batch : 4 --- Loss :     7.3934\n",
      "Epoch : 79 --batch : 5 --- Loss :     8.5808\n",
      "Testing Accuracy after epoch 78 : 0.9959999918937683\n",
      "Epoch : 80 --batch : 1 --- Loss :     8.0629\n",
      "Epoch : 80 --batch : 2 --- Loss :     1.1124\n",
      "Epoch : 80 --batch : 3 --- Loss :     4.3649\n",
      "Epoch : 80 --batch : 4 --- Loss :     6.2668\n",
      "Epoch : 80 --batch : 5 --- Loss :     7.6475\n",
      "Testing Accuracy after epoch 79 : 0.9959999918937683\n",
      "Epoch : 81 --batch : 1 --- Loss :     7.9805\n",
      "Epoch : 81 --batch : 2 --- Loss :     1.6312\n",
      "Epoch : 81 --batch : 3 --- Loss :     3.4297\n",
      "Epoch : 81 --batch : 4 --- Loss :     6.7244\n",
      "Epoch : 81 --batch : 5 --- Loss :     7.0199\n",
      "Testing Accuracy after epoch 80 : 0.9959999918937683\n",
      "Epoch : 82 --batch : 1 --- Loss :     6.8029\n",
      "Epoch : 82 --batch : 2 --- Loss :     1.6408\n",
      "Epoch : 82 --batch : 3 --- Loss :     4.1888\n",
      "Epoch : 82 --batch : 4 --- Loss :     7.4515\n",
      "Epoch : 82 --batch : 5 --- Loss :     7.4213\n",
      "Testing Accuracy after epoch 81 : 0.9959999918937683\n",
      "Epoch : 83 --batch : 1 --- Loss :     7.2923\n",
      "Epoch : 83 --batch : 2 --- Loss :     0.9502\n",
      "Epoch : 83 --batch : 3 --- Loss :     2.9182\n",
      "Epoch : 83 --batch : 4 --- Loss :     6.2593\n",
      "Epoch : 83 --batch : 5 --- Loss :     7.7626\n",
      "Testing Accuracy after epoch 82 : 0.9959999918937683\n",
      "Epoch : 84 --batch : 1 --- Loss :     6.5234\n",
      "Epoch : 84 --batch : 2 --- Loss :     0.4241\n",
      "Epoch : 84 --batch : 3 --- Loss :     3.4238\n",
      "Epoch : 84 --batch : 4 --- Loss :     5.3770\n",
      "Epoch : 84 --batch : 5 --- Loss :     7.7931\n",
      "Testing Accuracy after epoch 83 : 0.9959999918937683\n",
      "Epoch : 85 --batch : 1 --- Loss :     5.5355\n",
      "Epoch : 85 --batch : 2 --- Loss :     1.6302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 85 --batch : 3 --- Loss :     5.0813\n",
      "Epoch : 85 --batch : 4 --- Loss :     6.1763\n",
      "Epoch : 85 --batch : 5 --- Loss :     7.8791\n",
      "Testing Accuracy after epoch 84 : 0.9959999918937683\n",
      "Epoch : 86 --batch : 1 --- Loss :     4.9401\n",
      "Epoch : 86 --batch : 2 --- Loss :     1.4456\n",
      "Epoch : 86 --batch : 3 --- Loss :     4.4827\n",
      "Epoch : 86 --batch : 4 --- Loss :     5.7370\n",
      "Epoch : 86 --batch : 5 --- Loss :     8.1539\n",
      "Testing Accuracy after epoch 85 : 0.996999979019165\n",
      "Epoch : 87 --batch : 1 --- Loss :     6.0223\n",
      "Epoch : 87 --batch : 2 --- Loss :     0.3891\n",
      "Epoch : 87 --batch : 3 --- Loss :     2.9854\n",
      "Epoch : 87 --batch : 4 --- Loss :     4.9096\n",
      "Epoch : 87 --batch : 5 --- Loss :     7.6266\n",
      "Testing Accuracy after epoch 86 : 0.996999979019165\n",
      "Epoch : 88 --batch : 1 --- Loss :     4.8702\n",
      "Epoch : 88 --batch : 2 --- Loss :     1.2914\n",
      "Epoch : 88 --batch : 3 --- Loss :     3.3502\n",
      "Epoch : 88 --batch : 4 --- Loss :     5.4481\n",
      "Epoch : 88 --batch : 5 --- Loss :     6.6934\n",
      "Testing Accuracy after epoch 87 : 0.996999979019165\n",
      "Epoch : 89 --batch : 1 --- Loss :     5.3992\n",
      "Epoch : 89 --batch : 2 --- Loss :     0.4469\n",
      "Epoch : 89 --batch : 3 --- Loss :     3.3737\n",
      "Epoch : 89 --batch : 4 --- Loss :     5.0998\n",
      "Epoch : 89 --batch : 5 --- Loss :     5.6029\n",
      "Testing Accuracy after epoch 88 : 0.996999979019165\n",
      "Epoch : 90 --batch : 1 --- Loss :     4.5871\n",
      "Epoch : 90 --batch : 2 --- Loss :     0.9328\n",
      "Epoch : 90 --batch : 3 --- Loss :     4.1791\n",
      "Epoch : 90 --batch : 4 --- Loss :     4.8717\n",
      "Epoch : 90 --batch : 5 --- Loss :     6.1845\n",
      "Testing Accuracy after epoch 89 : 0.9959999918937683\n",
      "Epoch : 91 --batch : 1 --- Loss :     4.0092\n",
      "Epoch : 91 --batch : 2 --- Loss :     0.4942\n",
      "Epoch : 91 --batch : 3 --- Loss :     2.3598\n",
      "Epoch : 91 --batch : 4 --- Loss :     5.1735\n",
      "Epoch : 91 --batch : 5 --- Loss :     5.4731\n",
      "Testing Accuracy after epoch 90 : 0.9959999918937683\n",
      "Epoch : 92 --batch : 1 --- Loss :     3.4456\n",
      "Epoch : 92 --batch : 2 --- Loss :     1.0936\n",
      "Epoch : 92 --batch : 3 --- Loss :     1.0725\n",
      "Epoch : 92 --batch : 4 --- Loss :     4.6296\n",
      "Epoch : 92 --batch : 5 --- Loss :     5.9790\n",
      "Testing Accuracy after epoch 91 : 0.9959999918937683\n",
      "Epoch : 93 --batch : 1 --- Loss :     3.4143\n",
      "Epoch : 93 --batch : 2 --- Loss :     0.9620\n",
      "Epoch : 93 --batch : 3 --- Loss :     1.3696\n",
      "Epoch : 93 --batch : 4 --- Loss :     5.3157\n",
      "Epoch : 93 --batch : 5 --- Loss :     5.4398\n",
      "Testing Accuracy after epoch 92 : 0.996999979019165\n",
      "Epoch : 94 --batch : 1 --- Loss :     3.5155\n",
      "Epoch : 94 --batch : 2 --- Loss :     1.2847\n",
      "Epoch : 94 --batch : 3 --- Loss :     1.6236\n",
      "Epoch : 94 --batch : 4 --- Loss :     4.6803\n",
      "Epoch : 94 --batch : 5 --- Loss :     4.4078\n",
      "Testing Accuracy after epoch 93 : 0.996999979019165\n",
      "Epoch : 95 --batch : 1 --- Loss :     3.4708\n",
      "Epoch : 95 --batch : 2 --- Loss :     0.4547\n",
      "Epoch : 95 --batch : 3 --- Loss :     1.1435\n",
      "Epoch : 95 --batch : 4 --- Loss :     3.9468\n",
      "Epoch : 95 --batch : 5 --- Loss :     4.8462\n",
      "Testing Accuracy after epoch 94 : 0.996999979019165\n",
      "Epoch : 96 --batch : 1 --- Loss :     3.5661\n",
      "Epoch : 96 --batch : 2 --- Loss :     0.3039\n",
      "Epoch : 96 --batch : 3 --- Loss :     1.0595\n",
      "Epoch : 96 --batch : 4 --- Loss :     4.1127\n",
      "Epoch : 96 --batch : 5 --- Loss :     4.1582\n",
      "Testing Accuracy after epoch 95 : 0.996999979019165\n",
      "Epoch : 97 --batch : 1 --- Loss :     3.2060\n",
      "Epoch : 97 --batch : 2 --- Loss :     0.0209\n",
      "Epoch : 97 --batch : 3 --- Loss :     0.4799\n",
      "Epoch : 97 --batch : 4 --- Loss :     4.3665\n",
      "Epoch : 97 --batch : 5 --- Loss :     3.5688\n",
      "Testing Accuracy after epoch 96 : 0.996999979019165\n",
      "Epoch : 98 --batch : 1 --- Loss :     3.2260\n",
      "Epoch : 98 --batch : 2 --- Loss :     0.4445\n",
      "Epoch : 98 --batch : 3 --- Loss :     1.9859\n",
      "Epoch : 98 --batch : 4 --- Loss :     3.8979\n",
      "Epoch : 98 --batch : 5 --- Loss :     4.1072\n",
      "Testing Accuracy after epoch 97 : 0.996999979019165\n",
      "Epoch : 99 --batch : 1 --- Loss :     2.7929\n",
      "Epoch : 99 --batch : 2 --- Loss :     0.0000\n",
      "Epoch : 99 --batch : 3 --- Loss :     0.4437\n",
      "Epoch : 99 --batch : 4 --- Loss :     3.4935\n",
      "Epoch : 99 --batch : 5 --- Loss :     3.5849\n",
      "Testing Accuracy after epoch 98 : 0.996999979019165\n",
      "Epoch : 100 --batch : 1 --- Loss :     3.3263\n",
      "Epoch : 100 --batch : 2 --- Loss :     0.0000\n",
      "Epoch : 100 --batch : 3 --- Loss :     0.9259\n",
      "Epoch : 100 --batch : 4 --- Loss :     4.2779\n",
      "Epoch : 100 --batch : 5 --- Loss :     3.1894\n",
      "Testing Accuracy after epoch 99 : 0.996999979019165\n",
      "Predicted   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14   All\n",
      "Actual                                                                     \n",
      "0          70   0   0   0   0   0   0   0   0   0   0   0   0   0   0    70\n",
      "1           0  68   0   0   0   0   0   0   0   0   0   0   0   0   0    68\n",
      "2           0   0  62   1   0   0   0   0   0   0   0   0   0   0   0    63\n",
      "3           0   0   0  61   0   0   0   0   0   0   0   0   0   0   0    61\n",
      "4           0   0   0   0  61   0   0   0   0   0   0   0   0   0   0    61\n",
      "5           0   0   0   0   0  57   0   0   0   0   0   0   0   0   0    57\n",
      "6           0   0   0   0   0   1  61   0   0   0   0   0   0   0   0    62\n",
      "7           0   0   0   0   0   0   0  63   0   0   0   0   0   0   0    63\n",
      "8           0   0   0   0   0   0   0   0  59   0   0   0   0   0   0    59\n",
      "9           0   0   0   0   0   0   0   0   0  79   0   0   0   0   0    79\n",
      "10          0   0   0   0   0   0   0   0   0   0  70   0   0   0   0    70\n",
      "11          0   0   0   0   0   0   0   0   0   0   0  68   1   0   0    69\n",
      "12          0   0   0   0   0   0   0   0   0   0   0   0  82   0   0    82\n",
      "13          0   0   0   0   0   0   0   0   0   0   0   0   0  64   0    64\n",
      "14          0   0   0   0   0   0   0   0   0   0   0   0   0   0  72    72\n",
      "All        70  68  62  62  61  58  61  63  59  79  70  68  83  64  72  1000\n",
      "Model saved in file: model/final_static_model\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 100\n",
    "n_classes = 15 \n",
    "dropout = 0.80\n",
    "\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32],mean=0.0,stddev=1.0),name='wc1'),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64],mean=0.0,stddev=1.0),name='wc2'),\n",
    "    'wd1': tf.Variable(tf.random_normal([16*12*64, 1024],mean=0.0,stddev=1.0),name='wd1'),\n",
    "    'w_out': tf.Variable(tf.random_normal([1024, n_classes],mean=0.0,stddev=1.0),name='w_out')}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32]),name='bc1'),\n",
    "    'bc2': tf.Variable(tf.random_normal([64]),name='bc2'),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024]),name='bd1'),\n",
    "    'b_out': tf.Variable(tf.random_normal([n_classes]),name='b_out')}\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 64*48*1 to 32*24*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 32*24*32 to 16*12*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Fully connected layer - 16*12*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 5\n",
    "    out = tf.add(tf.matmul(fc1, weights['w_out']), biases['b_out'])\n",
    "    return out\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None,64,48,1])\n",
    "y = tf.placeholder(tf.float32, [None,n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model--output for each image\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# Launch the graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    multiplier_matrix = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for i in [1,2,3,4,5] :\n",
    "            batch_x = X_batch(i)\n",
    "            batch_y = Y_batch(i)\n",
    "        \n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "            print('Epoch : {:>2} --batch : {} --- Loss : {:>10.4f}'.format(epoch + 1,i,loss))\n",
    "        \n",
    "        test_acc = sess.run(accuracy, feed_dict={\n",
    "          x:X_test,\n",
    "          y:Y_test,\n",
    "          keep_prob: 1.0})\n",
    "\n",
    "        print('Testing Accuracy after epoch {} : {}'.format(epoch,test_acc))\n",
    "        \n",
    "        if (epoch+1 % 10 == 0) or (epoch == 0) or (epoch == 99) :\n",
    "            y_true_labels = np.matmul(Y_test,multiplier_matrix)\n",
    "            y_p = tf.argmax(logits, 1)\n",
    "            val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x:X_test,y:Y_test,keep_prob: 1.0})\n",
    "            #print(confusion_matrix(y_true_labels, y_pred))\n",
    "\n",
    "            y_actu = pd.Series(y_true_labels, name='Actual')\n",
    "            y_pred = pd.Series(y_pred, name='Predicted')\n",
    "            df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "            #print(df_confusion)    \n",
    "\n",
    "            df_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "            print(df_confusion)\n",
    "\n",
    "            #df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "            #print(df_conf_norm)\n",
    "            \n",
    "\n",
    "    weights_updated = {\n",
    "        'wc1_updated': tf.Variable(weights['wc1'],name='wc1_updated'),\n",
    "        'wc2_updated': tf.Variable(weights['wc2'],name='wc2_updated'),\n",
    "        'wd1_updated': tf.Variable(weights['wd1'],name='wd1_updated'),\n",
    "        'w_out_updated': tf.Variable(weights['w_out'],name='w_out_updated')}\n",
    "\n",
    "    biases_updated = {\n",
    "        'bc1_updated': tf.Variable(biases['bc1'],name='bc1_updated'),\n",
    "        'bc2_updated': tf.Variable(biases['bc2'],name='bc2_updated'),\n",
    "        'bd1_updated': tf.Variable(biases['bd1'],name='bd1_updated'),\n",
    "        'b_out_updated': tf.Variable(biases['b_out'],name='b_out_updated')}\n",
    "    \n",
    "    \"\"\"\n",
    "    y_true_labels = np.matmul(Y_test,multiplier_matrix)\n",
    "    y_p = tf.argmax(logits, 1)\n",
    "    val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x:X_test,y:Y_test,keep_prob: 1.0})\n",
    "    #print(y_pred.shape,y_true_labels.shape)\n",
    "    #y_pred = np.matmul(y_pred,multiplier_matrix)\n",
    "    print(confusion_matrix(y_true_labels, y_pred))\n",
    "    \"\"\"\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"model/final_static_model\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the benchmark model, I proposed to use decision tree classifier because unlike CNN, which are black box models, we can reason why a particular output was observed for the decision tree classifier. Moreover, since we have used decision tree classifier for skin detection, a question may arrise that why was decision tree classifier not used here as well instead of CNN those are very time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubam Sachdeva\\AppData\\Local\\Continuum\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8765 images in the training dataset\n",
      "There are 1000 images in the testing dataset\n",
      "Accuracy on testing data : 0.993\n",
      "Predicted  0   1   2   3   4   5   6   7   8   9   10  11  12  13  14\n",
      "Actual                                                               \n",
      "0          70   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "1           0  67   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "2           0   0  62   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "3           0   0   0  61   0   0   0   0   0   0   0   0   0   0   0\n",
      "4           0   0   0   0  61   0   0   0   0   0   0   0   0   0   0\n",
      "5           0   0   0   0   0  57   0   0   0   0   0   0   0   0   0\n",
      "6           0   0   0   0   0   0  62   0   0   0   0   0   0   0   0\n",
      "7           0   0   0   0   0   0   0  63   0   0   0   0   0   0   0\n",
      "8           0   0   0   0   0   0   0   0  57   2   0   0   0   0   0\n",
      "9           0   0   0   0   0   0   0   0   0  79   0   0   0   0   0\n",
      "10          0   0   0   0   0   0   0   0   0   0  70   0   0   0   0\n",
      "11          0   0   0   0   0   0   0   0   0   0   0  69   0   0   0\n",
      "12          0   0   0   0   0   0   0   0   1   0   0   1  79   1   0\n",
      "13          0   0   0   0   0   0   0   0   0   0   0   0   0  64   0\n",
      "14          0   0   0   0   0   0   0   0   0   0   0   0   0   0  72\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(traindata,labeldata, test_size=0.1024, random_state=100)\n",
    "\n",
    "print(\"There are {} images in the training dataset\".format(X_train.shape[0]))\n",
    "print(\"There are {} images in the testing dataset\".format(X_test.shape[0]))\n",
    "\n",
    "Y_train_labels = np.matmul(Y_train,multiplier_matrix)\n",
    "Y_test_labels = np.matmul(Y_test,multiplier_matrix)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X_train, Y_train_labels)\n",
    "#print(clf.feature_importances_)\n",
    "print(\"Accuracy on testing data : {}\".format(clf.score(X_test, Y_test_labels)))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_actu = pd.Series(Y_test_labels, name='Actual')\n",
    "y_pred = pd.Series(y_pred, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As clearly seen in the two confusion metrics formed, the CNN model performs better. The performance of decision tree is also very good because the data prerocessing step is very vital and improves the preformance vastly. Moreover, the enormous data available makes the accuracy better and generalizes the model better. Although the perfromace of decision tree is very good but CNN is even better. Therefore, we have successfully made an improved model. Even better than suggested in most research papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CNN, as the comfusion metrics suggests, there are only 3 images those are predicted wrong out of the 1000 testing images while for decision tree the wrong predictions are 7, more than double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1003\n",
      "1000\n",
      "1007\n",
      "\n",
      "For CNN :\n",
      "0.997\n",
      "1.0\n",
      "0.997\n",
      "0.99849774662\n",
      "\n",
      "For Decision Tree :\n",
      "0.993\n",
      "1.0\n",
      "0.993\n",
      "0.996487706974\n"
     ]
    }
   ],
   "source": [
    "#calculating precision and recall for both the classifiers :\n",
    "\n",
    "from sklearn.metrics import recall_score,f1_score,precision_score\n",
    "actual_label = [1]*1000\n",
    "\n",
    "CNN_predicted_label = actual_label[:997]\n",
    "for i in range(0,3) :\n",
    "    CNN_predicted_label.append(2)\n",
    "    \n",
    "DT_predicted_label = actual_label[:993]\n",
    "\n",
    "for i in range(0,7) :\n",
    "    DT_predicted_label.append(2)\n",
    "\n",
    "print(\"\\nFor CNN :\")\n",
    "print(accuracy_score(CNN_predicted_label,actual_label))\n",
    "print(recall_score(CNN_predicted_label,actual_label))\n",
    "print(precision_score(CNN_predicted_label,actual_label))\n",
    "print(f1_score(CNN_predicted_label,actual_label))\n",
    "\n",
    "print(\"\\nFor Decision Tree :\")\n",
    "print(accuracy_score(DT_predicted_label,actual_label))\n",
    "print(recall_score(DT_predicted_label,actual_label))\n",
    "print(precision_score(DT_predicted_label,actual_label))\n",
    "print(f1_score(DT_predicted_label,actual_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that f1 score that is considered most important for any classifier, because it captures both precision and recall, is also better for CNN. Hence we can safely state that form a 3 channel RGB  we have succesfully make very good data and further otu model works very well with the data made. CNN is the main classifier used in the project but the magical part for the project that enables such high accuracy and f1_score are the preprocessing steps taken for data making. Skin detection and largest contour detection simply eliminates all the possible reasons the accuracy would decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
